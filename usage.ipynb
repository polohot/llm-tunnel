{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THE MAIN API IN LOCAL\n",
    "# uvicorn main:app\n",
    "\n",
    "# RUN COMMAND ON KOYEB\n",
    "# uvicorn main:app --host 0.0.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "load_dotenv()\n",
    "openai_apikey = os.getenv(\"OPENAI_API_KEY\")\n",
    "llama_cloud_apikey = os.getenv(\"LLAMA_CLOUD_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-Bg45puFPULWQ5NMfiz56FwXRktnOk',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1749365209,\n",
       " 'model': 'gpt-4o-2024-08-06',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': '3 + 5 equals 8.',\n",
       "    'refusal': None,\n",
       "    'annotations': []},\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 43,\n",
       "  'completion_tokens': 8,\n",
       "  'total_tokens': 51,\n",
       "  'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0},\n",
       "  'completion_tokens_details': {'reasoning_tokens': 0,\n",
       "   'audio_tokens': 0,\n",
       "   'accepted_prediction_tokens': 0,\n",
       "   'rejected_prediction_tokens': 0}},\n",
       " 'service_tier': 'default',\n",
       " 'system_fingerprint': 'fp_07871e2ad8'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "url = \"http://127.0.0.1:8000/openai\"\n",
    "body = {\n",
    "    \"model\": \"gpt-4o\",\n",
    "    \"messages\": [\n",
    "        {'role': 'system', 'content': 'You are a helpful assistant.'},\n",
    "        {'role': 'user', 'content': 'Hello!'},\n",
    "        {'role': 'assistant', 'content': 'Hello! How can I assist you today?'},\n",
    "        {'role': 'user', 'content': 'What is 3+5?'},\n",
    "    ],\n",
    "    \"max_tokens\": 1000\n",
    "}\n",
    "response = requests.post(url, json=body, params={\"apikey\": openai_apikey})\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-Bg4605NKcxar2FiQzBMPQg9Eqyx1f',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1749365220,\n",
       " 'model': 'gpt-4o-2024-08-06',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': '3 + 5 equals 8.',\n",
       "    'refusal': None,\n",
       "    'annotations': []},\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 14,\n",
       "  'completion_tokens': 8,\n",
       "  'total_tokens': 22,\n",
       "  'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0},\n",
       "  'completion_tokens_details': {'reasoning_tokens': 0,\n",
       "   'audio_tokens': 0,\n",
       "   'accepted_prediction_tokens': 0,\n",
       "   'rejected_prediction_tokens': 0}},\n",
       " 'service_tier': 'default',\n",
       " 'system_fingerprint': 'fp_07871e2ad8'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "url = \"http://127.0.0.1:8000/openai\"\n",
    "body = {\n",
    "    \"model\": \"gpt-4o\",\n",
    "    \"messages\": [\n",
    "        {'role': 'user', 'content': 'What is 3+5?'},\n",
    "    ],\n",
    "    \"max_tokens\": 1000\n",
    "}\n",
    "response = requests.post(url, json=body, params={\"apikey\": openai_apikey})\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-Bg466bHkUd76nM3SJUMBG8ssOm83N',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1749365226,\n",
       " 'model': 'gpt-4o-2024-08-06',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': '3 + 5 equals 8.',\n",
       "    'refusal': None,\n",
       "    'annotations': []},\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 13,\n",
       "  'completion_tokens': 8,\n",
       "  'total_tokens': 21,\n",
       "  'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0},\n",
       "  'completion_tokens_details': {'reasoning_tokens': 0,\n",
       "   'audio_tokens': 0,\n",
       "   'accepted_prediction_tokens': 0,\n",
       "   'rejected_prediction_tokens': 0}},\n",
       " 'service_tier': 'default',\n",
       " 'system_fingerprint': 'fp_07871e2ad8'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"http://127.0.0.1:8000/openai_single\"\n",
    "question = 'what is 3+5'\n",
    "response = requests.post(url, params={\"apikey\": openai_apikey, 'question': question})\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-Bg46B8PRkvD4HqxUB9hFkQDZXHaZW',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1749365231,\n",
       " 'model': 'gpt-4o-2024-08-06',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': '3 + 5 equals 8.',\n",
       "    'refusal': None,\n",
       "    'annotations': []},\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 13,\n",
       "  'completion_tokens': 8,\n",
       "  'total_tokens': 21,\n",
       "  'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0},\n",
       "  'completion_tokens_details': {'reasoning_tokens': 0,\n",
       "   'audio_tokens': 0,\n",
       "   'accepted_prediction_tokens': 0,\n",
       "   'rejected_prediction_tokens': 0}},\n",
       " 'service_tier': 'default',\n",
       " 'system_fingerprint': 'fp_07871e2ad8'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://ancient-almeda-personal-personal-22e19704.koyeb.app/openai_single\"\n",
    "question = 'what is 3+5'\n",
    "response = requests.post(url, params={\"apikey\": openai_apikey, 'question': question})\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'pages': [{'page': 1,\n",
       "    'text': '                                ADRIAN LUYAPHAN\\n                                SENIOR APPLIED SCIENTIST, DATA AND AI\\n                                 CONTACT ME         Email: adrian.luyaphan@gmail.com      MY LINKEDIN       www.linkedin.com/in/\\n                                                    Cell: +66-96-071-0842                                   adrian-luyaphan\\nEDUCATION                                      in                   IT SKILLS\\nJAN 2019 - OCT 2019                                                 ESSENTIAL / GENERIC\\nExecutive Program in Algorithmic Trading - EPAT                     SQL | Python | Ubuntu\\nQuantInsti Quantitative Learning Pve. Ltd.                          MS Office | Solidworks | Photoshop\\n- Develop live trading strategies using python and APIs             MICROSOFT CLOUD\\n- Backtesting trading strategies on normal and volatile market      Fabric | Copilot Studio | DataBricks | Cloud SQL\\n- Evaluation of strategy (Sharpe,Sortino,RAROC)                     Azure Blob Storage | Azure Logic Apps\\n- Evaluation of portfolio metrics (Allocation, Concentration)       MACHINE LEARNING\\nGraduated 2014                                                      - Machine Learning for Structured / Unstructured Data\\nBachelor of Engineering | Aerospace                                 - Deep Learning on Structured Data and Image Recognition\\nChulalongkorn University                                            ACHIEVEMENTS\\nPathum Wan | Bangkok | Central Thailand\\nLANGUAGES                                                           JUNE 2019\\n                                                                    GRAB AIforSEA Competition                  Grab\\nTHAI - Native                                                       - Machine Learning Competition\\nENGLISH - Business Professional                                     - top 50 from 1200 global participants\\n                                                                    https://github.com/polohot/grabsubmission\\nWORK EXPERIENCE\\nSEP 2021 - Present\\nSenior Applied Scientist, Data & AI\\nDKSH Performance Material | Global Digital Analytics    [East Bangkok | Central Thailand]\\nOperational Excellence with AI (AI Engineer)\\n- Internal Chatbot - Building and deployment of internal chatbot on MS team, using copilot studio and standard RAG from JSON and PDF\\n                                               in\\n- Content Generation - Automation of product generation for frontend sales team using chatGPT API and python, reducing workload for\\nsales team by 85%\\n- Website product semantic search - Create search API for frontend by understand the context instead of keyword\\n- Product Classification - Auto classification of product category and business line from PDF input, using chatGPT API and python\\nData Mining for marketing support (Data Science)\\n- Customer classification by behaviour, Unsupervised Machine learning classification for customer grouping, Identify behavior of each\\ngroup and forward to sales team for targeting.\\n- Increase demand planning efficiency, Time series prediction for demand planning, suggesting sales team to stock inventory by statistic and\\nmachine learning model, create safety stock model by material lead time.\\nData Migration (Data Engineering)\\n- Leading Pipeline Migration of from legacy system to Microsoft Fabric Platform, Manage and mentor subordinate for efficient migration\\n- Managing Stakeholders and Business user for Microsoft Fabric\\nDesign and develop data structure for Analytic (Data Architecture & Data Engineering)\\n- Managing ETL pipeline on Databricks using Spark and Python, creating custom table for business consumption\\n- Each project reduce time on data transformation for business user by 50%-80% compare to PBI\\n- ML Model development speed up by at least 50%-60%\\nAutomation using python & Microsoft tool stack\\n- Demand planning (ANZ, CHINA, JAPAN, THAILAND, INDONESIA, VN)\\n Automation of sales forecast from sales employee and auto consolidate to SQL, reduce manual work by 60%-80%\\n- Daily report generation follow business logic taking data from SAP, Salesforce,SQL, reduce manual work by 80%-100%\\nJUL 2020 - AUG 2021\\nData Analyst\\nBangkok Bank | Research and Analytic Division    [CBD Bangkok | Central Thailand]\\nData Mining  for campaign support using SAS software,\\nData Validation test for Data ETL project using python and SQL\\nSEP 2019 - JUN 2020\\nPortfolio Management Officer\\nBangkok Bank | International Banking Division    [CBD Bangkok | Central Thailand]\\nData Service and Visualisation using Python and Excel, Develop Python script for Data Engineering process and Python application for Data\\nAutomation Project\\nDEC 2014 - SEP 2019\\nFlight Simulator Technician\\nPanAm International Flight Training Center   [ABAC Bangna | Samutprakarn | Central Thailand]\\nB737-800 and A320-200 simulator preflight and periodic inspection/maintenance. Realtime troubleshooting simulator issues with\\nsimulator manufacturers, Documentation and graphic design support.',\n",
       "    'md': '# ADRIAN LUYAPHAN\\n\\n# SENIOR APPLIED SCIENTIST, DATA AND AI\\n\\nCONTACT ME\\n\\nEmail: adrian.luyaphan@gmail.com\\n\\nCell: +66-96-071-0842\\n\\nMY LINKEDIN: www.linkedin.com/in/adrian-luyaphan\\n\\n# EDUCATION\\n\\nJAN 2019 - OCT 2019\\n\\nExecutive Program in Algorithmic Trading - EPAT\\n\\nQuantInsti Quantitative Learning Pve. Ltd.\\n\\n- Develop live trading strategies using python and APIs\\n\\n- Backtesting trading strategies on normal and volatile market\\n\\n- Evaluation of strategy (Sharpe, Sortino, RAROC)\\n\\n- Evaluation of portfolio metrics (Allocation, Concentration)\\n\\nGraduated 2014\\n\\nBachelor of Engineering | Aerospace\\n\\nChulalongkorn University\\n\\nPathum Wan | Bangkok | Central Thailand\\n\\n# IT SKILLS\\n\\n# ESSENTIAL / GENERIC\\n\\nSQL | Python | Ubuntu | MS Office | Solidworks | Photoshop\\n\\n# MICROSOFT CLOUD\\n\\nFabric | Copilot Studio | DataBricks | Cloud SQL | Azure Blob Storage | Azure Logic Apps\\n\\n# MACHINE LEARNING\\n\\n- Machine Learning for Structured / Unstructured Data\\n\\n- Deep Learning on Structured Data and Image Recognition\\n\\n# ACHIEVEMENTS\\n\\nJUNE 2019\\n\\nGRAB AIforSEA Competition - Grab\\n\\n- Machine Learning Competition\\n\\n- Top 50 from 1200 global participants\\n\\nhttps://github.com/polohot/grabsubmission\\n\\n# WORK EXPERIENCE\\n\\n# SEP 2021 - Present\\n\\n# Senior Applied Scientist, Data & AI\\n\\nDKSH Performance Material | Global Digital Analytics [East Bangkok | Central Thailand]\\n\\n# Operational Excellence with AI (AI Engineer)\\n\\n- Internal Chatbot - Building and deployment of internal chatbot on MS team, using copilot studio and standard RAG from JSON and PDF\\n- Content Generation - Automation of product generation for frontend sales team using chatGPT API and python, reducing workload for sales team by 85%\\n- Website product semantic search - Create search API for frontend by understanding the context instead of keyword\\n- Product Classification - Auto classification of product category and business line from PDF input, using chatGPT API and python\\n\\n# Data Mining for marketing support (Data Science)\\n\\n- Customer classification by behaviour, Unsupervised Machine learning classification for customer grouping, Identify behavior of each group and forward to sales team for targeting.\\n- Increase demand planning efficiency, Time series prediction for demand planning, suggesting sales team to stock inventory by statistic and machine learning model, create safety stock model by material lead time.\\n\\n# Data Migration (Data Engineering)\\n\\n- Leading Pipeline Migration of from legacy system to Microsoft Fabric Platform, Manage and mentor subordinate for efficient migration\\n- Managing Stakeholders and Business user for Microsoft Fabric\\n\\n# Design and develop data structure for Analytic (Data Architecture & Data Engineering)\\n\\n- Managing ETL pipeline on Databricks using Spark and Python, creating custom table for business consumption\\n- Each project reduce time on data transformation for business user by 50%-80% compare to PBI\\n- ML Model development speed up by at least 50%-60%\\n\\n# Automation using python & Microsoft tool stack\\n\\n- Demand planning (ANZ, CHINA, JAPAN, THAILAND, INDONESIA, VN) - Automation of sales forecast from sales employee and auto consolidate to SQL, reduce manual work by 60%-80%\\n- Daily report generation follow business logic taking data from SAP, Salesforce, SQL, reduce manual work by 80%-100%\\n\\n# JUL 2020 - AUG 2021\\n\\n# Data Analyst\\n\\nBangkok Bank | Research and Analytic Division [CBD Bangkok | Central Thailand]\\n\\n- Data Mining for campaign support using SAS software\\n- Data Validation test for Data ETL project using python and SQL\\n\\n# SEP 2019 - JUN 2020\\n\\n# Portfolio Management Officer\\n\\nBangkok Bank | International Banking Division [CBD Bangkok | Central Thailand]\\n\\n- Data Service and Visualisation using Python and Excel, Develop Python script for Data Engineering process and Python application for Data Automation Project\\n\\n# DEC 2014 - SEP 2019\\n\\n# Flight Simulator Technician\\n\\nPanAm International Flight Training Center [ABAC Bangna | Samutprakarn | Central Thailand]\\n\\n- B737-800 and A320-200 simulator preflight and periodic inspection/maintenance. Realtime troubleshooting simulator issues with simulator manufacturers, Documentation and graphic design support.',\n",
       "    'images': [{'name': 'img_p0_1.png',\n",
       "      'height': 1482.0,\n",
       "      'width': 1047.0,\n",
       "      'x': 0.500000016,\n",
       "      'y': -12.739035840720021,\n",
       "      'original_width': 1047,\n",
       "      'original_height': 1482,\n",
       "      'type': None},\n",
       "     {'name': 'img_p0_2.png',\n",
       "      'height': 14.0,\n",
       "      'width': 1199.0,\n",
       "      'x': 154.4981348652,\n",
       "      'y': 95.68110660510239,\n",
       "      'original_width': None,\n",
       "      'original_height': None,\n",
       "      'type': None},\n",
       "     {'name': 'img_p0_2.png',\n",
       "      'height': 14.0,\n",
       "      'width': 1199.0,\n",
       "      'x': 130.82476796592,\n",
       "      'y': 149.8444921965984,\n",
       "      'original_width': None,\n",
       "      'original_height': None,\n",
       "      'type': None},\n",
       "     {'name': 'img_p0_2.png',\n",
       "      'height': 14.0,\n",
       "      'width': 1199.0,\n",
       "      'x': 195.09438616428,\n",
       "      'y': 350.5000104285984,\n",
       "      'original_width': None,\n",
       "      'original_height': None,\n",
       "      'type': None},\n",
       "     {'name': 'img_p0_2.png',\n",
       "      'height': 14.0,\n",
       "      'width': 1199.0,\n",
       "      'x': 444.679148088,\n",
       "      'y': 270.2646141602784,\n",
       "      'original_width': None,\n",
       "      'original_height': None,\n",
       "      'type': None},\n",
       "     {'name': 'img_p0_3.png',\n",
       "      'height': 65.0,\n",
       "      'width': 157.0,\n",
       "      'x': 488.59363768223994,\n",
       "      'y': 295.82650159243195,\n",
       "      'original_width': 157,\n",
       "      'original_height': 65,\n",
       "      'type': None},\n",
       "     {'name': 'img_p0_2.png',\n",
       "      'height': 14.0,\n",
       "      'width': 1199.0,\n",
       "      'x': 388.505917944,\n",
       "      'y': 148.68490554531837,\n",
       "      'original_width': None,\n",
       "      'original_height': None,\n",
       "      'type': None},\n",
       "     {'name': 'img_p0_2.png',\n",
       "      'height': 14.0,\n",
       "      'width': 1199.0,\n",
       "      'x': 130.82478765096,\n",
       "      'y': 293.7683353848384,\n",
       "      'original_width': None,\n",
       "      'original_height': None,\n",
       "      'type': None},\n",
       "     {'name': 'img_p0_1.png',\n",
       "      'height': 1482.0,\n",
       "      'width': 1047.0,\n",
       "      'x': 0.500000016,\n",
       "      'y': 298.4075686041599,\n",
       "      'original_width': 1047,\n",
       "      'original_height': 1482,\n",
       "      'type': None},\n",
       "     {'name': 'img_p0_4.png',\n",
       "      'height': 1536.0,\n",
       "      'width': 1024.0,\n",
       "      'x': 32.6279538,\n",
       "      'y': 6.015748223999992,\n",
       "      'original_width': 1024,\n",
       "      'original_height': 1536,\n",
       "      'type': None}],\n",
       "    'charts': [],\n",
       "    'tables': [],\n",
       "    'layout': [],\n",
       "    'items': [{'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'ADRIAN LUYAPHAN',\n",
       "      'md': '# ADRIAN LUYAPHAN',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 163.0, 'y': 26.0, 'w': 240.0, 'h': 25.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'SENIOR APPLIED SCIENTIST, DATA AND AI',\n",
       "      'md': '# SENIOR APPLIED SCIENTIST, DATA AND AI',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 163.0, 'y': 69.0, 'w': 316.0, 'h': 16.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': 'CONTACT ME\\n\\nEmail: adrian.luyaphan@gmail.com\\n\\nCell: +66-96-071-0842\\n\\nMY LINKEDIN: www.linkedin.com/in/adrian-luyaphan',\n",
       "      'md': 'CONTACT ME\\n\\nEmail: adrian.luyaphan@gmail.com\\n\\nCell: +66-96-071-0842\\n\\nMY LINKEDIN: www.linkedin.com/in/adrian-luyaphan',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 164.0, 'y': 103.0, 'w': 389.0, 'h': 328.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'EDUCATION',\n",
       "      'md': '# EDUCATION',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 27.0, 'y': 142.0, 'w': 93.0, 'h': 16.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': 'JAN 2019 - OCT 2019\\n\\nExecutive Program in Algorithmic Trading - EPAT\\n\\nQuantInsti Quantitative Learning Pve. Ltd.\\n\\n- Develop live trading strategies using python and APIs\\n\\n- Backtesting trading strategies on normal and volatile market\\n\\n- Evaluation of strategy (Sharpe, Sortino, RAROC)\\n\\n- Evaluation of portfolio metrics (Allocation, Concentration)\\n\\nGraduated 2014\\n\\nBachelor of Engineering | Aerospace\\n\\nChulalongkorn University\\n\\nPathum Wan | Bangkok | Central Thailand',\n",
       "      'md': 'JAN 2019 - OCT 2019\\n\\nExecutive Program in Algorithmic Trading - EPAT\\n\\nQuantInsti Quantitative Learning Pve. Ltd.\\n\\n- Develop live trading strategies using python and APIs\\n\\n- Backtesting trading strategies on normal and volatile market\\n\\n- Evaluation of strategy (Sharpe, Sortino, RAROC)\\n\\n- Evaluation of portfolio metrics (Allocation, Concentration)\\n\\nGraduated 2014\\n\\nBachelor of Engineering | Aerospace\\n\\nChulalongkorn University\\n\\nPathum Wan | Bangkok | Central Thailand',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 27.0, 'y': 151.0, 'w': 240.0, 'h': 280.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'IT SKILLS',\n",
       "      'md': '# IT SKILLS',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 309.0, 'y': 141.0, 'w': 68.0, 'h': 16.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'ESSENTIAL / GENERIC',\n",
       "      'md': '# ESSENTIAL / GENERIC',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 309.0, 'y': 161.0, 'w': 92.0, 'h': 9.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': 'SQL | Python | Ubuntu | MS Office | Solidworks | Photoshop',\n",
       "      'md': 'SQL | Python | Ubuntu | MS Office | Solidworks | Photoshop',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 309.0, 'y': 171.0, 'w': 138.0, 'h': 20.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'MICROSOFT CLOUD',\n",
       "      'md': '# MICROSOFT CLOUD',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 309.0, 'y': 192.0, 'w': 85.0, 'h': 9.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': 'Fabric | Copilot Studio | DataBricks | Cloud SQL | Azure Blob Storage | Azure Logic Apps',\n",
       "      'md': 'Fabric | Copilot Studio | DataBricks | Cloud SQL | Azure Blob Storage | Azure Logic Apps',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 309.0, 'y': 203.0, 'w': 185.0, 'h': 19.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'MACHINE LEARNING',\n",
       "      'md': '# MACHINE LEARNING',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 224.0, 'y': 151.0, 'w': 174.0, 'h': 280.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '- Machine Learning for Structured / Unstructured Data\\n\\n- Deep Learning on Structured Data and Image Recognition',\n",
       "      'md': '- Machine Learning for Structured / Unstructured Data\\n\\n- Deep Learning on Structured Data and Image Recognition',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 224.0, 'y': 151.0, 'w': 315.0, 'h': 280.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'ACHIEVEMENTS',\n",
       "      'md': '# ACHIEVEMENTS',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 309.0, 'y': 262.0, 'w': 123.0, 'h': 16.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': 'JUNE 2019\\n\\nGRAB AIforSEA Competition - Grab\\n\\n- Machine Learning Competition\\n\\n- Top 50 from 1200 global participants\\n\\nhttps://github.com/polohot/grabsubmission',\n",
       "      'md': 'JUNE 2019\\n\\nGRAB AIforSEA Competition - Grab\\n\\n- Machine Learning Competition\\n\\n- Top 50 from 1200 global participants\\n\\nhttps://github.com/polohot/grabsubmission',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 224.0, 'y': 151.0, 'w': 325.0, 'h': 280.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'WORK EXPERIENCE',\n",
       "      'md': '# WORK EXPERIENCE',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 27.0, 'y': 343.0, 'w': 151.0, 'h': 16.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'SEP 2021 - Present',\n",
       "      'md': '# SEP 2021 - Present',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 27.0, 'y': 373.0, 'w': 77.0, 'h': 9.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Senior Applied Scientist, Data & AI',\n",
       "      'md': '# Senior Applied Scientist, Data & AI',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 27.0, 'y': 383.0, 'w': 141.0, 'h': 9.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': 'DKSH Performance Material | Global Digital Analytics [East Bangkok | Central Thailand]',\n",
       "      'md': 'DKSH Performance Material | Global Digital Analytics [East Bangkok | Central Thailand]',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 27.0, 'y': 394.0, 'w': 365.0, 'h': 9.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Operational Excellence with AI (AI Engineer)',\n",
       "      'md': '# Operational Excellence with AI (AI Engineer)',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 27.0, 'y': 151.0, 'w': 219.0, 'h': 280.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '- Internal Chatbot - Building and deployment of internal chatbot on MS team, using copilot studio and standard RAG from JSON and PDF\\n- Content Generation - Automation of product generation for frontend sales team using chatGPT API and python, reducing workload for sales team by 85%\\n- Website product semantic search - Create search API for frontend by understanding the context instead of keyword\\n- Product Classification - Auto classification of product category and business line from PDF input, using chatGPT API and python',\n",
       "      'md': '- Internal Chatbot - Building and deployment of internal chatbot on MS team, using copilot studio and standard RAG from JSON and PDF\\n- Content Generation - Automation of product generation for frontend sales team using chatGPT API and python, reducing workload for sales team by 85%\\n- Website product semantic search - Create search API for frontend by understanding the context instead of keyword\\n- Product Classification - Auto classification of product category and business line from PDF input, using chatGPT API and python',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 27.0, 'y': 151.0, 'w': 531.0, 'h': 315.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Data Mining for marketing support (Data Science)',\n",
       "      'md': '# Data Mining for marketing support (Data Science)',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 27.0, 'y': 151.0, 'w': 219.0, 'h': 535.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '- Customer classification by behaviour, Unsupervised Machine learning classification for customer grouping, Identify behavior of each group and forward to sales team for targeting.\\n- Increase demand planning efficiency, Time series prediction for demand planning, suggesting sales team to stock inventory by statistic and machine learning model, create safety stock model by material lead time.',\n",
       "      'md': '- Customer classification by behaviour, Unsupervised Machine learning classification for customer grouping, Identify behavior of each group and forward to sales team for targeting.\\n- Increase demand planning efficiency, Time series prediction for demand planning, suggesting sales team to stock inventory by statistic and machine learning model, create safety stock model by material lead time.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 27.0, 'y': 151.0, 'w': 541.0, 'h': 367.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Data Migration (Data Engineering)',\n",
       "      'md': '# Data Migration (Data Engineering)',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 27.0, 'y': 151.0, 'w': 219.0, 'h': 378.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '- Leading Pipeline Migration of from legacy system to Microsoft Fabric Platform, Manage and mentor subordinate for efficient migration\\n- Managing Stakeholders and Business user for Microsoft Fabric',\n",
       "      'md': '- Leading Pipeline Migration of from legacy system to Microsoft Fabric Platform, Manage and mentor subordinate for efficient migration\\n- Managing Stakeholders and Business user for Microsoft Fabric',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 27.0, 'y': 151.0, 'w': 529.0, 'h': 399.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Design and develop data structure for Analytic (Data Architecture & Data Engineering)',\n",
       "      'md': '# Design and develop data structure for Analytic (Data Architecture & Data Engineering)',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 27.0, 'y': 151.0, 'w': 352.0, 'h': 409.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '- Managing ETL pipeline on Databricks using Spark and Python, creating custom table for business consumption\\n- Each project reduce time on data transformation for business user by 50%-80% compare to PBI\\n- ML Model development speed up by at least 50%-60%',\n",
       "      'md': '- Managing ETL pipeline on Databricks using Spark and Python, creating custom table for business consumption\\n- Each project reduce time on data transformation for business user by 50%-80% compare to PBI\\n- ML Model development speed up by at least 50%-60%',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 27.0, 'y': 151.0, 'w': 434.0, 'h': 441.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Automation using python & Microsoft tool stack',\n",
       "      'md': '# Automation using python & Microsoft tool stack',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 27.0, 'y': 151.0, 'w': 219.0, 'h': 451.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '- Demand planning (ANZ, CHINA, JAPAN, THAILAND, INDONESIA, VN) - Automation of sales forecast from sales employee and auto consolidate to SQL, reduce manual work by 60%-80%\\n- Daily report generation follow business logic taking data from SAP, Salesforce, SQL, reduce manual work by 80%-100%',\n",
       "      'md': '- Demand planning (ANZ, CHINA, JAPAN, THAILAND, INDONESIA, VN) - Automation of sales forecast from sales employee and auto consolidate to SQL, reduce manual work by 60%-80%\\n- Daily report generation follow business logic taking data from SAP, Salesforce, SQL, reduce manual work by 80%-100%',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 27.0, 'y': 151.0, 'w': 465.0, 'h': 483.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'JUL 2020 - AUG 2021',\n",
       "      'md': '# JUL 2020 - AUG 2021',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 27.0, 'y': 646.0, 'w': 86.0, 'h': 9.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Data Analyst',\n",
       "      'md': '# Data Analyst',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 27.0, 'y': 656.0, 'w': 54.0, 'h': 9.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': 'Bangkok Bank | Research and Analytic Division [CBD Bangkok | Central Thailand]\\n\\n- Data Mining for campaign support using SAS software\\n- Data Validation test for Data ETL project using python and SQL',\n",
       "      'md': 'Bangkok Bank | Research and Analytic Division [CBD Bangkok | Central Thailand]\\n\\n- Data Mining for campaign support using SAS software\\n- Data Validation test for Data ETL project using python and SQL',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 27.0, 'y': 151.0, 'w': 336.0, 'h': 588.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'SEP 2019 - JUN 2020',\n",
       "      'md': '# SEP 2019 - JUN 2020',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 27.0, 'y': 709.0, 'w': 85.0, 'h': 9.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Portfolio Management Officer',\n",
       "      'md': '# Portfolio Management Officer',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 27.0, 'y': 719.0, 'w': 124.0, 'h': 9.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': 'Bangkok Bank | International Banking Division [CBD Bangkok | Central Thailand]\\n\\n- Data Service and Visualisation using Python and Excel, Develop Python script for Data Engineering process and Python application for Data Automation Project',\n",
       "      'md': 'Bangkok Bank | International Banking Division [CBD Bangkok | Central Thailand]\\n\\n- Data Service and Visualisation using Python and Excel, Develop Python script for Data Engineering process and Python application for Data Automation Project',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 27.0, 'y': 151.0, 'w': 543.0, 'h': 609.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'DEC 2014 - SEP 2019',\n",
       "      'md': '# DEC 2014 - SEP 2019',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 27.0, 'y': 772.0, 'w': 85.0, 'h': 9.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Flight Simulator Technician',\n",
       "      'md': '# Flight Simulator Technician',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 27.0, 'y': 782.0, 'w': 111.0, 'h': 9.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': 'PanAm International Flight Training Center [ABAC Bangna | Samutprakarn | Central Thailand]\\n\\n- B737-800 and A320-200 simulator preflight and periodic inspection/maintenance. Realtime troubleshooting simulator issues with simulator manufacturers, Documentation and graphic design support.',\n",
       "      'md': 'PanAm International Flight Training Center [ABAC Bangna | Samutprakarn | Central Thailand]\\n\\n- B737-800 and A320-200 simulator preflight and periodic inspection/maintenance. Realtime troubleshooting simulator issues with simulator manufacturers, Documentation and graphic design support.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 27.0, 'y': 151.0, 'w': 508.0, 'h': 672.0}}],\n",
       "    'status': 'OK',\n",
       "    'links': [],\n",
       "    'width': 595.0,\n",
       "    'height': 841.0,\n",
       "    'triggeredAutoMode': False,\n",
       "    'parsingMode': 'accurate',\n",
       "    'structuredData': None,\n",
       "    'noStructuredContent': False,\n",
       "    'noTextContent': False}],\n",
       "  'job_metadata': {'job_pages': 0,\n",
       "   'job_auto_mode_triggered_pages': 0,\n",
       "   'job_is_cache_hit': True},\n",
       "  'file_name': '/tmp/tmphhmdagf6.pdf',\n",
       "  'job_id': 'a5864394-5b16-4994-97f9-9b51c935ba21',\n",
       "  'is_done': False,\n",
       "  'error': None},\n",
       " {'pages': [{'page': 1,\n",
       "    'text': \"Simplifying Data Workflows: Introduction to\\nDatabricks and Its Core Features\\n                               databricks\\nIn the fast-paced world of data engineering and analytics, having a platform that combines big data processing,\\nmachine learning, and seamless collaboration is a game-changer. Enter Databricks - a unified analytics\\nplatform built to empower teams to solve complex data challenges efficiently. Whether you're wrangling data\\n    , building machine learning models     , or managing ETL pipelines     , Databricks provides a powerful yet\\nuser-friendly environment to bring your ideas to life.\\nIn this blog, we’ll take a guided tour of the Databricks interface  , covering key features like clusters,\\nnotebooks, and jobs. Along the way, we’ll demonstrate how to:\\n    •       Create your first cluster for processing data.\\n    •      Analyze datasets using interactive notebooks.\\n    •       Schedule jobs to automate your workflows.\\nBy the end of this guide, you’ll not only have a strong grasp of Databricks’ core functionalities but also the\\nconfidence to explore its advanced features for your next big data project. Let’s dive in!\\nWhat is Databricks?\\nAt its core, Databricks is a cloud-based platform that provides a collaborative workspace for working with big\\ndata and AI. It brings together the power of:\\n    •       Data Engineering for building pipelines.\\n    •       Data Science for training machine learning models.\\n    •       Business Analytics for generating insights.\",\n",
       "    'md': \"# Simplifying Data Workflows: Introduction to Databricks and Its Core Features\\n\\nIn the fast-paced world of data engineering and analytics, having a platform that combines big data processing, machine learning, and seamless collaboration is a game-changer. Enter Databricks - a unified analytics platform built to empower teams to solve complex data challenges efficiently. Whether you're wrangling data, building machine learning models, or managing ETL pipelines, Databricks provides a powerful yet user-friendly environment to bring your ideas to life.\\n\\nIn this blog, we’ll take a guided tour of the Databricks interface, covering key features like clusters, notebooks, and jobs. Along the way, we’ll demonstrate how to:\\n\\n- Create your first cluster for processing data.\\n- Analyze datasets using interactive notebooks.\\n- Schedule jobs to automate your workflows.\\n\\nBy the end of this guide, you’ll not only have a strong grasp of Databricks’ core functionalities but also the confidence to explore its advanced features for your next big data project. Let’s dive in!\\n\\n# What is Databricks?\\n\\nAt its core, Databricks is a cloud-based platform that provides a collaborative workspace for working with big data and AI. It brings together the power of:\\n\\n- Data Engineering for building pipelines.\\n- Data Science for training machine learning models.\\n- Business Analytics for generating insights.\",\n",
       "    'images': [{'name': 'img_p0_1.png',\n",
       "      'height': 600.0,\n",
       "      'width': 1200.0,\n",
       "      'x': 66.7,\n",
       "      'y': 103.10000000000002,\n",
       "      'original_width': 1200,\n",
       "      'original_height': 600,\n",
       "      'type': None}],\n",
       "    'charts': [],\n",
       "    'tables': [],\n",
       "    'layout': [],\n",
       "    'items': [{'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Simplifying Data Workflows: Introduction to Databricks and Its Core Features',\n",
       "      'md': '# Simplifying Data Workflows: Introduction to Databricks and Its Core Features',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 35.0, 'w': 462.0, 'h': 252.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': \"In the fast-paced world of data engineering and analytics, having a platform that combines big data processing, machine learning, and seamless collaboration is a game-changer. Enter Databricks - a unified analytics platform built to empower teams to solve complex data challenges efficiently. Whether you're wrangling data, building machine learning models, or managing ETL pipelines, Databricks provides a powerful yet user-friendly environment to bring your ideas to life.\\n\\nIn this blog, we’ll take a guided tour of the Databricks interface, covering key features like clusters, notebooks, and jobs. Along the way, we’ll demonstrate how to:\\n\\n- Create your first cluster for processing data.\\n- Analyze datasets using interactive notebooks.\\n- Schedule jobs to automate your workflows.\\n\\nBy the end of this guide, you’ll not only have a strong grasp of Databricks’ core functionalities but also the confidence to explore its advanced features for your next big data project. Let’s dive in!\",\n",
       "      'md': \"In the fast-paced world of data engineering and analytics, having a platform that combines big data processing, machine learning, and seamless collaboration is a game-changer. Enter Databricks - a unified analytics platform built to empower teams to solve complex data challenges efficiently. Whether you're wrangling data, building machine learning models, or managing ETL pipelines, Databricks provides a powerful yet user-friendly environment to bring your ideas to life.\\n\\nIn this blog, we’ll take a guided tour of the Databricks interface, covering key features like clusters, notebooks, and jobs. Along the way, we’ll demonstrate how to:\\n\\n- Create your first cluster for processing data.\\n- Analyze datasets using interactive notebooks.\\n- Schedule jobs to automate your workflows.\\n\\nBy the end of this guide, you’ll not only have a strong grasp of Databricks’ core functionalities but also the confidence to explore its advanced features for your next big data project. Let’s dive in!\",\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 237.0, 'w': 537.0, 'h': 352.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'What is Databricks?',\n",
       "      'md': '# What is Databricks?',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 237.0, 'w': 386.0, 'h': 398.96}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': 'At its core, Databricks is a cloud-based platform that provides a collaborative workspace for working with big data and AI. It brings together the power of:\\n\\n- Data Engineering for building pipelines.\\n- Data Science for training machine learning models.\\n- Business Analytics for generating insights.',\n",
       "      'md': 'At its core, Databricks is a cloud-based platform that provides a collaborative workspace for working with big data and AI. It brings together the power of:\\n\\n- Data Engineering for building pipelines.\\n- Data Science for training machine learning models.\\n- Business Analytics for generating insights.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 237.0, 'w': 533.0, 'h': 513.0}}],\n",
       "    'status': 'OK',\n",
       "    'links': [],\n",
       "    'width': 612.0,\n",
       "    'height': 792.0,\n",
       "    'triggeredAutoMode': False,\n",
       "    'parsingMode': 'accurate',\n",
       "    'structuredData': None,\n",
       "    'noStructuredContent': False,\n",
       "    'noTextContent': False},\n",
       "   {'page': 2,\n",
       "    'text': 'Databricks bridges the gap between these domains, offering an end-to-end solution for data preparation,\\nmachine learning, and business intelligence.\\nHistory\\nThe story of Databricks begins in 2013, when it was founded by a group of computer science experts from the\\nUniversity of California, Berkeley      .\\nThe founding team includes Matei Zaharia, the creator of Apache Spark, alongside co-founders Ali Ghodsi,\\nReynold Xin, Ion Stoica, Patrick Wendell, and Andy Konwinski. Their mission was simple but ambitious:\\n               \"Simplify big data processing and make AI accessible to everyone.\"\\nSince then, Databricks has grown into a leading analytics platform, trusted by global giants like Microsoft,\\nAmazon, and Google.\\nImportance of Databricks in Modern Workflows\\nIn today’s fast-paced world of data, traditional tools often struggle to handle the volume, velocity, and variety\\nof information. Databricks solves this by:\\n   •       Accelerating ETL processes, making data pipeline development more efficient.\\n   •       Enabling collaboration across teams, no matter where they are.\\n   •       Streamlining the entire data-to-AI lifecycle, from raw data to actionable insights.\\nIt has become a cornerstone in industries like finance, healthcare, and retail for real-time data processing and\\nAI-powered decision-making. Moreover, Databricks is built on Apache Spark, the distributed computing\\nframework known for its speed and scalability. This integration enables:\\n   •   Real-time data streaming.\\n   •   Batch processing of massive datasets.\\n   •   Advanced machine learning model training.\\nWith Spark at its heart, it delivers unmatched performance, making it the go-to choice for modern data-driven\\nworkflows.\\nNow, lets explore some of the key use-cases for this platform:\\n   1.  ETL (Extract, Transform, Load): Simplify data ingestion and transformation using Apache Spark for\\n       high-performance processing.\\n   2.  Machine Learning: Train and deploy scalable ML models directly within the platform.\\n   3.  Big Data Analytics: Run powerful SQL queries and generate visualizations to uncover trends in\\n       massive datasets.',\n",
       "    'md': '# Databricks bridges the gap between these domains, offering an end-to-end solution for data preparation, machine learning, and business intelligence.\\n\\n# History\\n\\nThe story of Databricks begins in 2013, when it was founded by a group of computer science experts from the University of California, Berkeley. The founding team includes Matei Zaharia, the creator of Apache Spark, alongside co-founders Ali Ghodsi, Reynold Xin, Ion Stoica, Patrick Wendell, and Andy Konwinski. Their mission was simple but ambitious: \"Simplify big data processing and make AI accessible to everyone.\" Since then, Databricks has grown into a leading analytics platform, trusted by global giants like Microsoft, Amazon, and Google.\\n\\n# Importance of Databricks in Modern Workflows\\n\\nIn today’s fast-paced world of data, traditional tools often struggle to handle the volume, velocity, and variety of information. Databricks solves this by:\\n\\n- Accelerating ETL processes, making data pipeline development more efficient.\\n- Enabling collaboration across teams, no matter where they are.\\n- Streamlining the entire data-to-AI lifecycle, from raw data to actionable insights.\\n\\nIt has become a cornerstone in industries like finance, healthcare, and retail for real-time data processing and AI-powered decision-making. Moreover, Databricks is built on Apache Spark, the distributed computing framework known for its speed and scalability. This integration enables:\\n\\n- Real-time data streaming.\\n- Batch processing of massive datasets.\\n- Advanced machine learning model training.\\n\\nWith Spark at its heart, it delivers unmatched performance, making it the go-to choice for modern data-driven workflows.\\n\\n# Key Use-Cases\\n\\n1. ETL (Extract, Transform, Load): Simplify data ingestion and transformation using Apache Spark for high-performance processing.\\n2. Machine Learning: Train and deploy scalable ML models directly within the platform.\\n3. Big Data Analytics: Run powerful SQL queries and generate visualizations to uncover trends in massive datasets.',\n",
       "    'images': [],\n",
       "    'charts': [],\n",
       "    'tables': [],\n",
       "    'layout': [],\n",
       "    'items': [{'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Databricks bridges the gap between these domains, offering an end-to-end solution for data preparation, machine learning, and business intelligence.',\n",
       "      'md': '# Databricks bridges the gap between these domains, offering an end-to-end solution for data preparation, machine learning, and business intelligence.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 36.0, 'w': 508.0, 'h': 114.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'History',\n",
       "      'md': '# History',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 95.96, 'w': 49.0, 'h': 14.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': 'The story of Databricks begins in 2013, when it was founded by a group of computer science experts from the University of California, Berkeley. The founding team includes Matei Zaharia, the creator of Apache Spark, alongside co-founders Ali Ghodsi, Reynold Xin, Ion Stoica, Patrick Wendell, and Andy Konwinski. Their mission was simple but ambitious: \"Simplify big data processing and make AI accessible to everyone.\" Since then, Databricks has grown into a leading analytics platform, trusted by global giants like Microsoft, Amazon, and Google.',\n",
       "      'md': 'The story of Databricks begins in 2013, when it was founded by a group of computer science experts from the University of California, Berkeley. The founding team includes Matei Zaharia, the creator of Apache Spark, alongside co-founders Ali Ghodsi, Reynold Xin, Ion Stoica, Patrick Wendell, and Andy Konwinski. Their mission was simple but ambitious: \"Simplify big data processing and make AI accessible to everyone.\" Since then, Databricks has grown into a leading analytics platform, trusted by global giants like Microsoft, Amazon, and Google.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 121.0, 'w': 528.0, 'h': 123.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Importance of Databricks in Modern Workflows',\n",
       "      'md': '# Importance of Databricks in Modern Workflows',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 276.96, 'w': 294.0, 'h': 14.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': 'In today’s fast-paced world of data, traditional tools often struggle to handle the volume, velocity, and variety of information. Databricks solves this by:\\n\\n- Accelerating ETL processes, making data pipeline development more efficient.\\n- Enabling collaboration across teams, no matter where they are.\\n- Streamlining the entire data-to-AI lifecycle, from raw data to actionable insights.\\n\\nIt has become a cornerstone in industries like finance, healthcare, and retail for real-time data processing and AI-powered decision-making. Moreover, Databricks is built on Apache Spark, the distributed computing framework known for its speed and scalability. This integration enables:\\n\\n- Real-time data streaming.\\n- Batch processing of massive datasets.\\n- Advanced machine learning model training.\\n\\nWith Spark at its heart, it delivers unmatched performance, making it the go-to choice for modern data-driven workflows.',\n",
       "      'md': 'In today’s fast-paced world of data, traditional tools often struggle to handle the volume, velocity, and variety of information. Databricks solves this by:\\n\\n- Accelerating ETL processes, making data pipeline development more efficient.\\n- Enabling collaboration across teams, no matter where they are.\\n- Streamlining the entire data-to-AI lifecycle, from raw data to actionable insights.\\n\\nIt has become a cornerstone in industries like finance, healthcare, and retail for real-time data processing and AI-powered decision-making. Moreover, Databricks is built on Apache Spark, the distributed computing framework known for its speed and scalability. This integration enables:\\n\\n- Real-time data streaming.\\n- Batch processing of massive datasets.\\n- Advanced machine learning model training.\\n\\nWith Spark at its heart, it delivers unmatched performance, making it the go-to choice for modern data-driven workflows.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 138.0, 'w': 537.0, 'h': 577.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Key Use-Cases',\n",
       "      'md': '# Key Use-Cases',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 0.0, 'y': 0.0, 'w': 612.0, 'h': 792.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '1. ETL (Extract, Transform, Load): Simplify data ingestion and transformation using Apache Spark for high-performance processing.\\n2. Machine Learning: Train and deploy scalable ML models directly within the platform.\\n3. Big Data Analytics: Run powerful SQL queries and generate visualizations to uncover trends in massive datasets.',\n",
       "      'md': '1. ETL (Extract, Transform, Load): Simplify data ingestion and transformation using Apache Spark for high-performance processing.\\n2. Machine Learning: Train and deploy scalable ML models directly within the platform.\\n3. Big Data Analytics: Run powerful SQL queries and generate visualizations to uncover trends in massive datasets.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 54.0, 'y': 138.0, 'w': 518.0, 'h': 577.0}}],\n",
       "    'status': 'OK',\n",
       "    'links': [],\n",
       "    'width': 612.0,\n",
       "    'height': 792.0,\n",
       "    'triggeredAutoMode': False,\n",
       "    'parsingMode': 'accurate',\n",
       "    'structuredData': None,\n",
       "    'noStructuredContent': False,\n",
       "    'noTextContent': False},\n",
       "   {'page': 3,\n",
       "    'text': 'Now, let’s explore the core behind Databricks!\\nUnderstanding Apache Spark: The Heart of Databricks\\n        Apache Spark is a powerful and versatile distributed computing platform designed to process vast\\namounts of data efficiently. It serves as the backbone of Databricks, enabling high-performance data processing\\nand advanced analytics. Let’s explore what makes Apache Spark an essential tool for modern data workflows.\\nKey Features of Apache Spark\\n    1.  Distributed Computing Platform\\n           o   Spark excels at processing large datasets by distributing tasks across a cluster of machines. This\\n               parallelism ensures faster computation, even with petabytes of data.\\n    2.  In-Memory Processing Engine\\n           o   Unlike traditional systems that rely on disk I/O, Spark performs most of its operations in-\\n               memory. This dramatically reduces latency and accelerates workflows.\\n    3.  Unified Engine\\n           o   Spark supports multiple processing paradigms, making it a one-stop solution for diverse data\\n               tasks:\\n                  ▪   SQL Queries: Analyze structured data efficiently.\\n                  ▪   Streaming: Handle real-time data streams for time-sensitive insights.\\n                  ▪   Machine Learning: Train scalable ML models with its MLlib library.\\n                  ▪   Graph Processing: Analyze relationships and patterns in large networks.\\nApache Spark Architecture\\nTo truly appreciate Spark’s capabilities, let’s take a look at its architecture:\\n                                     Spark          Spark ML        Spark Graph\\n                   Spark           Streaming\\n                    SQL\\n                                            DataFrame     Dataset APIs\\n                                       Spark SQL Engine\\n                    Catalyst Optimizer                        Tungsten\\n                                           Spark Core\\n                   Scala            Python             Java\\n                               Resilient Distributed Dataset (RDD)\\n                     Spark Standalone, YARN, Apache Mesos, Kubernetes',\n",
       "    'md': '# Now, let’s explore the core behind Databricks!\\n\\n# Understanding Apache Spark: The Heart of Databricks\\n\\nApache Spark is a powerful and versatile distributed computing platform designed to process vast amounts of data efficiently. It serves as the backbone of Databricks, enabling high-performance data processing and advanced analytics. Let’s explore what makes Apache Spark an essential tool for modern data workflows.\\n\\n# Key Features of Apache Spark\\n\\n1. Distributed Computing Platform\\n- Spark excels at processing large datasets by distributing tasks across a cluster of machines. This parallelism ensures faster computation, even with petabytes of data.\\n2. In-Memory Processing Engine\\n- Unlike traditional systems that rely on disk I/O, Spark performs most of its operations in-memory. This dramatically reduces latency and accelerates workflows.\\n3. Unified Engine\\n- Spark supports multiple processing paradigms, making it a one-stop solution for diverse data tasks:\\n\\n# Apache Spark Architecture\\n\\nTo truly appreciate Spark’s capabilities, let’s take a look at its architecture:\\n\\n| Spark                                            | Spark SQL | Spark Streaming | Spark ML         | Spark Graph                         |          |\\n| ------------------------------------------------ | --------- | --------------- | ---------------- | ----------------------------------- | -------- |\\n|                                                  | DataFrame | Dataset APIs    | Spark SQL Engine | Catalyst Optimizer                  | Tungsten |\\n| Spark Core                                       | Scala     | Python          | Java             | Resilient Distributed Dataset (RDD) |          |\\n| Spark Standalone, YARN, Apache Mesos, Kubernetes |           |                 |                  |                                     |          |\\n',\n",
       "    'images': [{'name': 'img_p2_1.png',\n",
       "      'height': 619.0,\n",
       "      'width': 731.0,\n",
       "      'x': 85.6,\n",
       "      'y': 485.33099999999996,\n",
       "      'original_width': 731,\n",
       "      'original_height': 619,\n",
       "      'type': None}],\n",
       "    'charts': [],\n",
       "    'tables': [],\n",
       "    'layout': [],\n",
       "    'items': [{'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Now, let’s explore the core behind Databricks!',\n",
       "      'md': '# Now, let’s explore the core behind Databricks!',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 36.0, 'w': 227.0, 'h': 290.04}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Understanding Apache Spark: The Heart of Databricks',\n",
       "      'md': '# Understanding Apache Spark: The Heart of Databricks',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 60.96, 'w': 332.0, 'h': 466.04}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': 'Apache Spark is a powerful and versatile distributed computing platform designed to process vast amounts of data efficiently. It serves as the backbone of Databricks, enabling high-performance data processing and advanced analytics. Let’s explore what makes Apache Spark an essential tool for modern data workflows.',\n",
       "      'md': 'Apache Spark is a powerful and versatile distributed computing platform designed to process vast amounts of data efficiently. It serves as the backbone of Databricks, enabling high-performance data processing and advanced analytics. Let’s explore what makes Apache Spark an essential tool for modern data workflows.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 87.0, 'w': 535.0, 'h': 440.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Key Features of Apache Spark',\n",
       "      'md': '# Key Features of Apache Spark',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 139.0, 'w': 216.0, 'h': 388.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '1. Distributed Computing Platform\\n- Spark excels at processing large datasets by distributing tasks across a cluster of machines. This parallelism ensures faster computation, even with petabytes of data.\\n2. In-Memory Processing Engine\\n- Unlike traditional systems that rely on disk I/O, Spark performs most of its operations in-memory. This dramatically reduces latency and accelerates workflows.\\n3. Unified Engine\\n- Spark supports multiple processing paradigms, making it a one-stop solution for diverse data tasks:',\n",
       "      'md': '1. Distributed Computing Platform\\n- Spark excels at processing large datasets by distributing tasks across a cluster of machines. This parallelism ensures faster computation, even with petabytes of data.\\n2. In-Memory Processing Engine\\n- Unlike traditional systems that rely on disk I/O, Spark performs most of its operations in-memory. This dramatically reduces latency and accelerates workflows.\\n3. Unified Engine\\n- Spark supports multiple processing paradigms, making it a one-stop solution for diverse data tasks:',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 54.0, 'y': 164.0, 'w': 514.0, 'h': 363.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Apache Spark Architecture',\n",
       "      'md': '# Apache Spark Architecture',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 443.0, 'w': 216.0, 'h': 84.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': 'To truly appreciate Spark’s capabilities, let’s take a look at its architecture:',\n",
       "      'md': 'To truly appreciate Spark’s capabilities, let’s take a look at its architecture:',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 189.04, 'w': 360.0, 'h': 337.96}},\n",
       "     {'type': 'table',\n",
       "      'lvl': None,\n",
       "      'value': None,\n",
       "      'md': '| Spark                                            | Spark SQL | Spark Streaming | Spark ML         | Spark Graph                         |          |\\n| ------------------------------------------------ | --------- | --------------- | ---------------- | ----------------------------------- | -------- |\\n|                                                  | DataFrame | Dataset APIs    | Spark SQL Engine | Catalyst Optimizer                  | Tungsten |\\n| Spark Core                                       | Scala     | Python          | Java             | Resilient Distributed Dataset (RDD) |          |\\n| Spark Standalone, YARN, Apache Mesos, Kubernetes |           |                 |                  |                                     |          |',\n",
       "      'rows': [['Spark',\n",
       "        'Spark SQL',\n",
       "        'Spark Streaming',\n",
       "        'Spark ML',\n",
       "        'Spark Graph',\n",
       "        ''],\n",
       "       ['',\n",
       "        'DataFrame',\n",
       "        'Dataset APIs',\n",
       "        'Spark SQL Engine',\n",
       "        'Catalyst Optimizer',\n",
       "        'Tungsten'],\n",
       "       ['Spark Core',\n",
       "        'Scala',\n",
       "        'Python',\n",
       "        'Java',\n",
       "        'Resilient Distributed Dataset (RDD)',\n",
       "        ''],\n",
       "       ['Spark Standalone, YARN, Apache Mesos, Kubernetes',\n",
       "        '',\n",
       "        '',\n",
       "        '',\n",
       "        '',\n",
       "        '']],\n",
       "      'bBox': {'x': 36.0, 'y': 36.0, 'w': 535.0, 'h': 670.0}}],\n",
       "    'status': 'OK',\n",
       "    'links': [],\n",
       "    'width': 612.0,\n",
       "    'height': 792.0,\n",
       "    'triggeredAutoMode': False,\n",
       "    'parsingMode': 'accurate',\n",
       "    'structuredData': None,\n",
       "    'noStructuredContent': False,\n",
       "    'noTextContent': False},\n",
       "   {'page': 4,\n",
       "    'text': 'The diagram illustrates the core components of Spark, including the Driver, Executors, and Cluster Manager.\\nThis architecture highlights its ability to process data at scale with remarkable speed and flexibility.\\nNow, let’s dive into the navigation panel!\\nGuide to the Navigation Panel\\nWhether you’re a data engineer, data scientist, or someone diving into the world of analytics, the navigation\\npanel on the left is your go-to toolkit. Let me guide you through each tab and what it offers, so you can hit the\\nground running with confidence.\\n     Microsoft Azure  databricks                                                                                                                                  databricks-WS\\n    New                                                                                 Welcome to Databricks\\n    Workspace                                                                Search datanotebooks recents and more;    CTRL\\n    Recents\\n    Catalog                                     Recents  Favorites  Popular  Mosaic    What $ new\\n    Workflows\\n    Compute\\n  SQL                                                                                       Start your journey\\n                                                                                 Try the \"New\" menu; where you can upload or connect to data and\\n    SQL Editor                                                                         then explore it innotebook or dashboard;\\n    Queries                                                                                        New\\n    Dashboards\\n    Genie\\n    Alerts\\n    Query History\\n    SQL Warehouses\\n  Data Engineering\\n    Job Runs\\n  81Data Ingestion\\n    Delta Live Tables\\n  Machine Learning\\n    Playground\\n    Experiments\\n    Features\\n    Models\\n    Serving\\n  23Marketplace\\n    Partner Connect\\n1. Workspace\\nThink of this as your project’s home base. You can organize and access your notebooks, datasets, and libraries\\nhere. Need to collaborate with teammates? Share folders or resources effortlessly right within the workspace.\\nIt’s all about keeping your work organized and easy to find!\\n2. Recents\\nLost track of what you worked on yesterday? No worries! The Recents tab is your shortcut to all the notebooks,\\ndashboards, and datasets you’ve recently accessed. It’s like your personalized time machine for productivity.\\n3. Catalog\\nThis is where the Unity Catalog shines! It’s all about data governance—manage your tables, schemas, and\\ndatabases with ease. Plus, it ensures your data is secure, traceable, and compliant. If you’re working with\\nsensitive or large-scale data, this is your best friend.\\n4. Workflows',\n",
       "    'md': '# Guide to the Navigation Panel\\n\\nWhether you’re a data engineer, data scientist, or someone diving into the world of analytics, the navigation panel on the left is your go-to toolkit. Let me guide you through each tab and what it offers, so you can hit the ground running with confidence.\\n\\n# Microsoft Azure databricks\\n\\nWelcome to Databricks\\n\\n- New\\n- Workspace\\n- Recents\\n- Catalog\\n- Workflows\\n- Compute\\n- SQL\\n- Data Engineering\\n- Machine Learning\\n- Marketplace\\n\\n# SQL\\n\\n- SQL Editor\\n- Queries\\n- Dashboards\\n- Genie\\n- Alerts\\n- Query History\\n- SQL Warehouses\\n\\n# Data Engineering\\n\\n- Job Runs\\n- Data Ingestion\\n- Delta Live Tables\\n\\n# Machine Learning\\n\\n- Playground\\n- Experiments\\n- Features\\n- Models\\n- Serving\\n\\n# 1. Workspace\\n\\nThink of this as your project’s home base. You can organize and access your notebooks, datasets, and libraries here. Need to collaborate with teammates? Share folders or resources effortlessly right within the workspace. It’s all about keeping your work organized and easy to find!\\n\\n# 2. Recents\\n\\nLost track of what you worked on yesterday? No worries! The Recents tab is your shortcut to all the notebooks, dashboards, and datasets you’ve recently accessed. It’s like your personalized time machine for productivity.\\n\\n# 3. Catalog\\n\\nThis is where the Unity Catalog shines! It’s all about data governance—manage your tables, schemas, and databases with ease. Plus, it ensures your data is secure, traceable, and compliant. If you’re working with sensitive or large-scale data, this is your best friend.\\n\\n# 4. Workflows',\n",
       "    'images': [{'name': 'img_p3_1.png',\n",
       "      'height': 950.0,\n",
       "      'width': 1917.0,\n",
       "      'x': 36.0,\n",
       "      'y': 197.60996948199994,\n",
       "      'original_width': 1917,\n",
       "      'original_height': 950,\n",
       "      'type': None}],\n",
       "    'charts': [],\n",
       "    'tables': [],\n",
       "    'layout': [],\n",
       "    'items': [{'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Guide to the Navigation Panel',\n",
       "      'md': '# Guide to the Navigation Panel',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 95.96, 'w': 183.0, 'h': 14.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': 'Whether you’re a data engineer, data scientist, or someone diving into the world of analytics, the navigation panel on the left is your go-to toolkit. Let me guide you through each tab and what it offers, so you can hit the ground running with confidence.',\n",
       "      'md': 'Whether you’re a data engineer, data scientist, or someone diving into the world of analytics, the navigation panel on the left is your go-to toolkit. Let me guide you through each tab and what it offers, so you can hit the ground running with confidence.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 122.0, 'w': 526.0, 'h': 41.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Microsoft Azure databricks',\n",
       "      'md': '# Microsoft Azure databricks',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 50.0, 'y': 201.0, 'w': 64.0, 'h': 6.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': 'Welcome to Databricks\\n\\n- New\\n- Workspace\\n- Recents\\n- Catalog\\n- Workflows\\n- Compute\\n- SQL\\n- Data Engineering\\n- Machine Learning\\n- Marketplace',\n",
       "      'md': 'Welcome to Databricks\\n\\n- New\\n- Workspace\\n- Recents\\n- Catalog\\n- Workflows\\n- Compute\\n- SQL\\n- Data Engineering\\n- Machine Learning\\n- Marketplace',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 41.0, 'y': 201.0, 'w': 326.0, 'h': 186.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'SQL',\n",
       "      'md': '# SQL',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 41.0, 'y': 276.0, 'w': 7.0, 'h': 4.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '- SQL Editor\\n- Queries\\n- Dashboards\\n- Genie\\n- Alerts\\n- Query History\\n- SQL Warehouses',\n",
       "      'md': '- SQL Editor\\n- Queries\\n- Dashboards\\n- Genie\\n- Alerts\\n- Query History\\n- SQL Warehouses',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 41.0, 'y': 276.0, 'w': 36.0, 'h': 61.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Data Engineering',\n",
       "      'md': '# Data Engineering',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 41.0, 'y': 345.0, 'w': 26.0, 'h': 5.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '- Job Runs\\n- Data Ingestion\\n- Delta Live Tables',\n",
       "      'md': '- Job Runs\\n- Data Ingestion\\n- Delta Live Tables',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 48.0, 'y': 354.0, 'w': 29.0, 'h': 20.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Machine Learning',\n",
       "      'md': '# Machine Learning',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 41.0, 'y': 382.0, 'w': 28.0, 'h': 5.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '- Playground\\n- Experiments\\n- Features\\n- Models\\n- Serving',\n",
       "      'md': '- Playground\\n- Experiments\\n- Features\\n- Models\\n- Serving',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 48.0, 'y': 391.0, 'w': 21.0, 'h': 38.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': '1. Workspace',\n",
       "      'md': '# 1. Workspace',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 229.0, 'w': 69.0, 'h': 281.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': 'Think of this as your project’s home base. You can organize and access your notebooks, datasets, and libraries here. Need to collaborate with teammates? Share folders or resources effortlessly right within the workspace. It’s all about keeping your work organized and easy to find!',\n",
       "      'md': 'Think of this as your project’s home base. You can organize and access your notebooks, datasets, and libraries here. Need to collaborate with teammates? Share folders or resources effortlessly right within the workspace. It’s all about keeping your work organized and easy to find!',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 229.0, 'w': 530.0, 'h': 334.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': '2. Recents',\n",
       "      'md': '# 2. Recents',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 238.0, 'w': 159.0, 'h': 350.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': 'Lost track of what you worked on yesterday? No worries! The Recents tab is your shortcut to all the notebooks, dashboards, and datasets you’ve recently accessed. It’s like your personalized time machine for productivity.',\n",
       "      'md': 'Lost track of what you worked on yesterday? No worries! The Recents tab is your shortcut to all the notebooks, dashboards, and datasets you’ve recently accessed. It’s like your personalized time machine for productivity.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 238.0, 'w': 538.0, 'h': 389.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': '3. Catalog',\n",
       "      'md': '# 3. Catalog',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 246.0, 'w': 52.0, 'h': 405.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': 'This is where the Unity Catalog shines! It’s all about data governance—manage your tables, schemas, and databases with ease. Plus, it ensures your data is secure, traceable, and compliant. If you’re working with sensitive or large-scale data, this is your best friend.',\n",
       "      'md': 'This is where the Unity Catalog shines! It’s all about data governance—manage your tables, schemas, and databases with ease. Plus, it ensures your data is secure, traceable, and compliant. If you’re working with sensitive or large-scale data, this is your best friend.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 246.0, 'w': 519.0, 'h': 458.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': '4. Workflows',\n",
       "      'md': '# 4. Workflows',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 254.0, 'w': 68.0, 'h': 475.0}}],\n",
       "    'status': 'OK',\n",
       "    'links': [],\n",
       "    'width': 612.0,\n",
       "    'height': 792.0,\n",
       "    'triggeredAutoMode': False,\n",
       "    'parsingMode': 'accurate',\n",
       "    'structuredData': None,\n",
       "    'noStructuredContent': False,\n",
       "    'noTextContent': False},\n",
       "   {'page': 5,\n",
       "    'text': 'Got repetitive tasks? Automate them here! Workflows allow you to schedule and monitor jobs like ETL\\npipelines or model training. No need to babysit your tasks—set them up and let them run like clockwork.\\n5. Compute\\nHere’s the engine that powers all your data crunching. Use this tab to manage clusters—create, configure, and\\nmonitor them as needed. Whether you’re running small experiments or large-scale data processing, you can\\noptimize performance and manage costs here.\\nSQL: The Heart of Analytics\\n6. SQL Editor\\nIf you’re into querying data, this is your playground. Write SQL queries, explore datasets, and extract\\nmeaningful insights.\\n7. Queries, Dashboards, and Alerts\\n   •   Queries: Save your favorite queries or revisit past ones.\\n   •   Dashboards: Turn your results into interactive visuals—perfect for presentations or team sharing.\\n   •   Alerts: Need to be notified about unusual trends or thresholds? Set up alerts to stay ahead of the game.\\nData Engineering: Build Like a Pro\\n8. Job Runs\\nWondering if your data pipeline finished running? This is the tab where you track all your jobs—running,\\ncompleted, or failed.\\n9. Data Ingestion\\nBringing data into Databricks is seamless here. Batch or streaming, this tab simplifies how you get your data\\nready for action.\\n10. Delta Live Tables\\nThis is the next-gen ETL experience! With Delta Live Tables, you can build robust data pipelines that adapt to\\nchanges automatically.\\nMachine Learning: For the Innovators\\n11. Playground and Experiments\\nWant to tinker with machine learning models? The Playground is perfect for experiments. Keep track of every\\nparameter and result in the Experiments tab—it’s your lab notebook!\\n12. Features, Models, and Serving\\n   •   Features: Centralize reusable features for model building.\\n   •   Models: Organize, version, and manage your ML models.\\n   •   Serving: Deploy your models effortlessly for real-time or batch predictions.',\n",
       "    'md': '# Got repetitive tasks? Automate them here!\\n\\nWorkflows allow you to schedule and monitor jobs like ETL pipelines or model training. No need to babysit your tasks—set them up and let them run like clockwork.\\n\\n# 5. Compute\\n\\nHere’s the engine that powers all your data crunching. Use this tab to manage clusters—create, configure, and monitor them as needed. Whether you’re running small experiments or large-scale data processing, you can optimize performance and manage costs here.\\n\\n# SQL: The Heart of Analytics\\n\\n# 6. SQL Editor\\n\\nIf you’re into querying data, this is your playground. Write SQL queries, explore datasets, and extract meaningful insights.\\n\\n# 7. Queries, Dashboards, and Alerts\\n\\n- Queries: Save your favorite queries or revisit past ones.\\n- Dashboards: Turn your results into interactive visuals—perfect for presentations or team sharing.\\n- Alerts: Need to be notified about unusual trends or thresholds? Set up alerts to stay ahead of the game.\\n\\n# Data Engineering: Build Like a Pro\\n\\n# 8. Job Runs\\n\\nWondering if your data pipeline finished running? This is the tab where you track all your jobs—running, completed, or failed.\\n\\n# 9. Data Ingestion\\n\\nBringing data into Databricks is seamless here. Batch or streaming, this tab simplifies how you get your data ready for action.\\n\\n# 10. Delta Live Tables\\n\\nThis is the next-gen ETL experience! With Delta Live Tables, you can build robust data pipelines that adapt to changes automatically.\\n\\n# Machine Learning: For the Innovators\\n\\n# 11. Playground and Experiments\\n\\nWant to tinker with machine learning models? The Playground is perfect for experiments. Keep track of every parameter and result in the Experiments tab—it’s your lab notebook!\\n\\n# 12. Features, Models, and Serving\\n\\n- Features: Centralize reusable features for model building.\\n- Models: Organize, version, and manage your ML models.\\n- Serving: Deploy your models effortlessly for real-time or batch predictions.',\n",
       "    'images': [],\n",
       "    'charts': [],\n",
       "    'tables': [],\n",
       "    'layout': [],\n",
       "    'items': [{'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Got repetitive tasks? Automate them here!',\n",
       "      'md': '# Got repetitive tasks? Automate them here!',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 0.0, 'y': 0.0, 'w': 612.0, 'h': 792.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': 'Workflows allow you to schedule and monitor jobs like ETL pipelines or model training. No need to babysit your tasks—set them up and let them run like clockwork.',\n",
       "      'md': 'Workflows allow you to schedule and monitor jobs like ETL pipelines or model training. No need to babysit your tasks—set them up and let them run like clockwork.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 51.0, 'w': 507.0, 'h': 12.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': '5. Compute',\n",
       "      'md': '# 5. Compute',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 98.0, 'w': 59.0, 'h': 12.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': 'Here’s the engine that powers all your data crunching. Use this tab to manage clusters—create, configure, and monitor them as needed. Whether you’re running small experiments or large-scale data processing, you can optimize performance and manage costs here.',\n",
       "      'md': 'Here’s the engine that powers all your data crunching. Use this tab to manage clusters—create, configure, and monitor them as needed. Whether you’re running small experiments or large-scale data processing, you can optimize performance and manage costs here.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 122.0, 'w': 527.0, 'h': 41.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'SQL: The Heart of Analytics',\n",
       "      'md': '# SQL: The Heart of Analytics',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 174.0, 'w': 150.0, 'h': 12.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': '6. SQL Editor',\n",
       "      'md': '# 6. SQL Editor',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 199.0, 'w': 72.0, 'h': 12.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': 'If you’re into querying data, this is your playground. Write SQL queries, explore datasets, and extract meaningful insights.',\n",
       "      'md': 'If you’re into querying data, this is your playground. Write SQL queries, explore datasets, and extract meaningful insights.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 222.0, 'w': 487.0, 'h': 27.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': '7. Queries, Dashboards, and Alerts',\n",
       "      'md': '# 7. Queries, Dashboards, and Alerts',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 260.0, 'w': 181.0, 'h': 12.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '- Queries: Save your favorite queries or revisit past ones.\\n- Dashboards: Turn your results into interactive visuals—perfect for presentations or team sharing.\\n- Alerts: Need to be notified about unusual trends or thresholds? Set up alerts to stay ahead of the game.',\n",
       "      'md': '- Queries: Save your favorite queries or revisit past ones.\\n- Dashboards: Turn your results into interactive visuals—perfect for presentations or team sharing.\\n- Alerts: Need to be notified about unusual trends or thresholds? Set up alerts to stay ahead of the game.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 72.0, 'y': 283.0, 'w': 496.0, 'h': 58.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Data Engineering: Build Like a Pro',\n",
       "      'md': '# Data Engineering: Build Like a Pro',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 375.0, 'w': 185.0, 'h': 12.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': '8. Job Runs',\n",
       "      'md': '# 8. Job Runs',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 399.0, 'w': 60.0, 'h': 12.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': 'Wondering if your data pipeline finished running? This is the tab where you track all your jobs—running, completed, or failed.',\n",
       "      'md': 'Wondering if your data pipeline finished running? This is the tab where you track all your jobs—running, completed, or failed.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 423.0, 'w': 506.0, 'h': 27.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': '9. Data Ingestion',\n",
       "      'md': '# 9. Data Ingestion',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 462.0, 'w': 87.0, 'h': 12.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': 'Bringing data into Databricks is seamless here. Batch or streaming, this tab simplifies how you get your data ready for action.',\n",
       "      'md': 'Bringing data into Databricks is seamless here. Batch or streaming, this tab simplifies how you get your data ready for action.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 486.0, 'w': 521.0, 'h': 27.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': '10. Delta Live Tables',\n",
       "      'md': '# 10. Delta Live Tables',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 525.0, 'w': 107.0, 'h': 12.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': 'This is the next-gen ETL experience! With Delta Live Tables, you can build robust data pipelines that adapt to changes automatically.',\n",
       "      'md': 'This is the next-gen ETL experience! With Delta Live Tables, you can build robust data pipelines that adapt to changes automatically.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 549.0, 'w': 533.0, 'h': 27.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Machine Learning: For the Innovators',\n",
       "      'md': '# Machine Learning: For the Innovators',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 587.0, 'w': 201.0, 'h': 12.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': '11. Playground and Experiments',\n",
       "      'md': '# 11. Playground and Experiments',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 611.0, 'w': 168.0, 'h': 12.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': 'Want to tinker with machine learning models? The Playground is perfect for experiments. Keep track of every parameter and result in the Experiments tab—it’s your lab notebook!',\n",
       "      'md': 'Want to tinker with machine learning models? The Playground is perfect for experiments. Keep track of every parameter and result in the Experiments tab—it’s your lab notebook!',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 635.0, 'w': 528.0, 'h': 27.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': '12. Features, Models, and Serving',\n",
       "      'md': '# 12. Features, Models, and Serving',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 672.0, 'w': 176.0, 'h': 12.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '- Features: Centralize reusable features for model building.\\n- Models: Organize, version, and manage your ML models.\\n- Serving: Deploy your models effortlessly for real-time or batch predictions.',\n",
       "      'md': '- Features: Centralize reusable features for model building.\\n- Models: Organize, version, and manage your ML models.\\n- Serving: Deploy your models effortlessly for real-time or batch predictions.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 72.0, 'y': 695.0, 'w': 368.0, 'h': 58.0}}],\n",
       "    'status': 'OK',\n",
       "    'links': [],\n",
       "    'width': 612.0,\n",
       "    'height': 792.0,\n",
       "    'triggeredAutoMode': False,\n",
       "    'parsingMode': 'accurate',\n",
       "    'structuredData': None,\n",
       "    'noStructuredContent': False,\n",
       "    'noTextContent': False},\n",
       "   {'page': 6,\n",
       "    'text': \"Other Tabs\\n13. Marketplace\\nNeed additional datasets or tools? The Marketplace connects you to third-party integrations to expand your\\npossibilities.\\n14. Partner Connect\\nQuickly link Databricks to external tools like BI platforms or ETL services. It’s integration made easy!\\n   Pro Tip: Start with exploring the Workspace and Catalog tabs if you’re new. As you dive deeper, tabs like\\nWorkflows and Compute will become part of your daily routine.\\nNow, Let's Explore Each Tab One by One!\\nTo truly understand the magic of Databricks, it’s important to dive into the details. Let’s start with the Compute\\ntab, where you’ll manage your clusters—the backbone of your Databricks environment.\\nClusters are the engines that power everything in Databricks. Whether you’re running a quick query, training a\\nmachine learning model, or processing terabytes of data, clusters are where the heavy lifting happens.\\nWhat is a Cluster?\\nA cluster is a group of virtual machines (VMs) that work together to process your data. Databricks uses these\\nclusters to distribute tasks across multiple nodes, giving you faster results and scalability.\\nWhy Are Clusters Important?\\n   •   Scalability: Need more power? Scale your cluster up or down based on your workload.\\n   •   Cost Efficiency: Only pay for what you use! Clusters can auto-terminate when idle to save costs.\\n   •   Flexibility: Different workloads? No problem! Create clusters tailored to SQL queries, data engineering\\n       pipelines, or machine learning tasks.\\nCluster Types:\\nDatabricks offers different cluster types, each designed to meet specific needs based on the workload\\nrequirements. Here's an overview of the two primary types of clusters: All Purpose and Job Cluster.\\n          Cluster Type                       All Purpose Cluster                        Job Cluster\\nCreation                              Created manually by the user          Created by jobs\\nCost                                  Cheaper to run                        Expensive to run\\nIsolation                             Isolated for specific tasks or jobs   Shared among many users\\nWorkload Suitability                  Suitable for interactive and          Suitable for automated, production\\n                                      automated workloads                   workloads\\nPersistence                           Terminated at the end of the job      Persistent until job completion\",\n",
       "    'md': \"# Other Tabs\\n\\n# 13. Marketplace\\n\\nNeed additional datasets or tools? The Marketplace connects you to third-party integrations to expand your possibilities.\\n\\n# 14. Partner Connect\\n\\nQuickly link Databricks to external tools like BI platforms or ETL services. It’s integration made easy!\\n\\nPro Tip: Start with exploring the Workspace and Catalog tabs if you’re new. As you dive deeper, tabs like Workflows and Compute will become part of your daily routine.\\n\\n# Now, Let's Explore Each Tab One by One!\\n\\nTo truly understand the magic of Databricks, it’s important to dive into the details. Let’s start with the Compute tab, where you’ll manage your clusters—the backbone of your Databricks environment.\\n\\n# What is a Cluster?\\n\\nA cluster is a group of virtual machines (VMs) that work together to process your data. Databricks uses these clusters to distribute tasks across multiple nodes, giving you faster results and scalability.\\n\\n# Why Are Clusters Important?\\n\\n- Scalability: Need more power? Scale your cluster up or down based on your workload.\\n- Cost Efficiency: Only pay for what you use! Clusters can auto-terminate when idle to save costs.\\n- Flexibility: Different workloads? No problem! Create clusters tailored to SQL queries, data engineering pipelines, or machine learning tasks.\\n\\n# Cluster Types:\\n\\nDatabricks offers different cluster types, each designed to meet specific needs based on the workload requirements. Here's an overview of the two primary types of clusters: All Purpose and Job Cluster.\\n\\n| Cluster Type         | All Purpose Cluster                              | Job Cluster                                  |\\n| -------------------- | ------------------------------------------------ | -------------------------------------------- |\\n| Creation             | Created manually by the user                     | Created by jobs                              |\\n| Cost                 | Cheaper to run                                   | Expensive to run                             |\\n| Isolation            | Isolated for specific tasks or jobs              | Shared among many users                      |\\n| Workload Suitability | Suitable for interactive and automated workloads | Suitable for automated, production workloads |\\n| Persistence          | Terminated at the end of the job                 | Persistent until job completion              |\\n\",\n",
       "    'images': [],\n",
       "    'charts': [],\n",
       "    'tables': [],\n",
       "    'layout': [],\n",
       "    'items': [{'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Other Tabs',\n",
       "      'md': '# Other Tabs',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 36.0, 'w': 60.0, 'h': 12.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': '13. Marketplace',\n",
       "      'md': '# 13. Marketplace',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 60.0, 'w': 83.0, 'h': 12.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': 'Need additional datasets or tools? The Marketplace connects you to third-party integrations to expand your possibilities.',\n",
       "      'md': 'Need additional datasets or tools? The Marketplace connects you to third-party integrations to expand your possibilities.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 84.0, 'w': 513.0, 'h': 27.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': '14. Partner Connect',\n",
       "      'md': '# 14. Partner Connect',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 123.0, 'w': 103.0, 'h': 12.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': 'Quickly link Databricks to external tools like BI platforms or ETL services. It’s integration made easy!\\n\\nPro Tip: Start with exploring the Workspace and Catalog tabs if you’re new. As you dive deeper, tabs like Workflows and Compute will become part of your daily routine.',\n",
       "      'md': 'Quickly link Databricks to external tools like BI platforms or ETL services. It’s integration made easy!\\n\\nPro Tip: Start with exploring the Workspace and Catalog tabs if you’re new. As you dive deeper, tabs like Workflows and Compute will become part of your daily routine.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 147.0, 'w': 524.0, 'h': 52.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': \"Now, Let's Explore Each Tab One by One!\",\n",
       "      'md': \"# Now, Let's Explore Each Tab One by One!\",\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 232.0, 'w': 217.0, 'h': 12.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': 'To truly understand the magic of Databricks, it’s important to dive into the details. Let’s start with the Compute tab, where you’ll manage your clusters—the backbone of your Databricks environment.',\n",
       "      'md': 'To truly understand the magic of Databricks, it’s important to dive into the details. Let’s start with the Compute tab, where you’ll manage your clusters—the backbone of your Databricks environment.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 255.0, 'w': 537.0, 'h': 27.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'What is a Cluster?',\n",
       "      'md': '# What is a Cluster?',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 330.96, 'w': 115.0, 'h': 14.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': 'A cluster is a group of virtual machines (VMs) that work together to process your data. Databricks uses these clusters to distribute tasks across multiple nodes, giving you faster results and scalability.',\n",
       "      'md': 'A cluster is a group of virtual machines (VMs) that work together to process your data. Databricks uses these clusters to distribute tasks across multiple nodes, giving you faster results and scalability.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 356.0, 'w': 525.0, 'h': 27.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Why Are Clusters Important?',\n",
       "      'md': '# Why Are Clusters Important?',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 394.0, 'w': 157.0, 'h': 12.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '- Scalability: Need more power? Scale your cluster up or down based on your workload.\\n- Cost Efficiency: Only pay for what you use! Clusters can auto-terminate when idle to save costs.\\n- Flexibility: Different workloads? No problem! Create clusters tailored to SQL queries, data engineering pipelines, or machine learning tasks.',\n",
       "      'md': '- Scalability: Need more power? Scale your cluster up or down based on your workload.\\n- Cost Efficiency: Only pay for what you use! Clusters can auto-terminate when idle to save costs.\\n- Flexibility: Different workloads? No problem! Create clusters tailored to SQL queries, data engineering pipelines, or machine learning tasks.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 35.0, 'y': 417.0, 'w': 536.0, 'h': 270.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Cluster Types:',\n",
       "      'md': '# Cluster Types:',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 500.0, 'w': 121.0, 'h': 96.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': \"Databricks offers different cluster types, each designed to meet specific needs based on the workload requirements. Here's an overview of the two primary types of clusters: All Purpose and Job Cluster.\",\n",
       "      'md': \"Databricks offers different cluster types, each designed to meet specific needs based on the workload requirements. Here's an overview of the two primary types of clusters: All Purpose and Job Cluster.\",\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 523.0, 'w': 490.0, 'h': 73.0}},\n",
       "     {'type': 'table',\n",
       "      'lvl': None,\n",
       "      'value': None,\n",
       "      'md': '| Cluster Type         | All Purpose Cluster                              | Job Cluster                                  |\\n| -------------------- | ------------------------------------------------ | -------------------------------------------- |\\n| Creation             | Created manually by the user                     | Created by jobs                              |\\n| Cost                 | Cheaper to run                                   | Expensive to run                             |\\n| Isolation            | Isolated for specific tasks or jobs              | Shared among many users                      |\\n| Workload Suitability | Suitable for interactive and automated workloads | Suitable for automated, production workloads |\\n| Persistence          | Terminated at the end of the job                 | Persistent until job completion              |',\n",
       "      'rows': [['Cluster Type', 'All Purpose Cluster', 'Job Cluster'],\n",
       "       ['Creation', 'Created manually by the user', 'Created by jobs'],\n",
       "       ['Cost', 'Cheaper to run', 'Expensive to run'],\n",
       "       ['Isolation',\n",
       "        'Isolated for specific tasks or jobs',\n",
       "        'Shared among many users'],\n",
       "       ['Workload Suitability',\n",
       "        'Suitable for interactive and automated workloads',\n",
       "        'Suitable for automated, production workloads'],\n",
       "       ['Persistence',\n",
       "        'Terminated at the end of the job',\n",
       "        'Persistent until job completion']],\n",
       "      'bBox': {'x': 35.0, 'y': 440.0, 'w': 518.0, 'h': 270.0}}],\n",
       "    'status': 'OK',\n",
       "    'links': [],\n",
       "    'width': 612.0,\n",
       "    'height': 792.0,\n",
       "    'triggeredAutoMode': False,\n",
       "    'parsingMode': 'accurate',\n",
       "    'structuredData': None,\n",
       "    'noStructuredContent': False,\n",
       "    'noTextContent': False},\n",
       "   {'page': 7,\n",
       "    'text': \"Resource Sharing                                                              Can be used by a single user or                                               Shared among multiple users\\n                                                                              isolated for a task\\nUsage                                                                         Ideal for development, testing, and                                           Ideal for scheduled, automated jobs\\n                                                                              ad-hoc analysis\\n\\nNow let’s create a cluster to understand it better.\\nJust click on the compute tab and you will be prompted with the below shown window.\\n Compute\\n All-purpose compute  Job compute  SQL warehouses   Pools  Policies  Apps 0\\n      Filter compute you have access to                   Created by        Only pinned                                                                                               Create with Personal Compute    Create compute\\n         State =T Name                                                 Policy                         Runtime       Active memory     Active cores    Active DBU /  Source       Creator                       Notebooks\\n                                                                                                                         No compute\\n                                                                              Create compute to run workloads from your notebooks and jobs Learn more about best practices for\\n                                                                                                                   compute configuration\\n                                                                                                             Create compute\\nHere, click on the create compute button and you will see the following screen.\\n  Compute   New compute\\n  Databricks /\\n   Policy 0                                                                                                                                                                                                                    SON\\n    Unrestricted                                                                                                                                                                   Summary\\n     Multi node     Single node                                                                                                                                                    2-8 Workers      32-128 CB Memcry\\n   Access mode      Single user access                                                                                                                                                              8-32 Core:\\n    Single user       Soham Patel                                                                                                                                                   Driver          16 G3 Memory; Ccres\\n                                                                                                                                                                                   Runtime          15.4x-cala2.12\\n  Performance                                                                                                                                                                       Photon Standard_D4ds_v5 6-18 DBU/h\\n   Databricks runtime version 0\\n    Runtime: 15.4 LTS (Scala 2.12, Spark 3.5.0}\\n     Use Photon Acceleration\\n  Worker type                                           Min workers   Max workers\\n    Standard_DAds_v5              16 GB Memcry;-  Cores                                 Spot instances\\n   Driver type\\n    Same as worker                16 GB Memcry; - Cores\\n     Enable autoscaling\\n     Terminate after 120      minutes of inactivity\\n  Tags\\n   Add tags\\n    Key                                           Value                                Add\\n     Automatically added tags\\n             Advanced options\\n    Create compute      Cance\\n\\n        1.     Policy Selection:\\n                            o  Set the policy to Unrestricted (or as per your organization's policy).\\n                            o  Simplifies the user interface\\n                            o  Enables standard users to create clusters\",\n",
       "    'md': \"# Resource Sharing\\n\\nCan be used by a single user or shared among multiple users isolated for a task\\n\\n# Usage\\n\\nIdeal for development, testing, and ad-hoc analysis\\n\\nIdeal for scheduled, automated jobs\\n\\n# Now let’s create a cluster to understand it better.\\n\\nJust click on the compute tab and you will be prompted with the below shown window.\\n\\n# Compute\\n\\nAll-purpose compute Job compute SQL warehouses Pools Policies Apps 0\\n\\nFilter compute you have access to Created by Only pinned Create with Personal Compute Create compute\\n\\nState =T Name Policy Runtime Active memory Active cores Active DBU / Source Creator Notebooks\\n\\nNo compute\\n\\nCreate compute to run workloads from your notebooks and jobs Learn more about best practices for compute configuration\\n\\nCreate compute\\n\\n# Here, click on the create compute button and you will see the following screen.\\n\\n# Compute\\n\\nNew compute\\n\\nDatabricks / Policy 0\\n\\nUnrestricted\\n\\n# Summary\\n\\nMulti node Single node\\n\\n2-8 Workers 32-128 Memory\\n\\nAccess mode Single user access\\n\\nSingle user Soham Patel\\n\\nDriver 16 GB Memory; Cores\\n\\nRuntime 15.4x-cala2.12\\n\\nPerformance\\n\\nDatabricks runtime version 0\\n\\nRuntime: 15.4 LTS (Scala 2.12, Spark 3.5.0)\\n\\nUse Photon Acceleration\\n\\n# Worker type\\n\\nStandard_DAds_v5 16 GB Memory; Cores Spot instances\\n\\n# Driver type\\n\\nSame as worker 16 GB Memory; Cores\\n\\n# Options\\n\\nEnable autoscaling\\n\\nTerminate after 120 minutes of inactivity\\n\\n# Tags\\n\\nAdd tags\\n\\nKey Value Add\\n\\nAutomatically added tags\\n\\n# Advanced options\\n\\nCreate compute Cancel\\n\\n# 1. Policy Selection:\\n\\n- Set the policy to Unrestricted (or as per your organization's policy).\\n- Simplifies the user interface\\n- Enables standard users to create clusters\",\n",
       "    'images': [{'name': 'img_p6_1.png',\n",
       "      'height': 448.0,\n",
       "      'width': 1699.0,\n",
       "      'x': 36.0,\n",
       "      'y': 180.57000000000002,\n",
       "      'original_width': 1699,\n",
       "      'original_height': 448,\n",
       "      'type': None},\n",
       "     {'name': 'img_p6_2.png',\n",
       "      'height': 908.0,\n",
       "      'width': 1719.0,\n",
       "      'x': 36.0,\n",
       "      'y': 355.03,\n",
       "      'original_width': 1719,\n",
       "      'original_height': 908,\n",
       "      'type': None}],\n",
       "    'charts': [],\n",
       "    'tables': [],\n",
       "    'layout': [],\n",
       "    'items': [{'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Resource Sharing',\n",
       "      'md': '# Resource Sharing',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 35.0, 'y': 36.0, 'w': 401.0, 'h': 717.04}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': 'Can be used by a single user or shared among multiple users isolated for a task',\n",
       "      'md': 'Can be used by a single user or shared among multiple users isolated for a task',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 44.0, 'y': 36.0, 'w': 502.0, 'h': 717.04}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Usage',\n",
       "      'md': '# Usage',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 35.0, 'y': 74.0, 'w': 33.0, 'h': 12.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': 'Ideal for development, testing, and ad-hoc analysis\\n\\nIdeal for scheduled, automated jobs',\n",
       "      'md': 'Ideal for development, testing, and ad-hoc analysis\\n\\nIdeal for scheduled, automated jobs',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 90.0, 'y': 74.0, 'w': 487.0, 'h': 679.04}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Now let’s create a cluster to understand it better.',\n",
       "      'md': '# Now let’s create a cluster to understand it better.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 134.0, 'w': 232.0, 'h': 619.04}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': 'Just click on the compute tab and you will be prompted with the below shown window.',\n",
       "      'md': 'Just click on the compute tab and you will be prompted with the below shown window.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 157.0, 'w': 421.0, 'h': 596.04}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Compute',\n",
       "      'md': '# Compute',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 38.0, 'y': 182.0, 'w': 58.0, 'h': 571.04}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': 'All-purpose compute Job compute SQL warehouses Pools Policies Apps 0\\n\\nFilter compute you have access to Created by Only pinned Create with Personal Compute Create compute\\n\\nState =T Name Policy Runtime Active memory Active cores Active DBU / Source Creator Notebooks\\n\\nNo compute\\n\\nCreate compute to run workloads from your notebooks and jobs Learn more about best practices for compute configuration\\n\\nCreate compute',\n",
       "      'md': 'All-purpose compute Job compute SQL warehouses Pools Policies Apps 0\\n\\nFilter compute you have access to Created by Only pinned Create with Personal Compute Create compute\\n\\nState =T Name Policy Runtime Active memory Active cores Active DBU / Source Creator Notebooks\\n\\nNo compute\\n\\nCreate compute to run workloads from your notebooks and jobs Learn more about best practices for compute configuration\\n\\nCreate compute',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 38.0, 'y': 182.0, 'w': 534.0, 'h': 571.04}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Here, click on the create compute button and you will see the following screen.',\n",
       "      'md': '# Here, click on the create compute button and you will see the following screen.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 182.0, 'w': 536.0, 'h': 571.04}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Compute',\n",
       "      'md': '# Compute',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 38.0, 'y': 182.0, 'w': 58.0, 'h': 571.04}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': 'New compute\\n\\nDatabricks / Policy 0\\n\\nUnrestricted',\n",
       "      'md': 'New compute\\n\\nDatabricks / Policy 0\\n\\nUnrestricted',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 38.0, 'y': 182.0, 'w': 178.0, 'h': 571.04}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Summary',\n",
       "      'md': '# Summary',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 458.0, 'y': 399.0, 'w': 32.0, 'h': 10.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': 'Multi node Single node\\n\\n2-8 Workers 32-128 Memory\\n\\nAccess mode Single user access\\n\\nSingle user Soham Patel\\n\\nDriver 16 GB Memory; Cores\\n\\nRuntime 15.4x-cala2.12\\n\\nPerformance\\n\\nDatabricks runtime version 0\\n\\nRuntime: 15.4 LTS (Scala 2.12, Spark 3.5.0)\\n\\nUse Photon Acceleration',\n",
       "      'md': 'Multi node Single node\\n\\n2-8 Workers 32-128 Memory\\n\\nAccess mode Single user access\\n\\nSingle user Soham Patel\\n\\nDriver 16 GB Memory; Cores\\n\\nRuntime 15.4x-cala2.12\\n\\nPerformance\\n\\nDatabricks runtime version 0\\n\\nRuntime: 15.4 LTS (Scala 2.12, Spark 3.5.0)\\n\\nUse Photon Acceleration',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 39.0, 'y': 232.0, 'w': 487.0, 'h': 521.04}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Worker type',\n",
       "      'md': '# Worker type',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 40.0, 'y': 489.0, 'w': 56.0, 'h': 264.04}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': 'Standard_DAds_v5 16 GB Memory; Cores Spot instances',\n",
       "      'md': 'Standard_DAds_v5 16 GB Memory; Cores Spot instances',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 44.0, 'y': 499.0, 'w': 228.0, 'h': 254.04}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Driver type',\n",
       "      'md': '# Driver type',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 41.0, 'y': 427.0, 'w': 433.0, 'h': 89.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': 'Same as worker 16 GB Memory; Cores',\n",
       "      'md': 'Same as worker 16 GB Memory; Cores',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 44.0, 'y': 499.0, 'w': 108.0, 'h': 254.04}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Options',\n",
       "      'md': '# Options',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 90.0, 'y': 697.04, 'w': 6.0, 'h': 56.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': 'Enable autoscaling\\n\\nTerminate after 120 minutes of inactivity',\n",
       "      'md': 'Enable autoscaling\\n\\nTerminate after 120 minutes of inactivity',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 48.0, 'y': 533.0, 'w': 96.0, 'h': 220.04}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Tags',\n",
       "      'md': '# Tags',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 39.0, 'y': 554.0, 'w': 14.0, 'h': 8.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': 'Add tags\\n\\nKey Value Add\\n\\nAutomatically added tags',\n",
       "      'md': 'Add tags\\n\\nKey Value Add\\n\\nAutomatically added tags',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 39.0, 'y': 554.0, 'w': 211.0, 'h': 199.04}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Advanced options',\n",
       "      'md': '# Advanced options',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 49.0, 'y': 606.0, 'w': 48.0, 'h': 147.04}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': 'Create compute Cancel',\n",
       "      'md': 'Create compute Cancel',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 38.0, 'y': 182.0, 'w': 534.0, 'h': 571.04}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': '1. Policy Selection:',\n",
       "      'md': '# 1. Policy Selection:',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 54.0, 'y': 230.0, 'w': 162.0, 'h': 523.04}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': \"- Set the policy to Unrestricted (or as per your organization's policy).\\n- Simplifies the user interface\\n- Enables standard users to create clusters\",\n",
       "      'md': \"- Set the policy to Unrestricted (or as per your organization's policy).\\n- Simplifies the user interface\\n- Enables standard users to create clusters\",\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 44.0, 'y': 230.0, 'w': 395.0, 'h': 523.04}}],\n",
       "    'status': 'OK',\n",
       "    'links': [],\n",
       "    'width': 612.0,\n",
       "    'height': 792.0,\n",
       "    'triggeredAutoMode': False,\n",
       "    'parsingMode': 'accurate',\n",
       "    'structuredData': None,\n",
       "    'noStructuredContent': False,\n",
       "    'noTextContent': False},\n",
       "   {'page': 8,\n",
       "    'text': '            o  Achieves cost control\\n            o  Only available on premium tier\\n    2.  Cluster Type:\\n            o  Choose between Multi node (default) or Single node, depending on workload requirements.\\n    3.  Access Mode:\\n            o  Select Single user and assign access to a specific user (e.g., \"Soham Patel\").\\n            o  Other options (like Shared) allow multiple users to access the cluster simultaneously.\\n    4.  Databricks Runtime Version:\\n            o  Select the desired runtime version (e.g., 15.4 LTS (Scala 2.12, Spark 3.5.0)).\\n            o  Enable Photon Acceleration for optimized performance.\\n    5.  Worker Configuration:\\n            o  Worker type: Choose the virtual machine type (e.g., Standard_D4ds_v5, 16 GB memory, 4\\n               cores).\\n            o  Min workers: Specify the minimum number of worker nodes (e.g., 2).\\n            o  Max workers: Specify the maximum number of worker nodes (e.g., 8).\\n            o  Spot instances: Optionally enable this to use spot instances for cost efficiency.\\n    6.  Driver Configuration:\\n            o  Select Same as worker (or customize separately if required).\\n    7.  Autoscaling and Termination:\\n            o  Enable Autoscaling to dynamically adjust the number of workers within the specified range.\\n            o  Set Terminate after (e.g., 120 minutes) to automatically shut down the cluster after inactivity.\\n    8.  Tags:\\n            o  Add Key-Value tags for cost management or identification (optional).\\n    9.  Advanced Options:\\n            o  Expand this section if additional customizations (e.g., Init scripts, Environment variables) are\\n               needed.\\n    10. Review and Create:\\n            o  Click Create compute to launch the cluster.\\nThe Summary Panel on the right provides an overview of the selected configuration, including the resource\\nallocation and cost estimate (in DBU/h).\\n    Note: DBU stands for Data Bricks Unit. The pricing is calculated as:',\n",
       "    'md': '# 1. Achieves cost control\\n\\n- Only available on premium tier\\n\\n# 2. Cluster Type:\\n\\n- Choose between Multi node (default) or Single node, depending on workload requirements.\\n\\n# 3. Access Mode:\\n\\n- Select Single user and assign access to a specific user (e.g., \"Soham Patel\").\\n- Other options (like Shared) allow multiple users to access the cluster simultaneously.\\n\\n# 4. Databricks Runtime Version:\\n\\n- Select the desired runtime version (e.g., 15.4 LTS (Scala 2.12, Spark 3.5.0)).\\n- Enable Photon Acceleration for optimized performance.\\n\\n# 5. Worker Configuration:\\n\\n- Worker type: Choose the virtual machine type (e.g., Standard_D4ds_v5, 16 GB memory, 4 cores).\\n- Min workers: Specify the minimum number of worker nodes (e.g., 2).\\n- Max workers: Specify the maximum number of worker nodes (e.g., 8).\\n- Spot instances: Optionally enable this to use spot instances for cost efficiency.\\n\\n# 6. Driver Configuration:\\n\\n- Select Same as worker (or customize separately if required).\\n\\n# 7. Autoscaling and Termination:\\n\\n- Enable Autoscaling to dynamically adjust the number of workers within the specified range.\\n- Set Terminate after (e.g., 120 minutes) to automatically shut down the cluster after inactivity.\\n\\n# 8. Tags:\\n\\n- Add Key-Value tags for cost management or identification (optional).\\n\\n# 9. Advanced Options:\\n\\n- Expand this section if additional customizations (e.g., Init scripts, Environment variables) are needed.\\n\\n# 10. Review and Create:\\n\\n- Click Create compute to launch the cluster.\\n\\nThe Summary Panel on the right provides an overview of the selected configuration, including the resource allocation and cost estimate (in DBU/h).\\n\\nNote: DBU stands for Data Bricks Unit. The pricing is calculated as:',\n",
       "    'images': [],\n",
       "    'charts': [],\n",
       "    'tables': [],\n",
       "    'layout': [],\n",
       "    'items': [{'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': '1. Achieves cost control',\n",
       "      'md': '# 1. Achieves cost control',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 90.0, 'y': 36.0, 'w': 124.0, 'h': 614.04}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '- Only available on premium tier',\n",
       "      'md': '- Only available on premium tier',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 90.0, 'y': 38.04, 'w': 171.0, 'h': 612.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': '2. Cluster Type:',\n",
       "      'md': '# 2. Cluster Type:',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 54.0, 'y': 82.0, 'w': 90.0, 'h': 12.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '- Choose between Multi node (default) or Single node, depending on workload requirements.',\n",
       "      'md': '- Choose between Multi node (default) or Single node, depending on workload requirements.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 90.0, 'y': 38.04, 'w': 466.0, 'h': 612.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': '3. Access Mode:',\n",
       "      'md': '# 3. Access Mode:',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 54.0, 'y': 38.04, 'w': 90.0, 'h': 612.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '- Select Single user and assign access to a specific user (e.g., \"Soham Patel\").\\n- Other options (like Shared) allow multiple users to access the cluster simultaneously.',\n",
       "      'md': '- Select Single user and assign access to a specific user (e.g., \"Soham Patel\").\\n- Other options (like Shared) allow multiple users to access the cluster simultaneously.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 90.0, 'y': 38.04, 'w': 428.0, 'h': 612.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': '4. Databricks Runtime Version:',\n",
       "      'md': '# 4. Databricks Runtime Version:',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 54.0, 'y': 38.04, 'w': 170.0, 'h': 612.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '- Select the desired runtime version (e.g., 15.4 LTS (Scala 2.12, Spark 3.5.0)).\\n- Enable Photon Acceleration for optimized performance.',\n",
       "      'md': '- Select the desired runtime version (e.g., 15.4 LTS (Scala 2.12, Spark 3.5.0)).\\n- Enable Photon Acceleration for optimized performance.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 54.0, 'y': 38.04, 'w': 429.0, 'h': 612.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': '5. Worker Configuration:',\n",
       "      'md': '# 5. Worker Configuration:',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 54.0, 'y': 38.04, 'w': 139.0, 'h': 612.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '- Worker type: Choose the virtual machine type (e.g., Standard_D4ds_v5, 16 GB memory, 4 cores).\\n- Min workers: Specify the minimum number of worker nodes (e.g., 2).\\n- Max workers: Specify the maximum number of worker nodes (e.g., 8).\\n- Spot instances: Optionally enable this to use spot instances for cost efficiency.',\n",
       "      'md': '- Worker type: Choose the virtual machine type (e.g., Standard_D4ds_v5, 16 GB memory, 4 cores).\\n- Min workers: Specify the minimum number of worker nodes (e.g., 2).\\n- Max workers: Specify the maximum number of worker nodes (e.g., 8).\\n- Spot instances: Optionally enable this to use spot instances for cost efficiency.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 90.0, 'y': 38.04, 'w': 464.0, 'h': 612.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': '6. Driver Configuration:',\n",
       "      'md': '# 6. Driver Configuration:',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 54.0, 'y': 38.04, 'w': 133.0, 'h': 612.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '- Select Same as worker (or customize separately if required).',\n",
       "      'md': '- Select Same as worker (or customize separately if required).',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 90.0, 'y': 38.04, 'w': 316.0, 'h': 612.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': '7. Autoscaling and Termination:',\n",
       "      'md': '# 7. Autoscaling and Termination:',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 54.0, 'y': 38.04, 'w': 173.0, 'h': 612.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '- Enable Autoscaling to dynamically adjust the number of workers within the specified range.\\n- Set Terminate after (e.g., 120 minutes) to automatically shut down the cluster after inactivity.',\n",
       "      'md': '- Enable Autoscaling to dynamically adjust the number of workers within the specified range.\\n- Set Terminate after (e.g., 120 minutes) to automatically shut down the cluster after inactivity.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 90.0, 'y': 38.04, 'w': 477.0, 'h': 612.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': '8. Tags:',\n",
       "      'md': '# 8. Tags:',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 54.0, 'y': 508.0, 'w': 47.0, 'h': 12.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '- Add Key-Value tags for cost management or identification (optional).',\n",
       "      'md': '- Add Key-Value tags for cost management or identification (optional).',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 90.0, 'y': 38.04, 'w': 357.0, 'h': 612.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': '9. Advanced Options:',\n",
       "      'md': '# 9. Advanced Options:',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 54.0, 'y': 38.04, 'w': 119.0, 'h': 612.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '- Expand this section if additional customizations (e.g., Init scripts, Environment variables) are needed.',\n",
       "      'md': '- Expand this section if additional customizations (e.g., Init scripts, Environment variables) are needed.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 90.0, 'y': 38.04, 'w': 466.0, 'h': 612.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': '10. Review and Create:',\n",
       "      'md': '# 10. Review and Create:',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 54.0, 'y': 615.0, 'w': 121.0, 'h': 12.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '- Click Create compute to launch the cluster.\\n\\nThe Summary Panel on the right provides an overview of the selected configuration, including the resource allocation and cost estimate (in DBU/h).\\n\\nNote: DBU stands for Data Bricks Unit. The pricing is calculated as:',\n",
       "      'md': '- Click Create compute to launch the cluster.\\n\\nThe Summary Panel on the right provides an overview of the selected configuration, including the resource allocation and cost estimate (in DBU/h).\\n\\nNote: DBU stands for Data Bricks Unit. The pricing is calculated as:',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 38.04, 'w': 520.0, 'h': 673.96}}],\n",
       "    'status': 'OK',\n",
       "    'links': [],\n",
       "    'width': 612.0,\n",
       "    'height': 792.0,\n",
       "    'triggeredAutoMode': False,\n",
       "    'parsingMode': 'accurate',\n",
       "    'structuredData': None,\n",
       "    'noStructuredContent': False,\n",
       "    'noTextContent': False},\n",
       "   {'page': 9,\n",
       "    'text': '       No of              Price based on               DBU Cost\\n       DBU                workload/ tier\\n       (Driver)              Price of VM                Driver              Total Cost of\\n                                                       Node Cost            the Cluster\\n       No of                 Price f VM                 Worker\\n      workers                                          Node Cost\\nAs you can see, there was a tab called \"Pool\" while we were creating the Cluster. They\\'re commonly known as\\ncluster pools.\\nCluster pools are a key feature in Databricks that optimize resource allocation, reduce cluster startup time, and\\nimprove cost efficiency. They are designed to manage a group of compute instances that can be shared across\\nmultiple clusters, making them particularly useful in environments with frequent cluster usage.\\nWhy Cluster Pools are Required:\\n    1.  Reduce Cluster Startup Time\\n        When a new cluster is created, it often requires provisioning and setting up compute resources, which\\n        can take several minutes. Cluster pools pre-provision and maintain a set of ready-to-use instances,\\n        drastically reducing startup time for new clusters.\\n    2.  Optimize Resource Utilization\\n        Without a pool, every cluster requires dedicated compute resources, leading to potential underutilization.\\n        Cluster pools allow multiple clusters to share a common set of instances, improving resource efficiency.\\n    3.  Handle High-Frequency Workloads\\n        For organizations running frequent jobs or workloads requiring many short-lived clusters, the repetitive\\n        overhead of cluster creation can slow down workflows. Cluster pools eliminate this inefficiency by\\n        maintaining a ready pool of instances.\\n    4.  Cost Management\\n        Provisioning clusters on-demand can lead to over-provisioning or underutilization of resources. Cluster\\n        pools enable better control over instance allocation, ensuring resources are used efficiently, which helps\\n        in managing costs.',\n",
       "    'md': '# Cluster Pools in Databricks\\n\\n| No of DBU (Driver) | Price based on workload/ tier | DBU Cost         |                           |\\n| ------------------ | ----------------------------- | ---------------- | ------------------------- |\\n| No of workers      | Price of VM                   | Driver Node Cost | Total Cost of the Cluster |\\n|                    | Price of VM                   | Worker Node Cost |                           |\\n\\nAs you can see, there was a tab called \"Pool\" while we were creating the Cluster. They\\'re commonly known as cluster pools.\\n\\nCluster pools are a key feature in Databricks that optimize resource allocation, reduce cluster startup time, and improve cost efficiency. They are designed to manage a group of compute instances that can be shared across multiple clusters, making them particularly useful in environments with frequent cluster usage.\\n\\n# Why Cluster Pools are Required:\\n\\n1. Reduce Cluster Startup Time\\nWhen a new cluster is created, it often requires provisioning and setting up compute resources, which can take several minutes. Cluster pools pre-provision and maintain a set of ready-to-use instances, drastically reducing startup time for new clusters.\\n2. Optimize Resource Utilization\\nWithout a pool, every cluster requires dedicated compute resources, leading to potential underutilization. Cluster pools allow multiple clusters to share a common set of instances, improving resource efficiency.\\n3. Handle High-Frequency Workloads\\nFor organizations running frequent jobs or workloads requiring many short-lived clusters, the repetitive overhead of cluster creation can slow down workflows. Cluster pools eliminate this inefficiency by maintaining a ready pool of instances.\\n4. Cost Management\\nProvisioning clusters on-demand can lead to over-provisioning or underutilization of resources. Cluster pools enable better control over instance allocation, ensuring resources are used efficiently, which helps in managing costs.',\n",
       "    'images': [{'name': 'img_p8_1.png',\n",
       "      'height': 532.0,\n",
       "      'width': 1135.0,\n",
       "      'x': 36.0,\n",
       "      'y': 54.39999999999995,\n",
       "      'original_width': 1135,\n",
       "      'original_height': 532,\n",
       "      'type': None}],\n",
       "    'charts': [],\n",
       "    'tables': [],\n",
       "    'layout': [],\n",
       "    'items': [{'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Cluster Pools in Databricks',\n",
       "      'md': '# Cluster Pools in Databricks',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 0.0, 'y': 0.0, 'w': 612.0, 'h': 792.0}},\n",
       "     {'type': 'table',\n",
       "      'lvl': None,\n",
       "      'value': None,\n",
       "      'md': '| No of DBU (Driver) | Price based on workload/ tier | DBU Cost         |                           |\\n| ------------------ | ----------------------------- | ---------------- | ------------------------- |\\n| No of workers      | Price of VM                   | Driver Node Cost | Total Cost of the Cluster |\\n|                    | Price of VM                   | Worker Node Cost |                           |',\n",
       "      'rows': [['No of DBU (Driver)',\n",
       "        'Price based on workload/ tier',\n",
       "        'DBU Cost',\n",
       "        ''],\n",
       "       ['No of workers',\n",
       "        'Price of VM',\n",
       "        'Driver Node Cost',\n",
       "        'Total Cost of the Cluster'],\n",
       "       ['', 'Price of VM', 'Worker Node Cost', '']],\n",
       "      'bBox': {'x': 36.0, 'y': 79.0, 'w': 539.0, 'h': 551.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': 'As you can see, there was a tab called \"Pool\" while we were creating the Cluster. They\\'re commonly known as cluster pools.\\n\\nCluster pools are a key feature in Databricks that optimize resource allocation, reduce cluster startup time, and improve cost efficiency. They are designed to manage a group of compute instances that can be shared across multiple clusters, making them particularly useful in environments with frequent cluster usage.',\n",
       "      'md': 'As you can see, there was a tab called \"Pool\" while we were creating the Cluster. They\\'re commonly known as cluster pools.\\n\\nCluster pools are a key feature in Databricks that optimize resource allocation, reduce cluster startup time, and improve cost efficiency. They are designed to manage a group of compute instances that can be shared across multiple clusters, making them particularly useful in environments with frequent cluster usage.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 153.0, 'w': 531.0, 'h': 245.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Why Cluster Pools are Required:',\n",
       "      'md': '# Why Cluster Pools are Required:',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 363.0, 'w': 173.0, 'h': 12.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '1. Reduce Cluster Startup Time\\nWhen a new cluster is created, it often requires provisioning and setting up compute resources, which can take several minutes. Cluster pools pre-provision and maintain a set of ready-to-use instances, drastically reducing startup time for new clusters.\\n2. Optimize Resource Utilization\\nWithout a pool, every cluster requires dedicated compute resources, leading to potential underutilization. Cluster pools allow multiple clusters to share a common set of instances, improving resource efficiency.\\n3. Handle High-Frequency Workloads\\nFor organizations running frequent jobs or workloads requiring many short-lived clusters, the repetitive overhead of cluster creation can slow down workflows. Cluster pools eliminate this inefficiency by maintaining a ready pool of instances.\\n4. Cost Management\\nProvisioning clusters on-demand can lead to over-provisioning or underutilization of resources. Cluster pools enable better control over instance allocation, ensuring resources are used efficiently, which helps in managing costs.',\n",
       "      'md': '1. Reduce Cluster Startup Time\\nWhen a new cluster is created, it often requires provisioning and setting up compute resources, which can take several minutes. Cluster pools pre-provision and maintain a set of ready-to-use instances, drastically reducing startup time for new clusters.\\n2. Optimize Resource Utilization\\nWithout a pool, every cluster requires dedicated compute resources, leading to potential underutilization. Cluster pools allow multiple clusters to share a common set of instances, improving resource efficiency.\\n3. Handle High-Frequency Workloads\\nFor organizations running frequent jobs or workloads requiring many short-lived clusters, the repetitive overhead of cluster creation can slow down workflows. Cluster pools eliminate this inefficiency by maintaining a ready pool of instances.\\n4. Cost Management\\nProvisioning clusters on-demand can lead to over-provisioning or underutilization of resources. Cluster pools enable better control over instance allocation, ensuring resources are used efficiently, which helps in managing costs.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 54.0, 'y': 386.0, 'w': 521.0, 'h': 244.0}}],\n",
       "    'status': 'OK',\n",
       "    'links': [],\n",
       "    'width': 612.0,\n",
       "    'height': 792.0,\n",
       "    'triggeredAutoMode': False,\n",
       "    'parsingMode': 'accurate',\n",
       "    'structuredData': None,\n",
       "    'noStructuredContent': False,\n",
       "    'noTextContent': False},\n",
       "   {'page': 10,\n",
       "    'text': 'You can configure the cluster pool in the following way shown below:\\n\\n                                      Clusters Pools Create Pooi\\n                                     Create Pool          Cancel     Create\\n                                      Name\\n                                       demo- pool\\n                                      Min Idle\\n                                      MaxCapacity @\\n                                      Idle Instance Auto Termination 0\\n                                      Terminate instances above minimum after   minutes of idle tin\\n                                      Instance Type 0\\n                                       Standard_ DS3_v2          14 GB Memory;  Cores\\n                                      Preloaded Databricks Runtime Version\\n                                       Runtime: 11.3 LTS (Scala 2.12, Spark 3.3.0)  X ~\\n                                        Use Photon preloaded runtimes\\n                                      Instances  Tags\\n                                      On-demand/Spot\\n                                        AIl On-demand OAII Spot\\nOnce you create a pool – one node will automatically be provisioned and then you will see that you will have an\\noption to select the worker from the pools you created.\\n                           Node type\\n                            Ktandard_DS3_v2                          14 GB Memory;    Cores\\n                            Pool 0\\n                             demo -                                   Standard_DS3_v2 @\\n                                    pool\\n                             General purpose',\n",
       "    'md': '# Cluster Pool Configuration\\n\\nYou can configure the cluster pool in the following way shown below:\\n\\n| Clusters Pools Create Pool              |                                             |\\n| --------------------------------------- | ------------------------------------------- |\\n| Create Pool                             | Cancel                                      |\\n| Name                                    | demo-pool                                   |\\n| Min Idle                                |                                             |\\n| Max Capacity                            |                                             |\\n| Idle Instance Auto Termination          | 0                                           |\\n| Terminate instances above minimum after | minutes of idle time                        |\\n| Instance Type                           | Standard\\\\_DS3\\\\_v2                           |\\n| Memory                                  | 14 GB                                       |\\n| Cores                                   |                                             |\\n| Preloaded Databricks Runtime Version    | Runtime: 11.3 LTS (Scala 2.12, Spark 3.3.0) |\\n| Use Photon preloaded runtimes           |                                             |\\n| Instances                               |                                             |\\n| Tags                                    |                                             |\\n| On-demand/Spot                          | All On-demand / All Spot                    |\\n\\nOnce you create a pool – one node will automatically be provisioned and then you will see that you will have an option to select the worker from the pools you created.\\n\\n# Node Type\\n\\n| Standard\\\\_DS3\\\\_v2 | 14 GB Memory | Cores           |\\n| ----------------- | ------------ | --------------- |\\n| Pool              | demo-pool    | General purpose |\\n',\n",
       "    'images': [{'name': 'img_p9_1.png',\n",
       "      'height': 685.0,\n",
       "      'width': 445.0,\n",
       "      'x': 185.7,\n",
       "      'y': 73.77999999999997,\n",
       "      'original_width': 445,\n",
       "      'original_height': 685,\n",
       "      'type': None},\n",
       "     {'name': 'img_p9_2.png',\n",
       "      'height': 151.0,\n",
       "      'width': 455.0,\n",
       "      'x': 135.0,\n",
       "      'y': 491.13,\n",
       "      'original_width': 455,\n",
       "      'original_height': 151,\n",
       "      'type': None}],\n",
       "    'charts': [],\n",
       "    'tables': [],\n",
       "    'layout': [],\n",
       "    'items': [{'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Cluster Pool Configuration',\n",
       "      'md': '# Cluster Pool Configuration',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 186.0, 'y': 572.0, 'w': 24.0, 'h': 9.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': 'You can configure the cluster pool in the following way shown below:',\n",
       "      'md': 'You can configure the cluster pool in the following way shown below:',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 36.0, 'w': 340.0, 'h': 545.0}},\n",
       "     {'type': 'table',\n",
       "      'lvl': None,\n",
       "      'value': None,\n",
       "      'md': '| Clusters Pools Create Pool              |                                             |\\n| --------------------------------------- | ------------------------------------------- |\\n| Create Pool                             | Cancel                                      |\\n| Name                                    | demo-pool                                   |\\n| Min Idle                                |                                             |\\n| Max Capacity                            |                                             |\\n| Idle Instance Auto Termination          | 0                                           |\\n| Terminate instances above minimum after | minutes of idle time                        |\\n| Instance Type                           | Standard\\\\_DS3\\\\_v2                           |\\n| Memory                                  | 14 GB                                       |\\n| Cores                                   |                                             |\\n| Preloaded Databricks Runtime Version    | Runtime: 11.3 LTS (Scala 2.12, Spark 3.3.0) |\\n| Use Photon preloaded runtimes           |                                             |\\n| Instances                               |                                             |\\n| Tags                                    |                                             |\\n| On-demand/Spot                          | All On-demand / All Spot                    |',\n",
       "      'rows': [['Clusters Pools Create Pool', ''],\n",
       "       ['Create Pool', 'Cancel'],\n",
       "       ['Name', 'demo-pool'],\n",
       "       ['Min Idle', ''],\n",
       "       ['Max Capacity', ''],\n",
       "       ['Idle Instance Auto Termination', '0'],\n",
       "       ['Terminate instances above minimum after', 'minutes of idle time'],\n",
       "       ['Instance Type', 'Standard_DS3_v2'],\n",
       "       ['Memory', '14 GB'],\n",
       "       ['Cores', ''],\n",
       "       ['Preloaded Databricks Runtime Version',\n",
       "        'Runtime: 11.3 LTS (Scala 2.12, Spark 3.3.0)'],\n",
       "       ['Use Photon preloaded runtimes', ''],\n",
       "       ['Instances', ''],\n",
       "       ['Tags', ''],\n",
       "       ['On-demand/Spot', 'All On-demand / All Spot']],\n",
       "      'bBox': {'x': 36.0, 'y': 36.0, 'w': 538.0, 'h': 567.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': 'Once you create a pool – one node will automatically be provisioned and then you will see that you will have an option to select the worker from the pools you created.',\n",
       "      'md': 'Once you create a pool – one node will automatically be provisioned and then you will see that you will have an option to select the worker from the pools you created.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 118.0, 'w': 538.0, 'h': 463.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Node Type',\n",
       "      'md': '# Node Type',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 146.0, 'y': 498.0, 'w': 51.0, 'h': 12.0}},\n",
       "     {'type': 'table',\n",
       "      'lvl': None,\n",
       "      'value': None,\n",
       "      'md': '| Standard\\\\_DS3\\\\_v2 | 14 GB Memory | Cores           |\\n| ----------------- | ------------ | --------------- |\\n| Pool              | demo-pool    | General purpose |',\n",
       "      'rows': [['Standard_DS3_v2', '14 GB Memory', 'Cores'],\n",
       "       ['Pool', 'demo-pool', 'General purpose']],\n",
       "      'bBox': {'x': 36.0, 'y': 36.0, 'w': 538.0, 'h': 567.0}}],\n",
       "    'status': 'OK',\n",
       "    'links': [],\n",
       "    'width': 612.0,\n",
       "    'height': 792.0,\n",
       "    'triggeredAutoMode': False,\n",
       "    'parsingMode': 'accurate',\n",
       "    'structuredData': None,\n",
       "    'noStructuredContent': False,\n",
       "    'noTextContent': False},\n",
       "   {'page': 11,\n",
       "    'text': \"Let’s explore how to create notebooks inside a cluster.\\nNotebooks Inside a Cluster in Databricks\\nNotebooks in Databricks serve as interactive environments for writing and executing code directly on a cluster.\\nThey are used for a variety of tasks, including data exploration, transformation, visualization, and machine\\nlearning. Notebooks are tied to a cluster, meaning that computations are executed using the selected cluster's\\nresources.\\nCreating a Notebook in Databricks\\nTo create a notebook in Databricks, follow these steps:\\n        1.  Navigate to the Workspace\\n            Inside the Databricks workspace, go to the desired location or folder where you want to create the\\n            notebook.\\n        2.  Create a Folder (Optional)\\n             o              Click on the Workspace tab.\\n             o              Right-click and select Create > Folder.\\n             o              Name the folder, e.g., databricks-folder.\\n        3.  Create a Notebook\\n             o              Open the folder (e.g., databricks-folder).\\n             o              Click on Create > Notebook.\\n             o              Fill in the required fields:\\n                             ▪  Notebook Name: For example, intro to notebook.\\n                             ▪  Default Language: Choose one of the supported languages (Python, Scala, SQL, or R).\\n                             ▪  Cluster Dropdown: Select the cluster where the notebook will run. This ensures that the\\n                                code inside the notebook executes on the chosen cluster.\\n        4.  Click Create to finalize the process.\\n Notebooks Introduction     Python                                                   Run all  databricks-course-cl_  Schedule  Share\\n Fille  Edit View Run Help  Last edit was minutes agq  Give feedback\\n                                                                                                            Python\\n  Shift+Enter\\nHere is the basic layout for how the notebooks looks like:\\n            Run All: Executes all the cells in the notebook sequentially.\\n            Cluster Dropdown: Allows you to select the cluster where the notebook will run.\\n            Schedule: Enables scheduling the notebook to run at specific time intervals using Cron jobs.\\n            Share: Provides the option to share the notebook with other team members.\\nP.S: You can clone the notebook through the File menu.\",\n",
       "    'md': \"# Notebooks Inside a Cluster in Databricks\\n\\nNotebooks in Databricks serve as interactive environments for writing and executing code directly on a cluster. They are used for a variety of tasks, including data exploration, transformation, visualization, and machine learning. Notebooks are tied to a cluster, meaning that computations are executed using the selected cluster's resources.\\n\\n# Creating a Notebook in Databricks\\n\\nTo create a notebook in Databricks, follow these steps:\\n\\n1. Navigate to the Workspace\\n\\nInside the Databricks workspace, go to the desired location or folder where you want to create the notebook.\\n2. Create a Folder (Optional)\\n\\n- Click on the Workspace tab.\\n- Right-click and select Create > Folder.\\n- Name the folder, e.g., databricks-folder.\\n3. Create a Notebook\\n\\n- Open the folder (e.g., databricks-folder).\\n- Click on Create > Notebook.\\n- Fill in the required fields:\\n4. Click Create to finalize the process.\\n\\n# Notebooks Introduction\\n\\nHere is the basic layout for how the notebooks looks like:\\n\\n- Run All: Executes all the cells in the notebook sequentially.\\n- Cluster Dropdown: Allows you to select the cluster where the notebook will run.\\n- Schedule: Enables scheduling the notebook to run at specific time intervals using Cron jobs.\\n- Share: Provides the option to share the notebook with other team members.\\n\\nP.S: You can clone the notebook through the File menu.\",\n",
       "    'images': [{'name': 'img_p10_1.png',\n",
       "      'height': 15.0,\n",
       "      'width': 15.0,\n",
       "      'x': 54.0,\n",
       "      'y': 636.5,\n",
       "      'original_width': None,\n",
       "      'original_height': None,\n",
       "      'type': None},\n",
       "     {'name': 'img_p10_1.png',\n",
       "      'height': 15.0,\n",
       "      'width': 15.0,\n",
       "      'x': 54.0,\n",
       "      'y': 651.25,\n",
       "      'original_width': None,\n",
       "      'original_height': None,\n",
       "      'type': None},\n",
       "     {'name': 'img_p10_1.png',\n",
       "      'height': 15.0,\n",
       "      'width': 15.0,\n",
       "      'x': 54.0,\n",
       "      'y': 667.6999999999999,\n",
       "      'original_width': None,\n",
       "      'original_height': None,\n",
       "      'type': None},\n",
       "     {'name': 'img_p10_1.png',\n",
       "      'height': 15.0,\n",
       "      'width': 15.0,\n",
       "      'x': 54.0,\n",
       "      'y': 684.0,\n",
       "      'original_width': None,\n",
       "      'original_height': None,\n",
       "      'type': None},\n",
       "     {'name': 'img_p10_2.png',\n",
       "      'height': 206.0,\n",
       "      'width': 1721.0,\n",
       "      'x': 36.0,\n",
       "      'y': 539.87,\n",
       "      'original_width': 1721,\n",
       "      'original_height': 206,\n",
       "      'type': None}],\n",
       "    'charts': [],\n",
       "    'tables': [],\n",
       "    'layout': [],\n",
       "    'items': [{'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Notebooks Inside a Cluster in Databricks',\n",
       "      'md': '# Notebooks Inside a Cluster in Databricks',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 57.96, 'w': 250.0, 'h': 364.08}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': \"Notebooks in Databricks serve as interactive environments for writing and executing code directly on a cluster. They are used for a variety of tasks, including data exploration, transformation, visualization, and machine learning. Notebooks are tied to a cluster, meaning that computations are executed using the selected cluster's resources.\",\n",
       "      'md': \"Notebooks in Databricks serve as interactive environments for writing and executing code directly on a cluster. They are used for a variety of tasks, including data exploration, transformation, visualization, and machine learning. Notebooks are tied to a cluster, meaning that computations are executed using the selected cluster's resources.\",\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 84.0, 'w': 533.0, 'h': 338.04}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Creating a Notebook in Databricks',\n",
       "      'md': '# Creating a Notebook in Databricks',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 151.0, 'w': 182.0, 'h': 271.04}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': 'To create a notebook in Databricks, follow these steps:\\n\\n1. Navigate to the Workspace\\n\\nInside the Databricks workspace, go to the desired location or folder where you want to create the notebook.\\n2. Create a Folder (Optional)\\n\\n- Click on the Workspace tab.\\n- Right-click and select Create > Folder.\\n- Name the folder, e.g., databricks-folder.\\n3. Create a Notebook\\n\\n- Open the folder (e.g., databricks-folder).\\n- Click on Create > Notebook.\\n- Fill in the required fields:\\n4. Click Create to finalize the process.',\n",
       "      'md': 'To create a notebook in Databricks, follow these steps:\\n\\n1. Navigate to the Workspace\\n\\nInside the Databricks workspace, go to the desired location or folder where you want to create the notebook.\\n2. Create a Folder (Optional)\\n\\n- Click on the Workspace tab.\\n- Right-click and select Create > Folder.\\n- Name the folder, e.g., databricks-folder.\\n3. Create a Notebook\\n\\n- Open the folder (e.g., databricks-folder).\\n- Click on Create > Notebook.\\n- Fill in the required fields:\\n4. Click Create to finalize the process.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 174.0, 'w': 505.0, 'h': 355.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Notebooks Introduction',\n",
       "      'md': '# Notebooks Introduction',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 42.0, 'y': 275.04, 'w': 63.0, 'h': 273.96}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': 'Here is the basic layout for how the notebooks looks like:\\n\\n- Run All: Executes all the cells in the notebook sequentially.\\n- Cluster Dropdown: Allows you to select the cluster where the notebook will run.\\n- Schedule: Enables scheduling the notebook to run at specific time intervals using Cron jobs.\\n- Share: Provides the option to share the notebook with other team members.\\n\\nP.S: You can clone the notebook through the File menu.',\n",
       "      'md': 'Here is the basic layout for how the notebooks looks like:\\n\\n- Run All: Executes all the cells in the notebook sequentially.\\n- Cluster Dropdown: Allows you to select the cluster where the notebook will run.\\n- Schedule: Enables scheduling the notebook to run at specific time intervals using Cron jobs.\\n- Share: Provides the option to share the notebook with other team members.\\n\\nP.S: You can clone the notebook through the File menu.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 275.04, 'w': 521.0, 'h': 444.96}}],\n",
       "    'status': 'OK',\n",
       "    'links': [],\n",
       "    'width': 612.0,\n",
       "    'height': 792.0,\n",
       "    'triggeredAutoMode': False,\n",
       "    'parsingMode': 'accurate',\n",
       "    'structuredData': None,\n",
       "    'noStructuredContent': False,\n",
       "    'noTextContent': False},\n",
       "   {'page': 12,\n",
       "    'text': \"Magic Commands in Notebooks\\n       Magic commands are special commands in notebooks that enhance functionality and streamline tasks,\\nallowing users to interact more efficiently with the environment. These commands typically begin with a % for\\nline magic (single-line) or %% for cell magic (multi-line). They are used for various purposes, such as\\ncontrolling notebook behavior, accessing external resources, or optimizing workflows. For example, %run can\\nexecute another notebook, %fs facilitates file system operations within Databricks, and %sql allows SQL\\nqueries to be executed directly in a cell. Additionally, %%time helps measure the execution time of code\\nblocks, providing insights into performance. These commands improve productivity by bridging scripting\\ncapabilities with enhanced notebook functionality.\\n             File System Operations with %fs\\n             The Xfs command allows you to interact with the Databricks File System (DBFS):\\n                Python                                                                     Copy code\\n                %fs    {mnt/data\\n              This lists all the filesanddirectories in the Imnt/data path\\n              Executing SQL Queries with %sql\\n              The %sql command lets you execute SQL queries directly within notebook cell:\\n                                                                                           Copyᶜᵒᶜᵉ\\n                Esql\\n                SELECT  FROM employees LIMIT 10;\\n             This retrieves the first 10 rows from the employees table\\nBenefits of Using Notebooks Inside a Cluster\\n   1.  Integration with Clusters\\n       - Directly execute code using cluster resources for scalable computations.\\n   2.  Multi-Language Support\\n       - Support for Python, Scala, SQL, and R allows flexibility in programming.\\n   3.  Interactive Environment\\n       - Write, execute, and visualize results in one place, streamlining data analysis workflows.\\n   4.  Collaboration\\n       - Multiple users can work on the same notebook, leveraging Databricks' versioning and commenting\\n       features.\\n   5.  Workflow Integration\\n       - Notebooks can be scheduled as part of workflows or used in jobs for automated tasks.\",\n",
       "    'md': \"# Magic Commands in Notebooks\\n\\nMagic commands are special commands in notebooks that enhance functionality and streamline tasks, allowing users to interact more efficiently with the environment. These commands typically begin with a % for line magic (single-line) or %% for cell magic (multi-line). They are used for various purposes, such as controlling notebook behavior, accessing external resources, or optimizing workflows. For example, %run can execute another notebook, %fs facilitates file system operations within Databricks, and %sql allows SQL queries to be executed directly in a cell. Additionally, %%time helps measure the execution time of code blocks, providing insights into performance. These commands improve productivity by bridging scripting capabilities with enhanced notebook functionality.\\n\\n# File System Operations with %fs\\n\\nThe %fs command allows you to interact with the Databricks File System (DBFS):\\n\\n%fs ls /mnt/data\\nThis lists all the files and directories in the /mnt/data path.\\n\\n# Executing SQL Queries with %sql\\n\\nThe %sql command lets you execute SQL queries directly within a notebook cell:\\n\\n%sql SELECT * FROM employees LIMIT 10;\\nThis retrieves the first 10 rows from the employees table.\\n\\n# Benefits of Using Notebooks Inside a Cluster\\n\\n1. Integration with Clusters\\n- Directly execute code using cluster resources for scalable computations.\\n2. Multi-Language Support\\n- Support for Python, Scala, SQL, and R allows flexibility in programming.\\n3. Interactive Environment\\n- Write, execute, and visualize results in one place, streamlining data analysis workflows.\\n4. Collaboration\\n- Multiple users can work on the same notebook, leveraging Databricks' versioning and commenting features.\\n5. Workflow Integration\\n- Notebooks can be scheduled as part of workflows or used in jobs for automated tasks.\",\n",
       "    'images': [{'name': 'img_p11_1.png',\n",
       "      'height': 433.0,\n",
       "      'width': 727.0,\n",
       "      'x': 85.45,\n",
       "      'y': 231.8,\n",
       "      'original_width': 727,\n",
       "      'original_height': 433,\n",
       "      'type': None}],\n",
       "    'charts': [],\n",
       "    'tables': [],\n",
       "    'layout': [],\n",
       "    'items': [{'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Magic Commands in Notebooks',\n",
       "      'md': '# Magic Commands in Notebooks',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 59.0, 'w': 166.0, 'h': 12.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': 'Magic commands are special commands in notebooks that enhance functionality and streamline tasks, allowing users to interact more efficiently with the environment. These commands typically begin with a % for line magic (single-line) or %% for cell magic (multi-line). They are used for various purposes, such as controlling notebook behavior, accessing external resources, or optimizing workflows. For example, %run can execute another notebook, %fs facilitates file system operations within Databricks, and %sql allows SQL queries to be executed directly in a cell. Additionally, %%time helps measure the execution time of code blocks, providing insights into performance. These commands improve productivity by bridging scripting capabilities with enhanced notebook functionality.',\n",
       "      'md': 'Magic commands are special commands in notebooks that enhance functionality and streamline tasks, allowing users to interact more efficiently with the environment. These commands typically begin with a % for line magic (single-line) or %% for cell magic (multi-line). They are used for various purposes, such as controlling notebook behavior, accessing external resources, or optimizing workflows. For example, %run can execute another notebook, %fs facilitates file system operations within Databricks, and %sql allows SQL queries to be executed directly in a cell. Additionally, %%time helps measure the execution time of code blocks, providing insights into performance. These commands improve productivity by bridging scripting capabilities with enhanced notebook functionality.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 82.0, 'w': 535.0, 'h': 235.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'File System Operations with %fs',\n",
       "      'md': '# File System Operations with %fs',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 102.0, 'y': 237.0, 'w': 146.0, 'h': 80.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': 'The %fs command allows you to interact with the Databricks File System (DBFS):\\n\\n%fs ls /mnt/data\\nThis lists all the files and directories in the /mnt/data path.',\n",
       "      'md': 'The %fs command allows you to interact with the Databricks File System (DBFS):\\n\\n%fs ls /mnt/data\\nThis lists all the files and directories in the /mnt/data path.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 114.0, 'y': 309.0, 'w': 17.0, 'h': 8.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Executing SQL Queries with %sql',\n",
       "      'md': '# Executing SQL Queries with %sql',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 104.0, 'y': 360.0, 'w': 147.0, 'h': 15.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': 'The %sql command lets you execute SQL queries directly within a notebook cell:\\n\\n%sql SELECT * FROM employees LIMIT 10;\\nThis retrieves the first 10 rows from the employees table.',\n",
       "      'md': 'The %sql command lets you execute SQL queries directly within a notebook cell:\\n\\n%sql SELECT * FROM employees LIMIT 10;\\nThis retrieves the first 10 rows from the employees table.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 102.0, 'y': 378.0, 'w': 349.0, 'h': 110.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Benefits of Using Notebooks Inside a Cluster',\n",
       "      'md': '# Benefits of Using Notebooks Inside a Cluster',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 503.0, 'w': 231.0, 'h': 12.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': \"1. Integration with Clusters\\n- Directly execute code using cluster resources for scalable computations.\\n2. Multi-Language Support\\n- Support for Python, Scala, SQL, and R allows flexibility in programming.\\n3. Interactive Environment\\n- Write, execute, and visualize results in one place, streamlining data analysis workflows.\\n4. Collaboration\\n- Multiple users can work on the same notebook, leveraging Databricks' versioning and commenting features.\\n5. Workflow Integration\\n- Notebooks can be scheduled as part of workflows or used in jobs for automated tasks.\",\n",
       "      'md': \"1. Integration with Clusters\\n- Directly execute code using cluster resources for scalable computations.\\n2. Multi-Language Support\\n- Support for Python, Scala, SQL, and R allows flexibility in programming.\\n3. Interactive Environment\\n- Write, execute, and visualize results in one place, streamlining data analysis workflows.\\n4. Collaboration\\n- Multiple users can work on the same notebook, leveraging Databricks' versioning and commenting features.\\n5. Workflow Integration\\n- Notebooks can be scheduled as part of workflows or used in jobs for automated tasks.\",\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 54.0, 'y': 283.0, 'w': 499.0, 'h': 436.0}}],\n",
       "    'status': 'OK',\n",
       "    'links': [],\n",
       "    'width': 612.0,\n",
       "    'height': 792.0,\n",
       "    'triggeredAutoMode': False,\n",
       "    'parsingMode': 'accurate',\n",
       "    'structuredData': None,\n",
       "    'noStructuredContent': False,\n",
       "    'noTextContent': False},\n",
       "   {'page': 13,\n",
       "    'text': 'Now, let’s explore how Databricks simplifies data ingestion and management, making it a breeze to work with\\neven the most complex datasets!\\nData Ingestion and Management in Databricks\\nData is at the heart of every workflow in Databricks, and the platform makes it incredibly easy to upload,\\nmanage, and interact with datasets. Whether you’re working with a small CSV file or a large distributed dataset,\\nDatabricks provides a seamless interface for handling your data. Let’s dive into how you can upload and\\nmanage datasets step-by-step!\\n       CSV                                                                                         Parquet\\n       JSON             Read Data               Transform Data               Write Data              Avro\\n       XML                                                                                          Delta\\n       JDBC                                                                                          JDBC\\n                    DataFrameReader API       DataFrame API              DataFrameWriter API\\n                                              Data Types\\n                                              Row\\n                                              Column\\n                                              Functions\\n                                              Window\\n                                              Grouping\\n1. Uploading Datasets\\nUploading data to Databricks can be done in just a few clicks:\\n   1.  Navigate to the Workspace:\\n       In the left navigation bar, click on Workspace to find your personal or shared workspace folder.\\n   2.  Locate the “Upload Data” Option:\\n           o   Inside the Workspace, click the “+” button and select Upload Data.\\n           o   Alternatively, navigate to the Data tab from the left panel and click Add Data.\\n   3.  Choose Your File:\\n           o   Drag and drop your file or select it from your local system. Databricks supports common file\\n               formats like CSV, JSON, Parquet, and more.\\n           o   Provide a destination for the file. This can be the default DBFS (Databricks File System) or an\\n               external storage like S3 or Azure Blob.\\n   4.  Preview Your Data:\\n       Databricks will automatically generate a preview of your dataset, allowing you to verify the file before\\n       saving.',\n",
       "    'md': '# Data Ingestion and Management in Databricks\\n\\nNow, let’s explore how Databricks simplifies data ingestion and management, making it a breeze to work with even the most complex datasets!\\n\\nData is at the heart of every workflow in Databricks, and the platform makes it incredibly easy to upload, manage, and interact with datasets. Whether you’re working with a small CSV file or a large distributed dataset, Databricks provides a seamless interface for handling your data. Let’s dive into how you can upload and manage datasets step-by-step!\\n\\n| CSV                 |           |                |            | Parquet |   |   |   |   |\\n| ------------------- | --------- | -------------- | ---------- | ------- | - | - | - | - |\\n| JSON                | Read Data | Transform Data | Write Data | Avro    |   |   |   |   |\\n| XML                 |           |                |            | Delta   |   |   |   |   |\\n| JDBC                | JDBC      |                |            |         |   |   |   |   |\\n| DataFrameReader API |           |                |            |         |   |   |   |   |\\n| DataFrame API       |           |                |            |         |   |   |   |   |\\n| DataFrameWriter API |           |                |            |         |   |   |   |   |\\n| Data Types          |           |                |            |         |   |   |   |   |\\n| Row                 |           |                |            |         |   |   |   |   |\\n| Column              |           |                |            |         |   |   |   |   |\\n| Functions           |           |                |            |         |   |   |   |   |\\n| Window              |           |                |            |         |   |   |   |   |\\n| Grouping            |           |                |            |         |   |   |   |   |\\n\\n# 1. Uploading Datasets\\n\\nUploading data to Databricks can be done in just a few clicks:\\n\\n1. Navigate to the Workspace:\\nIn the left navigation bar, click on Workspace to find your personal or shared workspace folder.\\n2. Locate the “Upload Data” Option:\\n- Inside the Workspace, click the “+” button and select Upload Data.\\n- Alternatively, navigate to the Data tab from the left panel and click Add Data.\\n3. Choose Your File:\\n- Drag and drop your file or select it from your local system. Databricks supports common file formats like CSV, JSON, Parquet, and more.\\n- Provide a destination for the file. This can be the default DBFS (Databricks File System) or an external storage like S3 or Azure Blob.\\n4. Preview Your Data:\\nDatabricks will automatically generate a preview of your dataset, allowing you to verify the file before saving.',\n",
       "    'images': [{'name': 'img_p12_1.png',\n",
       "      'height': 847.0,\n",
       "      'width': 1902.0,\n",
       "      'x': 36.0,\n",
       "      'y': 189.61,\n",
       "      'original_width': 1902,\n",
       "      'original_height': 847,\n",
       "      'type': None}],\n",
       "    'charts': [],\n",
       "    'tables': [],\n",
       "    'layout': [],\n",
       "    'items': [{'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Data Ingestion and Management in Databricks',\n",
       "      'md': '# Data Ingestion and Management in Databricks',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 72.96, 'w': 280.0, 'h': 593.08}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': 'Now, let’s explore how Databricks simplifies data ingestion and management, making it a breeze to work with even the most complex datasets!\\n\\nData is at the heart of every workflow in Databricks, and the platform makes it incredibly easy to upload, manage, and interact with datasets. Whether you’re working with a small CSV file or a large distributed dataset, Databricks provides a seamless interface for handling your data. Let’s dive into how you can upload and manage datasets step-by-step!',\n",
       "      'md': 'Now, let’s explore how Databricks simplifies data ingestion and management, making it a breeze to work with even the most complex datasets!\\n\\nData is at the heart of every workflow in Databricks, and the platform makes it incredibly easy to upload, manage, and interact with datasets. Whether you’re working with a small CSV file or a large distributed dataset, Databricks provides a seamless interface for handling your data. Let’s dive into how you can upload and manage datasets step-by-step!',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 36.0, 'w': 537.0, 'h': 630.04}},\n",
       "     {'type': 'table',\n",
       "      'lvl': None,\n",
       "      'value': None,\n",
       "      'md': '| CSV                 |           |                |            | Parquet |   |   |   |   |\\n| ------------------- | --------- | -------------- | ---------- | ------- | - | - | - | - |\\n| JSON                | Read Data | Transform Data | Write Data | Avro    |   |   |   |   |\\n| XML                 |           |                |            | Delta   |   |   |   |   |\\n| JDBC                | JDBC      |                |            |         |   |   |   |   |\\n| DataFrameReader API |           |                |            |         |   |   |   |   |\\n| DataFrame API       |           |                |            |         |   |   |   |   |\\n| DataFrameWriter API |           |                |            |         |   |   |   |   |\\n| Data Types          |           |                |            |         |   |   |   |   |\\n| Row                 |           |                |            |         |   |   |   |   |\\n| Column              |           |                |            |         |   |   |   |   |\\n| Functions           |           |                |            |         |   |   |   |   |\\n| Window              |           |                |            |         |   |   |   |   |\\n| Grouping            |           |                |            |         |   |   |   |   |',\n",
       "      'rows': [['CSV', '', '', '', 'Parquet', '', '', '', ''],\n",
       "       ['JSON',\n",
       "        'Read Data',\n",
       "        'Transform Data',\n",
       "        'Write Data',\n",
       "        'Avro',\n",
       "        '',\n",
       "        '',\n",
       "        '',\n",
       "        ''],\n",
       "       ['XML', '', '', '', 'Delta', '', '', '', ''],\n",
       "       ['JDBC', 'JDBC', '', '', '', '', '', '', ''],\n",
       "       ['DataFrameReader API', '', '', '', '', '', '', '', ''],\n",
       "       ['DataFrame API', '', '', '', '', '', '', '', ''],\n",
       "       ['DataFrameWriter API', '', '', '', '', '', '', '', ''],\n",
       "       ['Data Types', '', '', '', '', '', '', '', ''],\n",
       "       ['Row', '', '', '', '', '', '', '', ''],\n",
       "       ['Column', '', '', '', '', '', '', '', ''],\n",
       "       ['Functions', '', '', '', '', '', '', '', ''],\n",
       "       ['Window', '', '', '', '', '', '', '', ''],\n",
       "       ['Grouping', '', '', '', '', '', '', '', '']],\n",
       "      'bBox': {'x': 36.0, 'y': 36.0, 'w': 537.0, 'h': 698.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': '1. Uploading Datasets',\n",
       "      'md': '# 1. Uploading Datasets',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 441.0, 'w': 112.0, 'h': 225.04}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': 'Uploading data to Databricks can be done in just a few clicks:\\n\\n1. Navigate to the Workspace:\\nIn the left navigation bar, click on Workspace to find your personal or shared workspace folder.\\n2. Locate the “Upload Data” Option:\\n- Inside the Workspace, click the “+” button and select Upload Data.\\n- Alternatively, navigate to the Data tab from the left panel and click Add Data.\\n3. Choose Your File:\\n- Drag and drop your file or select it from your local system. Databricks supports common file formats like CSV, JSON, Parquet, and more.\\n- Provide a destination for the file. This can be the default DBFS (Databricks File System) or an external storage like S3 or Azure Blob.\\n4. Preview Your Data:\\nDatabricks will automatically generate a preview of your dataset, allowing you to verify the file before saving.',\n",
       "      'md': 'Uploading data to Databricks can be done in just a few clicks:\\n\\n1. Navigate to the Workspace:\\nIn the left navigation bar, click on Workspace to find your personal or shared workspace folder.\\n2. Locate the “Upload Data” Option:\\n- Inside the Workspace, click the “+” button and select Upload Data.\\n- Alternatively, navigate to the Data tab from the left panel and click Add Data.\\n3. Choose Your File:\\n- Drag and drop your file or select it from your local system. Databricks supports common file formats like CSV, JSON, Parquet, and more.\\n- Provide a destination for the file. This can be the default DBFS (Databricks File System) or an external storage like S3 or Azure Blob.\\n4. Preview Your Data:\\nDatabricks will automatically generate a preview of your dataset, allowing you to verify the file before saving.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 224.0, 'w': 529.0, 'h': 510.0}}],\n",
       "    'status': 'OK',\n",
       "    'links': [],\n",
       "    'width': 612.0,\n",
       "    'height': 792.0,\n",
       "    'triggeredAutoMode': False,\n",
       "    'parsingMode': 'accurate',\n",
       "    'structuredData': None,\n",
       "    'noStructuredContent': False,\n",
       "    'noTextContent': False},\n",
       "   {'page': 14,\n",
       "    'text': '     5.  Save the Dataset:\\n         Once confirmed, click Next and save your dataset to a pre-defined location (e.g., a workspace folder or a\\n         table in a SQL warehouse).\\n2. Managing Datasets\\nAfter uploading your data, it’s important to organize and manage it for easy access and reusability.\\nWhy Managing Datasets Matters\\nEfficient data management is critical for:\\n     •   Simplifying ETL Pipelines: Easily transform and move data between raw, clean, and curated layers.\\n     •   Seamless Collaboration: Centralize data in one place so team members can access it effortlessly.\\n     •   Accelerating Workflows: Quickly upload, query, and visualize your data to gain insights faster.\\na) Using Databricks File System (DBFS):\\n     •   DBFS acts as a distributed storage layer where you can store, organize, and retrieve your files.\\n     •   Use the command dbutils.fs to interact with DBFS through code:\\n           Python                                                                 Copyᶜᶜᵈᵉ\\n            dbutils.fs.ls(\" /mnt/data  List all files in the specified directory\\n           dbutils.fs.rm(\" /mnt/data/file.Csv\"  DeLete File\\nb) Mounting External Storage:\\nDatabricks allows you to integrate with cloud storage platforms like AWS S3, Azure Blob, or Google Cloud\\nStorage. Mount these external sources using:\\n  Python                                                                          Copyᶜᵒᵈᵉ\\n   dbutils.fs.mount(\\n      source= 53a://your bucket name\\n      mountpoint= [mnt/data\\n      extra_configs-{\"fs.53a access.key\\'  \"YOUR_ACCESS KEY\" \"fs: 53a secret-key\\'  YOUR SECR\\nc) Querying and Managing Tables:\\nOnce data is uploaded, you can load it into Delta tables or SQL tables for structured analysis:\\n                                                                                  Copy €ᶜᶜᵈᵉ\\n   CREATE TABLE my_table\\n  USING CSv\\n   OPTIONS (path [mnt/datafmy_file CSV  header true   inferSchema  true\" ) ;\\n3. Exploring and Visualizing Datasets\\nDatabricks offers interactive options to explore your datasets:\\n     •   Data Preview: Open your dataset in Databricks, and the platform will provide a tabular view with quick\\n         filters for exploration.',\n",
       "    'md': '# 5. Save the Dataset:\\n\\nOnce confirmed, click Next and save your dataset to a pre-defined location (e.g., a workspace folder or a table in a SQL warehouse).\\n\\n# 2. Managing Datasets\\n\\nAfter uploading your data, it’s important to organize and manage it for easy access and reusability.\\n\\n# Why Managing Datasets Matters\\n\\nEfficient data management is critical for:\\n\\n- Simplifying ETL Pipelines: Easily transform and move data between raw, clean, and curated layers.\\n- Seamless Collaboration: Centralize data in one place so team members can access it effortlessly.\\n- Accelerating Workflows: Quickly upload, query, and visualize your data to gain insights faster.\\n\\n# a) Using Databricks File System (DBFS):\\n\\n- DBFS acts as a distributed storage layer where you can store, organize, and retrieve your files.\\n- Use the command dbutils.fs to interact with DBFS through code:\\n\\nPython\\ndbutils.fs.ls(\"/mnt/data\")  # List all files in the specified directory\\ndbutils.fs.rm(\"/mnt/data/file.Csv\")  # Delete File\\n\\n# b) Mounting External Storage:\\n\\nDatabricks allows you to integrate with cloud storage platforms like AWS S3, Azure Blob, or Google Cloud Storage. Mount these external sources using:\\n\\nPython\\ndbutils.fs.mount(\\nsource=\"s3a://your_bucket_name\",\\nmountpoint=\"/mnt/data\",\\nextra_configs={\"fs.s3a.access.key\": \"YOUR_ACCESS_KEY\", \"fs.s3a.secret.key\": \"YOUR_SECRET_KEY\"})\\n\\n# c) Querying and Managing Tables:\\n\\nOnce data is uploaded, you can load it into Delta tables or SQL tables for structured analysis:\\n\\nCREATE TABLE my_table\\nUSING CSV\\nOPTIONS (path \"/mnt/data/my_file.csv\", header true, inferSchema true);\\n\\n# 3. Exploring and Visualizing Datasets\\n\\nDatabricks offers interactive options to explore your datasets:\\n\\n- Data Preview: Open your dataset in Databricks, and the platform will provide a tabular view with quick filters for exploration.',\n",
       "    'images': [{'name': 'img_p13_1.png',\n",
       "      'height': 115.0,\n",
       "      'width': 684.0,\n",
       "      'x': 72.0,\n",
       "      'y': 306.5,\n",
       "      'original_width': 684,\n",
       "      'original_height': 115,\n",
       "      'type': None},\n",
       "     {'name': 'img_p13_2.png',\n",
       "      'height': 214.0,\n",
       "      'width': 725.0,\n",
       "      'x': 36.0,\n",
       "      'y': 420.7899999999999,\n",
       "      'original_width': 725,\n",
       "      'original_height': 214,\n",
       "      'type': None},\n",
       "     {'name': 'img_p13_3.png',\n",
       "      'height': 150.0,\n",
       "      'width': 720.0,\n",
       "      'x': 36.0,\n",
       "      'y': 568.69,\n",
       "      'original_width': 720,\n",
       "      'original_height': 150,\n",
       "      'type': None}],\n",
       "    'charts': [],\n",
       "    'tables': [],\n",
       "    'layout': [],\n",
       "    'items': [{'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': '5. Save the Dataset:',\n",
       "      'md': '# 5. Save the Dataset:',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 54.0, 'y': 36.0, 'w': 108.0, 'h': 12.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': 'Once confirmed, click Next and save your dataset to a pre-defined location (e.g., a workspace folder or a table in a SQL warehouse).',\n",
       "      'md': 'Once confirmed, click Next and save your dataset to a pre-defined location (e.g., a workspace folder or a table in a SQL warehouse).',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 72.0, 'y': 51.0, 'w': 503.0, 'h': 26.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': '2. Managing Datasets',\n",
       "      'md': '# 2. Managing Datasets',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 90.0, 'w': 110.0, 'h': 12.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': 'After uploading your data, it’s important to organize and manage it for easy access and reusability.',\n",
       "      'md': 'After uploading your data, it’s important to organize and manage it for easy access and reusability.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 114.0, 'w': 475.0, 'h': 12.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Why Managing Datasets Matters',\n",
       "      'md': '# Why Managing Datasets Matters',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 136.0, 'w': 172.0, 'h': 12.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': 'Efficient data management is critical for:\\n\\n- Simplifying ETL Pipelines: Easily transform and move data between raw, clean, and curated layers.\\n- Seamless Collaboration: Centralize data in one place so team members can access it effortlessly.\\n- Accelerating Workflows: Quickly upload, query, and visualize your data to gain insights faster.',\n",
       "      'md': 'Efficient data management is critical for:\\n\\n- Simplifying ETL Pipelines: Easily transform and move data between raw, clean, and curated layers.\\n- Seamless Collaboration: Centralize data in one place so team members can access it effortlessly.\\n- Accelerating Workflows: Quickly upload, query, and visualize your data to gain insights faster.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 159.0, 'w': 523.0, 'h': 68.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'a) Using Databricks File System (DBFS):',\n",
       "      'md': '# a) Using Databricks File System (DBFS):',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 238.0, 'w': 230.0, 'h': 113.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '- DBFS acts as a distributed storage layer where you can store, organize, and retrieve your files.\\n- Use the command dbutils.fs to interact with DBFS through code:\\n\\nPython\\ndbutils.fs.ls(\"/mnt/data\")  # List all files in the specified directory\\ndbutils.fs.rm(\"/mnt/data/file.Csv\")  # Delete File',\n",
       "      'md': '- DBFS acts as a distributed storage layer where you can store, organize, and retrieve your files.\\n- Use the command dbutils.fs to interact with DBFS through code:\\n\\nPython\\ndbutils.fs.ls(\"/mnt/data\")  # List all files in the specified directory\\ndbutils.fs.rm(\"/mnt/data/file.Csv\")  # Delete File',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 45.0, 'y': 260.0, 'w': 483.0, 'h': 177.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'b) Mounting External Storage:',\n",
       "      'md': '# b) Mounting External Storage:',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 368.0, 'w': 160.0, 'h': 12.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': 'Databricks allows you to integrate with cloud storage platforms like AWS S3, Azure Blob, or Google Cloud Storage. Mount these external sources using:\\n\\nPython\\ndbutils.fs.mount(\\nsource=\"s3a://your_bucket_name\",\\nmountpoint=\"/mnt/data\",\\nextra_configs={\"fs.s3a.access.key\": \"YOUR_ACCESS_KEY\", \"fs.s3a.secret.key\": \"YOUR_SECRET_KEY\"})',\n",
       "      'md': 'Databricks allows you to integrate with cloud storage platforms like AWS S3, Azure Blob, or Google Cloud Storage. Mount these external sources using:\\n\\nPython\\ndbutils.fs.mount(\\nsource=\"s3a://your_bucket_name\",\\nmountpoint=\"/mnt/data\",\\nextra_configs={\"fs.s3a.access.key\": \"YOUR_ACCESS_KEY\", \"fs.s3a.secret.key\": \"YOUR_SECRET_KEY\"})',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 313.0, 'w': 518.0, 'h': 144.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'c) Querying and Managing Tables:',\n",
       "      'md': '# c) Querying and Managing Tables:',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 531.0, 'w': 180.0, 'h': 12.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': 'Once data is uploaded, you can load it into Delta tables or SQL tables for structured analysis:\\n\\nCREATE TABLE my_table\\nUSING CSV\\nOPTIONS (path \"/mnt/data/my_file.csv\", header true, inferSchema true);',\n",
       "      'md': 'Once data is uploaded, you can load it into Delta tables or SQL tables for structured analysis:\\n\\nCREATE TABLE my_table\\nUSING CSV\\nOPTIONS (path \"/mnt/data/my_file.csv\", header true, inferSchema true);',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 344.0, 'w': 450.0, 'h': 288.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': '3. Exploring and Visualizing Datasets',\n",
       "      'md': '# 3. Exploring and Visualizing Datasets',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 677.0, 'w': 192.0, 'h': 12.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': 'Databricks offers interactive options to explore your datasets:\\n\\n- Data Preview: Open your dataset in Databricks, and the platform will provide a tabular view with quick filters for exploration.',\n",
       "      'md': 'Databricks offers interactive options to explore your datasets:\\n\\n- Data Preview: Open your dataset in Databricks, and the platform will provide a tabular view with quick filters for exploration.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 701.0, 'w': 536.0, 'h': 49.0}}],\n",
       "    'status': 'OK',\n",
       "    'links': [],\n",
       "    'width': 612.0,\n",
       "    'height': 792.0,\n",
       "    'triggeredAutoMode': False,\n",
       "    'parsingMode': 'accurate',\n",
       "    'structuredData': None,\n",
       "    'noStructuredContent': False,\n",
       "    'noTextContent': False},\n",
       "   {'page': 15,\n",
       "    'text': \"    •   SQL Queries: Use the SQL Editor to run queries and fetch specific insights.\\n    •   Notebook Analysis: Load your data into a notebook using PySpark, pandas, or Koalas for custom\\n        analyses.\\n    Pro Tips for Dataset Management:\\n    •   Version Control: Use Delta Lake to version your data automatically and track changes.\\n    •   Optimize for Performance: Use Parquet or Delta formats instead of CSV for faster queries and reduced\\n        storage overhead.\\n    •   Collaborate on Datasets: Share access to datasets via the Databricks workspace or by publishing them\\n        in Unity Catalog (more on this later   ).\\n    •   Automate Ingestion: Set up Databricks workflows to automatically ingest data from external sources\\n        on a schedule.\\nNow, let’s uncover how Unity Catalog streamlines data governance and security, bringing clarity and control to\\nyour data ecosystem!\\nExploring Unity Catalog: Centralized Data Governance Made Simple\\n        Data governance (availability, usability, integrity and security of data) is critical in modern data\\nengineering workflows, and Unity Catalog makes it seamless. This feature provides centralized governance,\\nensuring consistent security, compliance, and access management across all your data assets within Databricks.\\nLet’s dive deeper into its purpose and how to use it effectively.\\nWhat is Unity Catalog?\\nUnity Catalog is Databricks' solution for managing and governing data across multiple clouds and workspaces.\\nIt simplifies how organizations define, monitor, and enforce data access policies.\\nKey Features of Unity Catalog:\\n    •   Centralized Access Control: Manage user access to data assets in one place.\\n    •   Data Lineage Tracking: Monitor where your data comes from, how it's used, and its transformations.\\n    •   Column-Level Security: Grant access permissions to specific columns, ensuring sensitive information\\n        stays secure.\\n    •   Auditability: Keep track of all data-related activities for compliance and troubleshooting.\",\n",
       "    'md': \"# SQL Queries\\n\\nUse the SQL Editor to run queries and fetch specific insights.\\n\\n# Notebook Analysis\\n\\nLoad your data into a notebook using PySpark, pandas, or Koalas for custom analyses.\\n\\n# Pro Tips for Dataset Management\\n\\n- Version Control: Use Delta Lake to version your data automatically and track changes.\\n- Optimize for Performance: Use Parquet or Delta formats instead of CSV for faster queries and reduced storage overhead.\\n- Collaborate on Datasets: Share access to datasets via the Databricks workspace or by publishing them in Unity Catalog (more on this later).\\n- Automate Ingestion: Set up Databricks workflows to automatically ingest data from external sources on a schedule.\\n\\n# Exploring Unity Catalog: Centralized Data Governance Made Simple\\n\\nData governance (availability, usability, integrity and security of data) is critical in modern data engineering workflows, and Unity Catalog makes it seamless. This feature provides centralized governance, ensuring consistent security, compliance, and access management across all your data assets within Databricks. Let’s dive deeper into its purpose and how to use it effectively.\\n\\n# What is Unity Catalog?\\n\\nUnity Catalog is Databricks' solution for managing and governing data across multiple clouds and workspaces. It simplifies how organizations define, monitor, and enforce data access policies.\\n\\n# Key Features of Unity Catalog\\n\\n- Centralized Access Control: Manage user access to data assets in one place.\\n- Data Lineage Tracking: Monitor where your data comes from, how it's used, and its transformations.\\n- Column-Level Security: Grant access permissions to specific columns, ensuring sensitive information stays secure.\\n- Auditability: Keep track of all data-related activities for compliance and troubleshooting.\",\n",
       "    'images': [],\n",
       "    'charts': [],\n",
       "    'tables': [],\n",
       "    'layout': [],\n",
       "    'items': [{'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'SQL Queries',\n",
       "      'md': '# SQL Queries',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 0.0, 'y': 0.0, 'w': 612.0, 'h': 792.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': 'Use the SQL Editor to run queries and fetch specific insights.',\n",
       "      'md': 'Use the SQL Editor to run queries and fetch specific insights.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 0.0, 'y': 0.0, 'w': 612.0, 'h': 792.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Notebook Analysis',\n",
       "      'md': '# Notebook Analysis',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 0.0, 'y': 0.0, 'w': 612.0, 'h': 792.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': 'Load your data into a notebook using PySpark, pandas, or Koalas for custom analyses.',\n",
       "      'md': 'Load your data into a notebook using PySpark, pandas, or Koalas for custom analyses.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 72.0, 'y': 73.0, 'w': 46.0, 'h': 12.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Pro Tips for Dataset Management',\n",
       "      'md': '# Pro Tips for Dataset Management',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 0.0, 'y': 0.0, 'w': 612.0, 'h': 792.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '- Version Control: Use Delta Lake to version your data automatically and track changes.\\n- Optimize for Performance: Use Parquet or Delta formats instead of CSV for faster queries and reduced storage overhead.\\n- Collaborate on Datasets: Share access to datasets via the Databricks workspace or by publishing them in Unity Catalog (more on this later).\\n- Automate Ingestion: Set up Databricks workflows to automatically ingest data from external sources on a schedule.',\n",
       "      'md': '- Version Control: Use Delta Lake to version your data automatically and track changes.\\n- Optimize for Performance: Use Parquet or Delta formats instead of CSV for faster queries and reduced storage overhead.\\n- Collaborate on Datasets: Share access to datasets via the Databricks workspace or by publishing them in Unity Catalog (more on this later).\\n- Automate Ingestion: Set up Databricks workflows to automatically ingest data from external sources on a schedule.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 72.0, 'y': 145.0, 'w': 503.0, 'h': 127.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Exploring Unity Catalog: Centralized Data Governance Made Simple',\n",
       "      'md': '# Exploring Unity Catalog: Centralized Data Governance Made Simple',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 342.96, 'w': 416.0, 'h': 14.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': 'Data governance (availability, usability, integrity and security of data) is critical in modern data engineering workflows, and Unity Catalog makes it seamless. This feature provides centralized governance, ensuring consistent security, compliance, and access management across all your data assets within Databricks. Let’s dive deeper into its purpose and how to use it effectively.',\n",
       "      'md': 'Data governance (availability, usability, integrity and security of data) is critical in modern data engineering workflows, and Unity Catalog makes it seamless. This feature provides centralized governance, ensuring consistent security, compliance, and access management across all your data assets within Databricks. Let’s dive deeper into its purpose and how to use it effectively.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 369.0, 'w': 534.0, 'h': 56.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'What is Unity Catalog?',\n",
       "      'md': '# What is Unity Catalog?',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 436.0, 'w': 123.0, 'h': 12.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': \"Unity Catalog is Databricks' solution for managing and governing data across multiple clouds and workspaces. It simplifies how organizations define, monitor, and enforce data access policies.\",\n",
       "      'md': \"Unity Catalog is Databricks' solution for managing and governing data across multiple clouds and workspaces. It simplifies how organizations define, monitor, and enforce data access policies.\",\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 459.0, 'w': 532.0, 'h': 27.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Key Features of Unity Catalog',\n",
       "      'md': '# Key Features of Unity Catalog',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 0.0, 'y': 0.0, 'w': 612.0, 'h': 792.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': \"- Centralized Access Control: Manage user access to data assets in one place.\\n- Data Lineage Tracking: Monitor where your data comes from, how it's used, and its transformations.\\n- Column-Level Security: Grant access permissions to specific columns, ensuring sensitive information stays secure.\\n- Auditability: Keep track of all data-related activities for compliance and troubleshooting.\",\n",
       "      'md': \"- Centralized Access Control: Manage user access to data assets in one place.\\n- Data Lineage Tracking: Monitor where your data comes from, how it's used, and its transformations.\\n- Column-Level Security: Grant access permissions to specific columns, ensuring sensitive information stays secure.\\n- Auditability: Keep track of all data-related activities for compliance and troubleshooting.\",\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 72.0, 'y': 520.0, 'w': 494.0, 'h': 95.0}}],\n",
       "    'status': 'OK',\n",
       "    'links': [],\n",
       "    'width': 612.0,\n",
       "    'height': 792.0,\n",
       "    'triggeredAutoMode': False,\n",
       "    'parsingMode': 'accurate',\n",
       "    'structuredData': None,\n",
       "    'noStructuredContent': False,\n",
       "    'noTextContent': False},\n",
       "   {'page': 16,\n",
       "    'text': '                                                 Databricks Unity Catalog\\n                                         User Management         Metastore\\n                                          Databricks            Databricks\\n                                         Workspace 1           Workspace 2\\n                                           Compute               Compute\\n\\n\\nHow to Use Unity Catalog in Databricks\\n1. Enabling Unity Catalog\\n    •   Administrator Role: Unity Catalog requires an admin to configure workspace settings.\\n    •   Enable Unity Catalog in your Databricks account settings and link it to your cloud storage.\\n2. Accessing Unity Catalog\\n    •   Navigate to the Catalog tab from the left-hand menu.\\n    •   You’ll see a hierarchical view of data assets: Catalogs, Schemas, and Tables.\\n3. Managing Data Access\\n    •   Assign permissions to users or groups for catalogs, schemas, or tables.\\n    •   Use SQL commands or the Databricks UI to define access policies:\\n                                                                                   Copyccce\\n          GRANT SELECT   TABLE sales_data   user  john. doe@databrickscom\\n4. Data Lineage Exploration\\n    •   Open a table or asset and navigate to its Lineage View.\\n    •   This provides a visual representation of upstream and downstream dependencies, helping identify\\n        bottlenecks or inefficiencies.',\n",
       "    'md': '# Databricks Unity Catalog\\n\\n# User Management\\n\\n# Metastore\\n\\n# Databricks Workspace 1\\n\\n# Databricks Workspace 2\\n\\n# Compute\\n\\n# Compute\\n\\n# How to Use Unity Catalog in Databricks\\n\\n# 1. Enabling Unity Catalog\\n\\n- Administrator Role: Unity Catalog requires an admin to configure workspace settings.\\n- Enable Unity Catalog in your Databricks account settings and link it to your cloud storage.\\n\\n# 2. Accessing Unity Catalog\\n\\n- Navigate to the Catalog tab from the left-hand menu.\\n- You’ll see a hierarchical view of data assets: Catalogs, Schemas, and Tables.\\n\\n# 3. Managing Data Access\\n\\n- Assign permissions to users or groups for catalogs, schemas, or tables.\\n- Use SQL commands or the Databricks UI to define access policies:\\n\\nGRANT SELECT TABLE sales_data user john.doe@databricks.com\\n\\n# 4. Data Lineage Exploration\\n\\n- Open a table or asset and navigate to its Lineage View.\\n- This provides a visual representation of upstream and downstream dependencies, helping identify bottlenecks or inefficiencies.',\n",
       "    'images': [{'name': 'img_p15_1.png',\n",
       "      'height': 422.0,\n",
       "      'width': 535.0,\n",
       "      'x': 206.25,\n",
       "      'y': 36.00000000000006,\n",
       "      'original_width': 535,\n",
       "      'original_height': 422,\n",
       "      'type': None},\n",
       "     {'name': 'img_p15_2.png',\n",
       "      'height': 105.0,\n",
       "      'width': 688.0,\n",
       "      'x': 72.0,\n",
       "      'y': 453.69999999999993,\n",
       "      'original_width': 688,\n",
       "      'original_height': 105,\n",
       "      'type': None}],\n",
       "    'charts': [],\n",
       "    'tables': [],\n",
       "    'layout': [],\n",
       "    'items': [{'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Databricks Unity Catalog',\n",
       "      'md': '# Databricks Unity Catalog',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 236.0, 'y': 49.0, 'w': 136.0, 'h': 84.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'User Management',\n",
       "      'md': '# User Management',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 227.0, 'y': 78.0, 'w': 58.0, 'h': 420.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Metastore',\n",
       "      'md': '# Metastore',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 335.0, 'y': 79.0, 'w': 33.0, 'h': 7.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Databricks Workspace 1',\n",
       "      'md': '# Databricks Workspace 1',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 232.0, 'y': 124.0, 'w': 140.0, 'h': 19.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Databricks Workspace 2',\n",
       "      'md': '# Databricks Workspace 2',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 236.0, 'y': 124.0, 'w': 140.0, 'h': 20.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Compute',\n",
       "      'md': '# Compute',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 239.0, 'y': 155.0, 'w': 130.0, 'h': 10.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Compute',\n",
       "      'md': '# Compute',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 239.0, 'y': 155.0, 'w': 130.0, 'h': 10.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'How to Use Unity Catalog in Databricks',\n",
       "      'md': '# How to Use Unity Catalog in Databricks',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 124.0, 'w': 336.0, 'h': 112.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': '1. Enabling Unity Catalog',\n",
       "      'md': '# 1. Enabling Unity Catalog',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 247.0, 'w': 136.0, 'h': 12.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '- Administrator Role: Unity Catalog requires an admin to configure workspace settings.\\n- Enable Unity Catalog in your Databricks account settings and link it to your cloud storage.',\n",
       "      'md': '- Administrator Role: Unity Catalog requires an admin to configure workspace settings.\\n- Enable Unity Catalog in your Databricks account settings and link it to your cloud storage.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 72.0, 'y': 124.0, 'w': 442.0, 'h': 181.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': '2. Accessing Unity Catalog',\n",
       "      'md': '# 2. Accessing Unity Catalog',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 316.0, 'w': 139.0, 'h': 12.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '- Navigate to the Catalog tab from the left-hand menu.\\n- You’ll see a hierarchical view of data assets: Catalogs, Schemas, and Tables.',\n",
       "      'md': '- Navigate to the Catalog tab from the left-hand menu.\\n- You’ll see a hierarchical view of data assets: Catalogs, Schemas, and Tables.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 72.0, 'y': 339.0, 'w': 376.0, 'h': 35.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': '3. Managing Data Access',\n",
       "      'md': '# 3. Managing Data Access',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 385.0, 'w': 131.0, 'h': 12.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '- Assign permissions to users or groups for catalogs, schemas, or tables.\\n- Use SQL commands or the Databricks UI to define access policies:\\n\\nGRANT SELECT TABLE sales_data user john.doe@databricks.com',\n",
       "      'md': '- Assign permissions to users or groups for catalogs, schemas, or tables.\\n- Use SQL commands or the Databricks UI to define access policies:\\n\\nGRANT SELECT TABLE sales_data user john.doe@databricks.com',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 72.0, 'y': 124.0, 'w': 344.0, 'h': 375.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': '4. Data Lineage Exploration',\n",
       "      'md': '# 4. Data Lineage Exploration',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 522.0, 'w': 147.0, 'h': 12.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '- Open a table or asset and navigate to its Lineage View.\\n- This provides a visual representation of upstream and downstream dependencies, helping identify bottlenecks or inefficiencies.',\n",
       "      'md': '- Open a table or asset and navigate to its Lineage View.\\n- This provides a visual representation of upstream and downstream dependencies, helping identify bottlenecks or inefficiencies.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 72.0, 'y': 545.0, 'w': 468.0, 'h': 50.0}}],\n",
       "    'status': 'OK',\n",
       "    'links': [],\n",
       "    'width': 612.0,\n",
       "    'height': 792.0,\n",
       "    'triggeredAutoMode': False,\n",
       "    'parsingMode': 'accurate',\n",
       "    'structuredData': None,\n",
       "    'noStructuredContent': False,\n",
       "    'noTextContent': False},\n",
       "   {'page': 17,\n",
       "    'text': '                                                                Create Azure Databricks Workspace\\n        DatabricksUnity\\n            Catalog                                             Create Azure Data Lake Gen2\\n           Metastore                     Access Connector for\\n                                                   Databricks   Create Access Connector\\n                                  Storage Blob Data]            Add role Storage Blob Data Contributor\\n               Databricks         Contributor Role\\n               Workspace                      Default Storage   Create Unit Catalog Metastore\\n           Compute                      (ADLS Gen2 Container)\\n                                                                Enable Databricks Workspace for Unity Catalog\\n\\n\\n\\n\\nSteps to Create a Unity Catalog Metastore\\n1. Prerequisites\\n    •   Databricks Premium Plan: Unity Catalog is available only on the Premium or Enterprise plans.\\n    •   Administrator Access: You need workspace and cloud admin permissions.\\n    •   Cloud Storage Configuration: A cloud storage bucket is required to store metadata (AWS S3, Azure\\n        Data Lake, or Google Cloud Storage).\\n    •   IAM Role/Permissions: Ensure proper IAM permissions for Unity Catalog to access your cloud\\n        storage.\\n2. Setting Up a Metastore\\n    Step 1: Create a Metastore\\n        •  Navigate to the Admin Console in Databricks.\\n        •  Go to the Metastore tab and click Create Metastore.\\n        •  Provide the following details:\\n               o   Name: A unique name for the metastore.\\n               o   Region: Select the cloud region (should match your workspace region).\\n               o   Cloud Storage: Provide the storage location (e.g., S3 bucket, ADLS container).\\n    Step 2: Assign Workspaces to the Metastore',\n",
       "    'md': '# Create Azure Databricks Workspace\\n\\n# Databricks Unity Catalog\\n\\n# Create Azure Data Lake Gen2\\n\\n# Access Connector for Databricks\\n\\n# Create Access Connector\\n\\n| Add role                      | Storage Blob Data Contributor                 |\\n| ----------------------------- | --------------------------------------------- |\\n| Databricks Contributor Role   | Default Storage                               |\\n| Create Unit Catalog Metastore | Enable Databricks Workspace for Unity Catalog |\\n\\n# Steps to Create a Unity Catalog Metastore\\n\\n# 1. Prerequisites\\n\\n- Databricks Premium Plan: Unity Catalog is available only on the Premium or Enterprise plans.\\n- Administrator Access: You need workspace and cloud admin permissions.\\n- Cloud Storage Configuration: A cloud storage bucket is required to store metadata (AWS S3, Azure Data Lake, or Google Cloud Storage).\\n- IAM Role/Permissions: Ensure proper IAM permissions for Unity Catalog to access your cloud storage.\\n\\n# 2. Setting Up a Metastore\\n\\n# Step 1: Create a Metastore\\n\\n- Navigate to the Admin Console in Databricks.\\n- Go to the Metastore tab and click Create Metastore.\\n- Provide the following details:\\n- Name: A unique name for the metastore.\\n- Region: Select the cloud region (should match your workspace region).\\n- Cloud Storage: Provide the storage location (e.g., S3 bucket, ADLS container).\\n\\n# Step 2: Assign Workspaces to the Metastore',\n",
       "    'images': [{'name': 'img_p16_1.png',\n",
       "      'height': 472.0,\n",
       "      'width': 1164.0,\n",
       "      'x': 36.0,\n",
       "      'y': 36.00000000000006,\n",
       "      'original_width': 1164,\n",
       "      'original_height': 472,\n",
       "      'type': None}],\n",
       "    'charts': [],\n",
       "    'tables': [],\n",
       "    'layout': [],\n",
       "    'items': [{'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Create Azure Databricks Workspace',\n",
       "      'md': '# Create Azure Databricks Workspace',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 86.0, 'y': 60.0, 'w': 427.0, 'h': 660.04}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Databricks Unity Catalog',\n",
       "      'md': '# Databricks Unity Catalog',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 74.0, 'y': 79.0, 'w': 259.0, 'h': 641.04}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Create Azure Data Lake Gen2',\n",
       "      'md': '# Create Azure Data Lake Gen2',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 345.0, 'y': 94.0, 'w': 137.0, 'h': 12.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Access Connector for Databricks',\n",
       "      'md': '# Access Connector for Databricks',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 87.0, 'y': 107.0, 'w': 246.0, 'h': 613.04}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Create Access Connector',\n",
       "      'md': '# Create Access Connector',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 108.0, 'y': 128.0, 'w': 356.0, 'h': 592.04}},\n",
       "     {'type': 'table',\n",
       "      'lvl': None,\n",
       "      'value': None,\n",
       "      'md': '| Add role                      | Storage Blob Data Contributor                 |\\n| ----------------------------- | --------------------------------------------- |\\n| Databricks Contributor Role   | Default Storage                               |\\n| Create Unit Catalog Metastore | Enable Databricks Workspace for Unity Catalog |',\n",
       "      'rows': [['Add role', 'Storage Blob Data Contributor'],\n",
       "       ['Databricks Contributor Role', 'Default Storage'],\n",
       "       ['Create Unit Catalog Metastore',\n",
       "        'Enable Databricks Workspace for Unity Catalog']],\n",
       "      'bBox': {'x': 265.0, 'y': 160.0, 'w': 300.0, 'h': 84.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Steps to Create a Unity Catalog Metastore',\n",
       "      'md': '# Steps to Create a Unity Catalog Metastore',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 93.0, 'w': 216.0, 'h': 627.04}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': '1. Prerequisites',\n",
       "      'md': '# 1. Prerequisites',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 381.0, 'w': 81.0, 'h': 12.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '- Databricks Premium Plan: Unity Catalog is available only on the Premium or Enterprise plans.\\n- Administrator Access: You need workspace and cloud admin permissions.\\n- Cloud Storage Configuration: A cloud storage bucket is required to store metadata (AWS S3, Azure Data Lake, or Google Cloud Storage).\\n- IAM Role/Permissions: Ensure proper IAM permissions for Unity Catalog to access your cloud storage.',\n",
       "      'md': '- Databricks Premium Plan: Unity Catalog is available only on the Premium or Enterprise plans.\\n- Administrator Access: You need workspace and cloud admin permissions.\\n- Cloud Storage Configuration: A cloud storage bucket is required to store metadata (AWS S3, Azure Data Lake, or Google Cloud Storage).\\n- IAM Role/Permissions: Ensure proper IAM permissions for Unity Catalog to access your cloud storage.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 72.0, 'y': 93.0, 'w': 487.0, 'h': 627.04}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': '2. Setting Up a Metastore',\n",
       "      'md': '# 2. Setting Up a Metastore',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 115.0, 'w': 133.0, 'h': 605.04}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Step 1: Create a Metastore',\n",
       "      'md': '# Step 1: Create a Metastore',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 54.0, 'y': 115.0, 'w': 139.0, 'h': 605.04}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '- Navigate to the Admin Console in Databricks.\\n- Go to the Metastore tab and click Create Metastore.\\n- Provide the following details:\\n- Name: A unique name for the metastore.\\n- Region: Select the cloud region (should match your workspace region).\\n- Cloud Storage: Provide the storage location (e.g., S3 bucket, ADLS container).',\n",
       "      'md': '- Navigate to the Admin Console in Databricks.\\n- Go to the Metastore tab and click Create Metastore.\\n- Provide the following details:\\n- Name: A unique name for the metastore.\\n- Region: Select the cloud region (should match your workspace region).\\n- Cloud Storage: Provide the storage location (e.g., S3 bucket, ADLS container).',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 86.0, 'y': 115.0, 'w': 426.0, 'h': 605.04}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Step 2: Assign Workspaces to the Metastore',\n",
       "      'md': '# Step 2: Assign Workspaces to the Metastore',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 54.0, 'y': 115.0, 'w': 226.0, 'h': 628.0}}],\n",
       "    'status': 'OK',\n",
       "    'links': [],\n",
       "    'width': 612.0,\n",
       "    'height': 792.0,\n",
       "    'triggeredAutoMode': False,\n",
       "    'parsingMode': 'accurate',\n",
       "    'structuredData': None,\n",
       "    'noStructuredContent': False,\n",
       "    'noTextContent': False},\n",
       "   {'page': 18,\n",
       "    'text': '        •  Once the metastore is created, you can attach it to one or more Databricks workspaces.\\n        •  In the Metastore tab, select Assign Workspace, and choose the workspace(s) to connect.\\n    Step 3: Configure IAM Roles\\n        •  Assign appropriate permissions for Unity Catalog to manage your data assets in the specified storage\\n           bucket.\\n        •  For AWS: Attach a policy with S3 access (e.g., s3:GetObject, s3:ListBucket).\\n        •  For Azure: Provide access to the ADLS container.\\n    Step 4: Enable Unity Catalog\\n        •  In your Databricks workspace, go to Admin Settings > Unity Catalog.\\n        •  Select the created metastore and enable Unity Catalog for the workspace.\\nUnity Catalog Object Model\\nUnity Catalog organizes your data assets into a structured hierarchy. Here’s a breakdown:\\n1. Metastore\\n    •   The top-most container for all data assets.\\n    •   A Metastore can span multiple Databricks workspaces but must reside in a single cloud region.\\n2. Catalogs\\n    •   Catalogs group related schemas and are often used to represent business units, teams, or projects.\\n    •   Example: marketing_catalog, finance_catalog.\\n3. Schemas (Databases)\\n    •   Schemas sit inside catalogs and group related tables, views, and functions.\\n    •   Example: A sales schema might contain customers, orders, and products tables.\\n4. Tables\\n    •   Tables are data objects where structured data is stored. Unity Catalog supports both:\\n           o   Managed Tables: Stored directly in the metastore’s cloud storage.\\n           o   External Tables: Point to external locations in your cloud storage.\\n5. Views\\n    •   Logical representations of data derived from tables using SQL queries.\\n    •   Example: A view named top_customers might filter the customers table to show only those with high\\n        purchase amounts.',\n",
       "    'md': '• Once the metastore is created, you can attach it to one or more Databricks workspaces.\\n\\n• In the Metastore tab, select Assign Workspace, and choose the workspace(s) to connect.\\n\\n# Step 3: Configure IAM Roles\\n\\n• Assign appropriate permissions for Unity Catalog to manage your data assets in the specified storage bucket.\\n\\n• For AWS: Attach a policy with S3 access (e.g., s3:GetObject, s3:ListBucket).\\n\\n• For Azure: Provide access to the ADLS container.\\n\\n# Step 4: Enable Unity Catalog\\n\\n• In your Databricks workspace, go to Admin Settings &gt; Unity Catalog.\\n\\n• Select the created metastore and enable Unity Catalog for the workspace.\\n\\n# Unity Catalog Object Model\\n\\nUnity Catalog organizes your data assets into a structured hierarchy. Here’s a breakdown:\\n\\n# 1. Metastore\\n\\n• The top-most container for all data assets.\\n\\n• A Metastore can span multiple Databricks workspaces but must reside in a single cloud region.\\n\\n# 2. Catalogs\\n\\n• Catalogs group related schemas and are often used to represent business units, teams, or projects.\\n\\n• Example: marketing_catalog, finance_catalog.\\n\\n# 3. Schemas (Databases)\\n\\n• Schemas sit inside catalogs and group related tables, views, and functions.\\n\\n• Example: A sales schema might contain customers, orders, and products tables.\\n\\n# 4. Tables\\n\\n• Tables are data objects where structured data is stored. Unity Catalog supports both:\\n\\n- Managed Tables: Stored directly in the metastore’s cloud storage.\\n- External Tables: Point to external locations in your cloud storage.\\n\\n# 5. Views\\n\\n• Logical representations of data derived from tables using SQL queries.\\n\\n• Example: A view named top_customers might filter the customers table to show only those with high purchase amounts.',\n",
       "    'images': [],\n",
       "    'charts': [],\n",
       "    'tables': [],\n",
       "    'layout': [],\n",
       "    'items': [{'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '• Once the metastore is created, you can attach it to one or more Databricks workspaces.\\n\\n• In the Metastore tab, select Assign Workspace, and choose the workspace(s) to connect.',\n",
       "      'md': '• Once the metastore is created, you can attach it to one or more Databricks workspaces.\\n\\n• In the Metastore tab, select Assign Workspace, and choose the workspace(s) to connect.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 54.0, 'y': 36.0, 'w': 465.0, 'h': 647.04}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Step 3: Configure IAM Roles',\n",
       "      'md': '# Step 3: Configure IAM Roles',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 54.0, 'y': 82.0, 'w': 152.0, 'h': 532.04}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '• Assign appropriate permissions for Unity Catalog to manage your data assets in the specified storage bucket.\\n\\n• For AWS: Attach a policy with S3 access (e.g., s3:GetObject, s3:ListBucket).\\n\\n• For Azure: Provide access to the ADLS container.',\n",
       "      'md': '• Assign appropriate permissions for Unity Catalog to manage your data assets in the specified storage bucket.\\n\\n• For AWS: Attach a policy with S3 access (e.g., s3:GetObject, s3:ListBucket).\\n\\n• For Azure: Provide access to the ADLS container.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 54.0, 'y': 38.04, 'w': 520.0, 'h': 645.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Step 4: Enable Unity Catalog',\n",
       "      'md': '# Step 4: Enable Unity Catalog',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 54.0, 'y': 188.0, 'w': 152.0, 'h': 426.04}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '• In your Databricks workspace, go to Admin Settings &gt; Unity Catalog.\\n\\n• Select the created metastore and enable Unity Catalog for the workspace.',\n",
       "      'md': '• In your Databricks workspace, go to Admin Settings &gt; Unity Catalog.\\n\\n• Select the created metastore and enable Unity Catalog for the workspace.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 54.0, 'y': 38.04, 'w': 390.0, 'h': 645.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Unity Catalog Object Model',\n",
       "      'md': '# Unity Catalog Object Model',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 281.0, 'w': 145.0, 'h': 333.04}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': 'Unity Catalog organizes your data assets into a structured hierarchy. Here’s a breakdown:',\n",
       "      'md': 'Unity Catalog organizes your data assets into a structured hierarchy. Here’s a breakdown:',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 305.0, 'w': 432.0, 'h': 309.04}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': '1. Metastore',\n",
       "      'md': '# 1. Metastore',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 328.0, 'w': 66.0, 'h': 286.04}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '• The top-most container for all data assets.\\n\\n• A Metastore can span multiple Databricks workspaces but must reside in a single cloud region.',\n",
       "      'md': '• The top-most container for all data assets.\\n\\n• A Metastore can span multiple Databricks workspaces but must reside in a single cloud region.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 54.0, 'y': 38.04, 'w': 480.0, 'h': 645.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': '2. Catalogs',\n",
       "      'md': '# 2. Catalogs',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 396.0, 'w': 60.0, 'h': 218.04}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '• Catalogs group related schemas and are often used to represent business units, teams, or projects.\\n\\n• Example: marketing_catalog, finance_catalog.',\n",
       "      'md': '• Catalogs group related schemas and are often used to represent business units, teams, or projects.\\n\\n• Example: marketing_catalog, finance_catalog.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 54.0, 'y': 38.04, 'w': 490.0, 'h': 645.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': '3. Schemas (Databases)',\n",
       "      'md': '# 3. Schemas (Databases)',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 465.0, 'w': 122.0, 'h': 12.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '• Schemas sit inside catalogs and group related tables, views, and functions.\\n\\n• Example: A sales schema might contain customers, orders, and products tables.',\n",
       "      'md': '• Schemas sit inside catalogs and group related tables, views, and functions.\\n\\n• Example: A sales schema might contain customers, orders, and products tables.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 54.0, 'y': 38.04, 'w': 401.0, 'h': 645.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': '4. Tables',\n",
       "      'md': '# 4. Tables',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 534.0, 'w': 47.0, 'h': 12.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '• Tables are data objects where structured data is stored. Unity Catalog supports both:\\n\\n- Managed Tables: Stored directly in the metastore’s cloud storage.\\n- External Tables: Point to external locations in your cloud storage.',\n",
       "      'md': '• Tables are data objects where structured data is stored. Unity Catalog supports both:\\n\\n- Managed Tables: Stored directly in the metastore’s cloud storage.\\n- External Tables: Point to external locations in your cloud storage.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 54.0, 'y': 38.04, 'w': 426.0, 'h': 645.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': '5. Views',\n",
       "      'md': '# 5. Views',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 625.0, 'w': 45.0, 'h': 12.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '• Logical representations of data derived from tables using SQL queries.\\n\\n• Example: A view named top_customers might filter the customers table to show only those with high purchase amounts.',\n",
       "      'md': '• Logical representations of data derived from tables using SQL queries.\\n\\n• Example: A view named top_customers might filter the customers table to show only those with high purchase amounts.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 54.0, 'y': 38.04, 'w': 504.0, 'h': 659.96}}],\n",
       "    'status': 'OK',\n",
       "    'links': [],\n",
       "    'width': 612.0,\n",
       "    'height': 792.0,\n",
       "    'triggeredAutoMode': False,\n",
       "    'parsingMode': 'accurate',\n",
       "    'structuredData': None,\n",
       "    'noStructuredContent': False,\n",
       "    'noTextContent': False},\n",
       "   {'page': 19,\n",
       "    'text': '6. Files\\n    •  Unity Catalog supports file-based data (e.g., CSV, JSON, Parquet) stored in cloud storage, enabling you\\n       to query files as tables.\\n                     Metastore: company metastore\\n                          Catalog: sales catalog\\n                                Schema   orders Schema\\n                                       Table: custoner_orders\\n                                       Table: product_sales\\n                                     View: topselling_products\\n                                Schema   marketing_schema\\n                                      Table:campaign_data\\n                                      External Table: external_clickstream_data\\n\\nSummary:\\nWhy Unity Catalogue?\\n    •  Streamlined Collaboration: Teams work with the same data while respecting access policies.\\n    •  Compliance and Security: Meet regulatory requirements with audit-ready logs and fine-grained\\n       permissions.\\n    •  Efficient Data Management: Easily organize and retrieve data with catalogs and schemas.\\nNow, let’s see how Job Workflows in Databricks make orchestrating complex data pipelines seamless and\\nefficient!\\nOrchestrating Data Pipelines with Job Workflows\\nIn any data-driven environment, automation is key to efficiency. Databricks Job Workflows allow you to\\nschedule, monitor, and automate tasks like ETL pipelines, machine learning model training, and data validation.\\nThis powerful feature ensures your workflows run seamlessly and reliably, freeing up your time for more\\ncritical tasks.\\nWhat is a Job Workflow?\\nA Job Workflow in Databricks is a sequence of tasks designed to execute one or multiple notebooks, Python\\nscripts, JAR files, or custom commands in a defined order. It allows:\\n    •  Task Orchestration: Automate dependencies between tasks.\\n    •  Scheduling: Set up jobs to run on a specific schedule (e.g., hourly, daily, or weekly).\\n    •  Error Handling: Define actions to take when tasks fail.',\n",
       "    'md': '# 6. Files\\n\\n- Unity Catalog supports file-based data (e.g., CSV, JSON, Parquet) stored in cloud storage, enabling you to query files as tables.\\n\\n| Metastore:      | company metastore           |\\n| --------------- | --------------------------- |\\n| Catalog:        | sales catalog               |\\n| Schema          | orders Schema               |\\n| Table:          | custoner\\\\_orders            |\\n| Table:          | product\\\\_sales              |\\n| View:           | topselling\\\\_products        |\\n| Schema          | marketing\\\\_schema           |\\n| Table:          | campaign\\\\_data              |\\n| External Table: | external\\\\_clickstream\\\\_data |\\n\\n# Summary:\\n\\n# Why Unity Catalogue?\\n\\n- Streamlined Collaboration: Teams work with the same data while respecting access policies.\\n- Compliance and Security: Meet regulatory requirements with audit-ready logs and fine-grained permissions.\\n- Efficient Data Management: Easily organize and retrieve data with catalogs and schemas.\\n\\n# Orchestrating Data Pipelines with Job Workflows\\n\\nIn any data-driven environment, automation is key to efficiency. Databricks Job Workflows allow you to schedule, monitor, and automate tasks like ETL pipelines, machine learning model training, and data validation. This powerful feature ensures your workflows run seamlessly and reliably, freeing up your time for more critical tasks.\\n\\n# What is a Job Workflow?\\n\\nA Job Workflow in Databricks is a sequence of tasks designed to execute one or multiple notebooks, Python scripts, JAR files, or custom commands in a defined order. It allows:\\n\\n- Task Orchestration: Automate dependencies between tasks.\\n- Scheduling: Set up jobs to run on a specific schedule (e.g., hourly, daily, or weekly).\\n- Error Handling: Define actions to take when tasks fail.',\n",
       "    'images': [{'name': 'img_p18_1.png',\n",
       "      'height': 235.0,\n",
       "      'width': 464.0,\n",
       "      'x': 132.0,\n",
       "      'y': 96.66999999999996,\n",
       "      'original_width': 464,\n",
       "      'original_height': 235,\n",
       "      'type': None}],\n",
       "    'charts': [],\n",
       "    'tables': [],\n",
       "    'layout': [],\n",
       "    'items': [{'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': '6. Files',\n",
       "      'md': '# 6. Files',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 36.0, 'w': 39.0, 'h': 12.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '- Unity Catalog supports file-based data (e.g., CSV, JSON, Parquet) stored in cloud storage, enabling you to query files as tables.',\n",
       "      'md': '- Unity Catalog supports file-based data (e.g., CSV, JSON, Parquet) stored in cloud storage, enabling you to query files as tables.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 72.0, 'y': 59.0, 'w': 498.0, 'h': 26.0}},\n",
       "     {'type': 'table',\n",
       "      'lvl': None,\n",
       "      'value': None,\n",
       "      'md': '| Metastore:      | company metastore           |\\n| --------------- | --------------------------- |\\n| Catalog:        | sales catalog               |\\n| Schema          | orders Schema               |\\n| Table:          | custoner\\\\_orders            |\\n| Table:          | product\\\\_sales              |\\n| View:           | topselling\\\\_products        |\\n| Schema          | marketing\\\\_schema           |\\n| Table:          | campaign\\\\_data              |\\n| External Table: | external\\\\_clickstream\\\\_data |',\n",
       "      'rows': [['Metastore:', 'company metastore'],\n",
       "       ['Catalog:', 'sales catalog'],\n",
       "       ['Schema', 'orders Schema'],\n",
       "       ['Table:', 'custoner_orders'],\n",
       "       ['Table:', 'product_sales'],\n",
       "       ['View:', 'topselling_products'],\n",
       "       ['Schema', 'marketing_schema'],\n",
       "       ['Table:', 'campaign_data'],\n",
       "       ['External Table:', 'external_clickstream_data']],\n",
       "      'bBox': {'x': 72.0, 'y': 102.0, 'w': 442.0, 'h': 331.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Summary:',\n",
       "      'md': '# Summary:',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 328.0, 'w': 60.0, 'h': 12.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Why Unity Catalogue?',\n",
       "      'md': '# Why Unity Catalogue?',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 351.0, 'w': 117.0, 'h': 12.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '- Streamlined Collaboration: Teams work with the same data while respecting access policies.\\n- Compliance and Security: Meet regulatory requirements with audit-ready logs and fine-grained permissions.\\n- Efficient Data Management: Easily organize and retrieve data with catalogs and schemas.',\n",
       "      'md': '- Streamlined Collaboration: Teams work with the same data while respecting access policies.\\n- Compliance and Security: Meet regulatory requirements with audit-ready logs and fine-grained permissions.\\n- Efficient Data Management: Easily organize and retrieve data with catalogs and schemas.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 72.0, 'y': 139.0, 'w': 466.0, 'h': 294.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Orchestrating Data Pipelines with Job Workflows',\n",
       "      'md': '# Orchestrating Data Pipelines with Job Workflows',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 504.96, 'w': 297.0, 'h': 14.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': 'In any data-driven environment, automation is key to efficiency. Databricks Job Workflows allow you to schedule, monitor, and automate tasks like ETL pipelines, machine learning model training, and data validation. This powerful feature ensures your workflows run seamlessly and reliably, freeing up your time for more critical tasks.',\n",
       "      'md': 'In any data-driven environment, automation is key to efficiency. Databricks Job Workflows allow you to schedule, monitor, and automate tasks like ETL pipelines, machine learning model training, and data validation. This powerful feature ensures your workflows run seamlessly and reliably, freeing up your time for more critical tasks.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 530.0, 'w': 537.0, 'h': 57.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'What is a Job Workflow?',\n",
       "      'md': '# What is a Job Workflow?',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 623.0, 'w': 130.0, 'h': 12.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': 'A Job Workflow in Databricks is a sequence of tasks designed to execute one or multiple notebooks, Python scripts, JAR files, or custom commands in a defined order. It allows:\\n\\n- Task Orchestration: Automate dependencies between tasks.\\n- Scheduling: Set up jobs to run on a specific schedule (e.g., hourly, daily, or weekly).\\n- Error Handling: Define actions to take when tasks fail.',\n",
       "      'md': 'A Job Workflow in Databricks is a sequence of tasks designed to execute one or multiple notebooks, Python scripts, JAR files, or custom commands in a defined order. It allows:\\n\\n- Task Orchestration: Automate dependencies between tasks.\\n- Scheduling: Set up jobs to run on a specific schedule (e.g., hourly, daily, or weekly).\\n- Error Handling: Define actions to take when tasks fail.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 646.0, 'w': 523.0, 'h': 95.0}}],\n",
       "    'status': 'OK',\n",
       "    'links': [],\n",
       "    'width': 612.0,\n",
       "    'height': 792.0,\n",
       "    'triggeredAutoMode': False,\n",
       "    'parsingMode': 'accurate',\n",
       "    'structuredData': None,\n",
       "    'noStructuredContent': False,\n",
       "    'noTextContent': False},\n",
       "   {'page': 20,\n",
       "    'text': '    •   Parameterization: Pass arguments dynamically for reusable workflows.\\nHow to Create a Job Workflow\\n    1. Access the Jobs Tab\\n        •  From the left-hand navigation menu, click Workflows.\\n        •  This will open the Jobs interface, where you can view and manage workflows.\\n    2. Create a New Job\\n        •  Click Create Job and give your job a name (e.g., \"Daily ETL Pipeline\").\\n    3. Add Tasks\\n    Tasks represent individual steps in your workflow. Here’s how to configure them:\\n        •  Task Name: Provide a descriptive name for the task.\\n        •  Task Type: Select the type of task to execute:\\n               o   Notebook: Run a specific Databricks notebook.\\n               o   Python Script: Execute a Python file from a connected storage.\\n               o   JAR File: For Spark applications.\\n        •  Cluster Settings: Assign a new or existing cluster for the task.\\n        •  Dependencies: Add dependency logic between tasks, ensuring tasks run in the correct order.\\n    4. Configure Parameters\\n        •  Pass parameters to notebooks or scripts to make workflows dynamic.\\n    5. Set the Schedule\\n        •  Schedule your workflow to run at specific intervals (e.g., daily at midnight).\\n        •  Alternatively, trigger jobs manually or via API calls.\\n    6. Define Notifications\\n        •  Add email or Slack notifications for success, failure, or task retries.\\nFeatures of Databricks Job Workflows\\n        1. Task Dependencies\\n        You can build complex workflows with interdependent tasks. For example:\\n           •   Task A: Load raw data into a Delta table.\\n           •   Task B: Transform data into a clean format.\\n           •   Task C: Train a machine learning model on the clean data.',\n",
       "    'md': '# Parameterization: Pass arguments dynamically for reusable workflows.\\n\\n# How to Create a Job Workflow\\n\\n1. # Access the Jobs Tab\\n\\n- From the left-hand navigation menu, click Workflows.\\n- This will open the Jobs interface, where you can view and manage workflows.\\n2. # Create a New Job\\n\\n- Click Create Job and give your job a name (e.g., \"Daily ETL Pipeline\").\\n3. # Add Tasks\\n\\nTasks represent individual steps in your workflow. Here’s how to configure them:\\n\\n- Task Name: Provide a descriptive name for the task.\\n- Task Type: Select the type of task to execute:\\n- Notebook: Run a specific Databricks notebook.\\n- Python Script: Execute a Python file from a connected storage.\\n- JAR File: For Spark applications.\\n- Cluster Settings: Assign a new or existing cluster for the task.\\n- Dependencies: Add dependency logic between tasks, ensuring tasks run in the correct order.\\n4. # Configure Parameters\\n\\n- Pass parameters to notebooks or scripts to make workflows dynamic.\\n5. # Set the Schedule\\n\\n- Schedule your workflow to run at specific intervals (e.g., daily at midnight).\\n- Alternatively, trigger jobs manually or via API calls.\\n6. # Define Notifications\\n\\n- Add email or Slack notifications for success, failure, or task retries.\\n\\n# Features of Databricks Job Workflows\\n\\n1. # Task Dependencies\\n\\nYou can build complex workflows with interdependent tasks. For example:\\n\\n- Task A: Load raw data into a Delta table.\\n- Task B: Transform data into a clean format.\\n- Task C: Train a machine learning model on the clean data.',\n",
       "    'images': [],\n",
       "    'charts': [],\n",
       "    'tables': [],\n",
       "    'layout': [],\n",
       "    'items': [{'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Parameterization: Pass arguments dynamically for reusable workflows.',\n",
       "      'md': '# Parameterization: Pass arguments dynamically for reusable workflows.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 72.0, 'y': 36.0, 'w': 352.0, 'h': 332.04}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'How to Create a Job Workflow',\n",
       "      'md': '# How to Create a Job Workflow',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 82.0, 'w': 162.0, 'h': 286.04}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '1. # Access the Jobs Tab\\n\\n- From the left-hand navigation menu, click Workflows.\\n- This will open the Jobs interface, where you can view and manage workflows.\\n2. # Create a New Job\\n\\n- Click Create Job and give your job a name (e.g., \"Daily ETL Pipeline\").\\n3. # Add Tasks\\n\\nTasks represent individual steps in your workflow. Here’s how to configure them:\\n\\n- Task Name: Provide a descriptive name for the task.\\n- Task Type: Select the type of task to execute:\\n- Notebook: Run a specific Databricks notebook.\\n- Python Script: Execute a Python file from a connected storage.\\n- JAR File: For Spark applications.\\n- Cluster Settings: Assign a new or existing cluster for the task.\\n- Dependencies: Add dependency logic between tasks, ensuring tasks run in the correct order.\\n4. # Configure Parameters\\n\\n- Pass parameters to notebooks or scripts to make workflows dynamic.\\n5. # Set the Schedule\\n\\n- Schedule your workflow to run at specific intervals (e.g., daily at midnight).\\n- Alternatively, trigger jobs manually or via API calls.\\n6. # Define Notifications\\n\\n- Add email or Slack notifications for success, failure, or task retries.',\n",
       "      'md': '1. # Access the Jobs Tab\\n\\n- From the left-hand navigation menu, click Workflows.\\n- This will open the Jobs interface, where you can view and manage workflows.\\n2. # Create a New Job\\n\\n- Click Create Job and give your job a name (e.g., \"Daily ETL Pipeline\").\\n3. # Add Tasks\\n\\nTasks represent individual steps in your workflow. Here’s how to configure them:\\n\\n- Task Name: Provide a descriptive name for the task.\\n- Task Type: Select the type of task to execute:\\n- Notebook: Run a specific Databricks notebook.\\n- Python Script: Execute a Python file from a connected storage.\\n- JAR File: For Spark applications.\\n- Cluster Settings: Assign a new or existing cluster for the task.\\n- Dependencies: Add dependency logic between tasks, ensuring tasks run in the correct order.\\n4. # Configure Parameters\\n\\n- Pass parameters to notebooks or scripts to make workflows dynamic.\\n5. # Set the Schedule\\n\\n- Schedule your workflow to run at specific intervals (e.g., daily at midnight).\\n- Alternatively, trigger jobs manually or via API calls.\\n6. # Define Notifications\\n\\n- Add email or Slack notifications for success, failure, or task retries.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 54.0, 'y': 127.0, 'w': 482.0, 'h': 450.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Features of Databricks Job Workflows',\n",
       "      'md': '# Features of Databricks Job Workflows',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 312.04, 'w': 200.0, 'h': 309.96}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '1. # Task Dependencies\\n\\nYou can build complex workflows with interdependent tasks. For example:\\n\\n- Task A: Load raw data into a Delta table.\\n- Task B: Transform data into a clean format.\\n- Task C: Train a machine learning model on the clean data.',\n",
       "      'md': '1. # Task Dependencies\\n\\nYou can build complex workflows with interdependent tasks. For example:\\n\\n- Task A: Load raw data into a Delta table.\\n- Task B: Transform data into a clean format.\\n- Task C: Train a machine learning model on the clean data.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 72.0, 'y': 312.04, 'w': 363.0, 'h': 424.96}}],\n",
       "    'status': 'OK',\n",
       "    'links': [],\n",
       "    'width': 612.0,\n",
       "    'height': 792.0,\n",
       "    'triggeredAutoMode': False,\n",
       "    'parsingMode': 'accurate',\n",
       "    'structuredData': None,\n",
       "    'noStructuredContent': False,\n",
       "    'noTextContent': False},\n",
       "   {'page': 21,\n",
       "    'text': '            •   If Task A fails, Task B and Task C won’t start.\\n        2. Retries and Alerts\\n            •   Configure retries for failed tasks (e.g., retry up to 3 times).\\n            •   Alerts notify you instantly via email, webhooks, or third-party integrations.\\n        3. Monitoring and Debugging\\n            •   Access detailed logs for each task in real time.\\n            •   View progress in the Jobs UI to identify bottlenecks or errors.\\n        4. Integration with REST APIs\\n            •   Use REST APIs to trigger jobs programmatically.\\n            •   Example API call for starting a job:\\n                         bash                                                             Copyᶜᶜᵈᵉ\\n                         curl   POST    Authorization: Bearer <TOKEN>\\n                            {\"job_id\": 12345}\\n                         https: / /<workspace-url>/api/2.0/jobs_ run-now\\nReal-World Use Case for Job Workflows\\nETL Pipeline Automation:\\n    1.  Task 1: Extract raw data from a cloud storage bucket (e.g., S3).\\n    2.  Task 2: Transform the data into a usable format using a Databricks notebook.\\n    3.  Task 3: Load the transformed data into a Delta Lake table for downstream analysis.\\n    4.  Task 4: Run a notebook to generate reports and email stakeholders.\\nMachine Learning Workflow:\\n    1.  Data Preprocessing: Load and clean raw data.\\n    2.  Model Training: Train an ML model using Databricks MLflow integration.\\n    3.  Model Deployment: Deploy the trained model using Databricks Serving.\\nWhy Use Job Workflows?\\n    •   Automation: Minimize manual effort and human error.\\n    •   Scalability: Execute workflows on-demand or at scale.\\n    •   Flexibility: Build workflows with any combination of notebooks, scripts, and JARs.\\n    •   Reliability: Monitor task progress and receive alerts for immediate action.',\n",
       "    'md': '# Job Workflows\\n\\n1. If Task A fails, Task B and Task C won’t start.\\n\\n# Retries and Alerts\\n\\n- Configure retries for failed tasks (e.g., retry up to 3 times).\\n- Alerts notify you instantly via email, webhooks, or third-party integrations.\\n\\n# Monitoring and Debugging\\n\\n- Access detailed logs for each task in real time.\\n- View progress in the Jobs UI to identify bottlenecks or errors.\\n\\n# Integration with REST APIs\\n\\n- Use REST APIs to trigger jobs programmatically.\\n- Example API call for starting a job:\\n\\ncurl -X POST -H \"Authorization: Bearer <TOKEN>\" -d \\'{\"job_id\": 12345}\\' https://<workspace-url>/api/2.0/jobs/run-now\\n\\n# Real-World Use Case for Job Workflows\\n\\n# ETL Pipeline Automation:\\n\\n1. Task 1: Extract raw data from a cloud storage bucket (e.g., S3).\\n2. Task 2: Transform the data into a usable format using a Databricks notebook.\\n3. Task 3: Load the transformed data into a Delta Lake table for downstream analysis.\\n4. Task 4: Run a notebook to generate reports and email stakeholders.\\n\\n# Machine Learning Workflow:\\n\\n1. Data Preprocessing: Load and clean raw data.\\n2. Model Training: Train an ML model using Databricks MLflow integration.\\n3. Model Deployment: Deploy the trained model using Databricks Serving.\\n\\n# Why Use Job Workflows?\\n\\n- Automation: Minimize manual effort and human error.\\n- Scalability: Execute workflows on-demand or at scale.\\n- Flexibility: Build workflows with any combination of notebooks, scripts, and JARs.\\n- Reliability: Monitor task progress and receive alerts for immediate action.',\n",
       "    'images': [{'name': 'img_p20_1.png',\n",
       "      'height': 150.0,\n",
       "      'width': 686.0,\n",
       "      'x': 142.95,\n",
       "      'y': 267.25,\n",
       "      'original_width': 686,\n",
       "      'original_height': 150,\n",
       "      'type': None}],\n",
       "    'charts': [],\n",
       "    'tables': [],\n",
       "    'layout': [],\n",
       "    'items': [{'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Job Workflows',\n",
       "      'md': '# Job Workflows',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 0.0, 'y': 0.0, 'w': 612.0, 'h': 792.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '1. If Task A fails, Task B and Task C won’t start.',\n",
       "      'md': '1. If Task A fails, Task B and Task C won’t start.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 54.0, 'y': 36.0, 'w': 277.0, 'h': 513.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Retries and Alerts',\n",
       "      'md': '# Retries and Alerts',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 0.0, 'y': 0.0, 'w': 612.0, 'h': 792.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '- Configure retries for failed tasks (e.g., retry up to 3 times).\\n- Alerts notify you instantly via email, webhooks, or third-party integrations.',\n",
       "      'md': '- Configure retries for failed tasks (e.g., retry up to 3 times).\\n- Alerts notify you instantly via email, webhooks, or third-party integrations.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 108.0, 'y': 84.0, 'w': 364.0, 'h': 35.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Monitoring and Debugging',\n",
       "      'md': '# Monitoring and Debugging',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 0.0, 'y': 0.0, 'w': 612.0, 'h': 792.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '- Access detailed logs for each task in real time.\\n- View progress in the Jobs UI to identify bottlenecks or errors.',\n",
       "      'md': '- Access detailed logs for each task in real time.\\n- View progress in the Jobs UI to identify bottlenecks or errors.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 108.0, 'y': 152.0, 'w': 299.0, 'h': 35.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Integration with REST APIs',\n",
       "      'md': '# Integration with REST APIs',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 0.0, 'y': 0.0, 'w': 612.0, 'h': 792.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '- Use REST APIs to trigger jobs programmatically.\\n- Example API call for starting a job:\\n\\ncurl -X POST -H \"Authorization: Bearer <TOKEN>\" -d \\'{\"job_id\": 12345}\\' https://<workspace-url>/api/2.0/jobs/run-now',\n",
       "      'md': '- Use REST APIs to trigger jobs programmatically.\\n- Example API call for starting a job:\\n\\ncurl -X POST -H \"Authorization: Bearer <TOKEN>\" -d \\'{\"job_id\": 12345}\\' https://<workspace-url>/api/2.0/jobs/run-now',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 54.0, 'y': 221.0, 'w': 295.0, 'h': 351.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Real-World Use Case for Job Workflows',\n",
       "      'md': '# Real-World Use Case for Job Workflows',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 376.0, 'w': 208.0, 'h': 12.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'ETL Pipeline Automation:',\n",
       "      'md': '# ETL Pipeline Automation:',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 399.0, 'w': 138.0, 'h': 12.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '1. Task 1: Extract raw data from a cloud storage bucket (e.g., S3).\\n2. Task 2: Transform the data into a usable format using a Databricks notebook.\\n3. Task 3: Load the transformed data into a Delta Lake table for downstream analysis.\\n4. Task 4: Run a notebook to generate reports and email stakeholders.',\n",
       "      'md': '1. Task 1: Extract raw data from a cloud storage bucket (e.g., S3).\\n2. Task 2: Transform the data into a usable format using a Databricks notebook.\\n3. Task 3: Load the transformed data into a Delta Lake table for downstream analysis.\\n4. Task 4: Run a notebook to generate reports and email stakeholders.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 54.0, 'y': 422.0, 'w': 422.0, 'h': 172.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Machine Learning Workflow:',\n",
       "      'md': '# Machine Learning Workflow:',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 514.0, 'w': 156.0, 'h': 12.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '1. Data Preprocessing: Load and clean raw data.\\n2. Model Training: Train an ML model using Databricks MLflow integration.\\n3. Model Deployment: Deploy the trained model using Databricks Serving.',\n",
       "      'md': '1. Data Preprocessing: Load and clean raw data.\\n2. Model Training: Train an ML model using Databricks MLflow integration.\\n3. Model Deployment: Deploy the trained model using Databricks Serving.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 54.0, 'y': 422.0, 'w': 385.0, 'h': 172.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Why Use Job Workflows?',\n",
       "      'md': '# Why Use Job Workflows?',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 628.0, 'w': 138.0, 'h': 12.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '- Automation: Minimize manual effort and human error.\\n- Scalability: Execute workflows on-demand or at scale.\\n- Flexibility: Build workflows with any combination of notebooks, scripts, and JARs.\\n- Reliability: Monitor task progress and receive alerts for immediate action.',\n",
       "      'md': '- Automation: Minimize manual effort and human error.\\n- Scalability: Execute workflows on-demand or at scale.\\n- Flexibility: Build workflows with any combination of notebooks, scripts, and JARs.\\n- Reliability: Monitor task progress and receive alerts for immediate action.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 72.0, 'y': 651.0, 'w': 408.0, 'h': 81.0}}],\n",
       "    'status': 'OK',\n",
       "    'links': [],\n",
       "    'width': 612.0,\n",
       "    'height': 792.0,\n",
       "    'triggeredAutoMode': False,\n",
       "    'parsingMode': 'accurate',\n",
       "    'structuredData': None,\n",
       "    'noStructuredContent': False,\n",
       "    'noTextContent': False},\n",
       "   {'page': 22,\n",
       "    'text': \"Databricks Job Workflows are your go-to tool for running reliable, scalable, and automated pipelines in any data\\nor machine learning project.\\nNow, let’s dive into how the SQL Editor in Databricks takes advanced SQL analytics to the next level for\\npowerful querying and insights!\\nAdvanced SQL Analytics with SQL Editor\\n        Whether you're building dashboards, running complex analytics, or optimizing queries, Databricks\\nSQL Editor is your go-to tool for querying your data with precision and speed.\\nWhy Use Databricks SQL Editor?\\n    •   Intuitive Interface: Simple UI for writing and executing queries.\\n    •   Advanced Visualizations: Create charts and dashboards directly from your query results.\\n    •   Seamless Integration: Query data across multiple tables, schemas, or catalogs.\\n    •   Collaboration: Share queries and dashboards with team members in just a few clicks.\\nKey Features of Databricks SQL Editor\\n    1.  Auto-Complete: The editor suggests tables, columns, and functions to speed up your workflow.\\n    2.  Query History: Access previously executed queries for reference or reuse.\\n    3.  Visualizations: Transform query results into bar charts, pie charts, or custom visualizations.\\n    4.  Parameterized Queries: Create flexible queries by defining variables.\\n    5.  Alerts: Set up notifications for specific query results (e.g., when a KPI crosses a threshold).\\nUsing Databricks SQL Editor: Step-by-Step Guide\\n    Step 1: Navigate to the SQL Editor\\n        •   From the Databricks navigation panel, click on SQL Editor under the SQL section.\\n        •   You’ll land on a page with a blank query editor and a toolbar for managing queries.\\n    Step 2: Connect to a SQL Warehouse\\n        •   Select a SQL Warehouse from the dropdown in the top-right corner of the editor.\\n        •   SQL Warehouses are computing clusters optimized for SQL workloads, ensuring fast query\\n            execution.\\n    Step 3: Write a Query\\n        •   Use the intuitive editor to write your SQL query. The editor provides syntax highlighting and auto-\\n            complete suggestions.\",\n",
       "    'md': \"# Databricks Job Workflows\\n\\nDatabricks Job Workflows are your go-to tool for running reliable, scalable, and automated pipelines in any data or machine learning project.\\n\\nNow, let’s dive into how the SQL Editor in Databricks takes advanced SQL analytics to the next level for powerful querying and insights!\\n\\n# Advanced SQL Analytics with SQL Editor\\n\\nWhether you're building dashboards, running complex analytics, or optimizing queries, Databricks SQL Editor is your go-to tool for querying your data with precision and speed.\\n\\n# Why Use Databricks SQL Editor?\\n\\n- Intuitive Interface: Simple UI for writing and executing queries.\\n- Advanced Visualizations: Create charts and dashboards directly from your query results.\\n- Seamless Integration: Query data across multiple tables, schemas, or catalogs.\\n- Collaboration: Share queries and dashboards with team members in just a few clicks.\\n\\n# Key Features of Databricks SQL Editor\\n\\n1. Auto-Complete: The editor suggests tables, columns, and functions to speed up your workflow.\\n2. Query History: Access previously executed queries for reference or reuse.\\n3. Visualizations: Transform query results into bar charts, pie charts, or custom visualizations.\\n4. Parameterized Queries: Create flexible queries by defining variables.\\n5. Alerts: Set up notifications for specific query results (e.g., when a KPI crosses a threshold).\\n\\n# Using Databricks SQL Editor: Step-by-Step Guide\\n\\n# Step 1: Navigate to the SQL Editor\\n\\n- From the Databricks navigation panel, click on SQL Editor under the SQL section.\\n- You’ll land on a page with a blank query editor and a toolbar for managing queries.\\n\\n# Step 2: Connect to a SQL Warehouse\\n\\n- Select a SQL Warehouse from the dropdown in the top-right corner of the editor.\\n- SQL Warehouses are computing clusters optimized for SQL workloads, ensuring fast query execution.\\n\\n# Step 3: Write a Query\\n\\n- Use the intuitive editor to write your SQL query. The editor provides syntax highlighting and auto-complete suggestions.\",\n",
       "    'images': [],\n",
       "    'charts': [],\n",
       "    'tables': [],\n",
       "    'layout': [],\n",
       "    'items': [{'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Databricks Job Workflows',\n",
       "      'md': '# Databricks Job Workflows',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 0.0, 'y': 0.0, 'w': 612.0, 'h': 792.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': 'Databricks Job Workflows are your go-to tool for running reliable, scalable, and automated pipelines in any data or machine learning project.\\n\\nNow, let’s dive into how the SQL Editor in Databricks takes advanced SQL analytics to the next level for powerful querying and insights!',\n",
       "      'md': 'Databricks Job Workflows are your go-to tool for running reliable, scalable, and automated pipelines in any data or machine learning project.\\n\\nNow, let’s dive into how the SQL Editor in Databricks takes advanced SQL analytics to the next level for powerful querying and insights!',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 36.0, 'w': 539.0, 'h': 87.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Advanced SQL Analytics with SQL Editor',\n",
       "      'md': '# Advanced SQL Analytics with SQL Editor',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 135.96, 'w': 253.0, 'h': 14.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': \"Whether you're building dashboards, running complex analytics, or optimizing queries, Databricks SQL Editor is your go-to tool for querying your data with precision and speed.\",\n",
       "      'md': \"Whether you're building dashboards, running complex analytics, or optimizing queries, Databricks SQL Editor is your go-to tool for querying your data with precision and speed.\",\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 162.0, 'w': 514.0, 'h': 27.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Why Use Databricks SQL Editor?',\n",
       "      'md': '# Why Use Databricks SQL Editor?',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 223.0, 'w': 177.0, 'h': 12.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '- Intuitive Interface: Simple UI for writing and executing queries.\\n- Advanced Visualizations: Create charts and dashboards directly from your query results.\\n- Seamless Integration: Query data across multiple tables, schemas, or catalogs.\\n- Collaboration: Share queries and dashboards with team members in just a few clicks.',\n",
       "      'md': '- Intuitive Interface: Simple UI for writing and executing queries.\\n- Advanced Visualizations: Create charts and dashboards directly from your query results.\\n- Seamless Integration: Query data across multiple tables, schemas, or catalogs.\\n- Collaboration: Share queries and dashboards with team members in just a few clicks.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 72.0, 'y': 246.0, 'w': 459.0, 'h': 80.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Key Features of Databricks SQL Editor',\n",
       "      'md': '# Key Features of Databricks SQL Editor',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 360.0, 'w': 206.0, 'h': 12.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '1. Auto-Complete: The editor suggests tables, columns, and functions to speed up your workflow.\\n2. Query History: Access previously executed queries for reference or reuse.\\n3. Visualizations: Transform query results into bar charts, pie charts, or custom visualizations.\\n4. Parameterized Queries: Create flexible queries by defining variables.\\n5. Alerts: Set up notifications for specific query results (e.g., when a KPI crosses a threshold).',\n",
       "      'md': '1. Auto-Complete: The editor suggests tables, columns, and functions to speed up your workflow.\\n2. Query History: Access previously executed queries for reference or reuse.\\n3. Visualizations: Transform query results into bar charts, pie charts, or custom visualizations.\\n4. Parameterized Queries: Create flexible queries by defining variables.\\n5. Alerts: Set up notifications for specific query results (e.g., when a KPI crosses a threshold).',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 54.0, 'y': 383.0, 'w': 504.0, 'h': 104.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Using Databricks SQL Editor: Step-by-Step Guide',\n",
       "      'md': '# Using Databricks SQL Editor: Step-by-Step Guide',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 521.0, 'w': 262.0, 'h': 12.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Step 1: Navigate to the SQL Editor',\n",
       "      'md': '# Step 1: Navigate to the SQL Editor',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 54.0, 'y': 543.0, 'w': 181.0, 'h': 12.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '- From the Databricks navigation panel, click on SQL Editor under the SQL section.\\n- You’ll land on a page with a blank query editor and a toolbar for managing queries.',\n",
       "      'md': '- From the Databricks navigation panel, click on SQL Editor under the SQL section.\\n- You’ll land on a page with a blank query editor and a toolbar for managing queries.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 90.0, 'y': 566.0, 'w': 403.0, 'h': 35.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Step 2: Connect to a SQL Warehouse',\n",
       "      'md': '# Step 2: Connect to a SQL Warehouse',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 54.0, 'y': 612.0, 'w': 192.0, 'h': 12.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '- Select a SQL Warehouse from the dropdown in the top-right corner of the editor.\\n- SQL Warehouses are computing clusters optimized for SQL workloads, ensuring fast query execution.',\n",
       "      'md': '- Select a SQL Warehouse from the dropdown in the top-right corner of the editor.\\n- SQL Warehouses are computing clusters optimized for SQL workloads, ensuring fast query execution.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 90.0, 'y': 635.0, 'w': 438.0, 'h': 50.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Step 3: Write a Query',\n",
       "      'md': '# Step 3: Write a Query',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 54.0, 'y': 696.0, 'w': 115.0, 'h': 12.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '- Use the intuitive editor to write your SQL query. The editor provides syntax highlighting and auto-complete suggestions.',\n",
       "      'md': '- Use the intuitive editor to write your SQL query. The editor provides syntax highlighting and auto-complete suggestions.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 90.0, 'y': 719.0, 'w': 474.0, 'h': 26.0}}],\n",
       "    'status': 'OK',\n",
       "    'links': [],\n",
       "    'width': 612.0,\n",
       "    'height': 792.0,\n",
       "    'triggeredAutoMode': False,\n",
       "    'parsingMode': 'accurate',\n",
       "    'structuredData': None,\n",
       "    'noStructuredContent': False,\n",
       "    'noTextContent': False},\n",
       "   {'page': 23,\n",
       "    'text': '                                                                                            Copyᶜᵒᵈᵉ\\n                       SELECT product name, SUM(sales_amount) AS total_sales\\n                       FROM sales catalog.orders_schema. product_sales\\n                       WHERE orderdate    2024\\n                       GROUP BY product name\\n                       ORDER   total_sales DESC;\\n    Step 4: Visualize Query Results\\n        •   Run the query and view the results in a tabular format.\\n        •   Click on the Visualization tab and choose a chart type (e.g., bar chart, line graph).\\n        •   Customize the chart by selecting fields, labels, and colors.\\n    Step 5: Save and Share\\n        •   Save your query and visualization for future use.\\n        •   Share it with your team or embed it into a dashboard for real-time updates.\\n\\n\\nNow, let’s explore how you can transform your queries into interactive dashboards for real-time insights and\\ndata storytelling!\\nCreating Dashboards from Queries\\nDashboards in Databricks allow you to present multiple visualizations and insights in one place. Here’s how:\\n    1.  Save a Query as a Visualization: From the SQL Editor, save your query with its visualization.\\n    2.  Create a New Dashboard: Go to the Dashboards tab, click on Create Dashboard, and add your saved\\n        visualizations.\\n    3.  Organize Your Dashboard: Drag and drop visualizations to arrange them.\\n    4.  Schedule Updates: Configure your dashboard to refresh at specific intervals for real-time insights.\\nBest Practices for Advanced SQL Analytics\\n    •   Optimize Queries: Use query filters, indexed tables, and partitioned datasets for faster execution.\\n    •   Leverage Functions: Use built-in SQL functions like AVG(), CASE, and WINDOW functions for\\n        advanced analytics.\\n    •   Enable Alerts: Stay proactive by setting alerts for KPIs or thresholds critical to your business.\\n    •   Collaborate: Share queries and dashboards with relevant stakeholders to ensure alignment.',\n",
       "    'md': '# Copyᶜᵒᵈᵉ\\n\\nSELECT product name, SUM(sales_amount) AS total_sales\\n\\nFROM sales catalog.orders_schema.product_sales\\n\\nWHERE orderdate 2024\\n\\nGROUP BY product name\\n\\nORDER total_sales DESC;\\n\\n# Step 4: Visualize Query Results\\n\\n- Run the query and view the results in a tabular format.\\n- Click on the Visualization tab and choose a chart type (e.g., bar chart, line graph).\\n- Customize the chart by selecting fields, labels, and colors.\\n\\n# Step 5: Save and Share\\n\\n- Save your query and visualization for future use.\\n- Share it with your team or embed it into a dashboard for real-time updates.\\n\\nNow, let’s explore how you can transform your queries into interactive dashboards for real-time insights and data storytelling!\\n\\n# Creating Dashboards from Queries\\n\\nDashboards in Databricks allow you to present multiple visualizations and insights in one place. Here’s how:\\n\\n1. Save a Query as a Visualization: From the SQL Editor, save your query with its visualization.\\n2. Create a New Dashboard: Go to the Dashboards tab, click on Create Dashboard, and add your saved visualizations.\\n3. Organize Your Dashboard: Drag and drop visualizations to arrange them.\\n4. Schedule Updates: Configure your dashboard to refresh at specific intervals for real-time insights.\\n\\n# Best Practices for Advanced SQL Analytics\\n\\n- Optimize Queries: Use query filters, indexed tables, and partitioned datasets for faster execution.\\n- Leverage Functions: Use built-in SQL functions like AVG(), CASE, and WINDOW functions for advanced analytics.\\n- Enable Alerts: Stay proactive by setting alerts for KPIs or thresholds critical to your business.\\n- Collaborate: Share queries and dashboards with relevant stakeholders to ensure alignment.',\n",
       "    'images': [{'name': 'img_p22_1.png',\n",
       "      'height': 206.0,\n",
       "      'width': 701.0,\n",
       "      'x': 129.5,\n",
       "      'y': 36.0,\n",
       "      'original_width': 701,\n",
       "      'original_height': 206,\n",
       "      'type': None}],\n",
       "    'charts': [],\n",
       "    'tables': [],\n",
       "    'layout': [],\n",
       "    'items': [{'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Copyᶜᵒᵈᵉ',\n",
       "      'md': '# Copyᶜᵒᵈᵉ',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 454.0, 'y': 45.0, 'w': 32.0, 'h': 12.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': 'SELECT product name, SUM(sales_amount) AS total_sales\\n\\nFROM sales catalog.orders_schema.product_sales\\n\\nWHERE orderdate 2024\\n\\nGROUP BY product name\\n\\nORDER total_sales DESC;',\n",
       "      'md': 'SELECT product name, SUM(sales_amount) AS total_sales\\n\\nFROM sales catalog.orders_schema.product_sales\\n\\nWHERE orderdate 2024\\n\\nGROUP BY product name\\n\\nORDER total_sales DESC;',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 142.0, 'y': 69.0, 'w': 218.0, 'h': 60.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Step 4: Visualize Query Results',\n",
       "      'md': '# Step 4: Visualize Query Results',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 54.0, 'y': 154.0, 'w': 163.0, 'h': 12.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '- Run the query and view the results in a tabular format.\\n- Click on the Visualization tab and choose a chart type (e.g., bar chart, line graph).\\n- Customize the chart by selecting fields, labels, and colors.',\n",
       "      'md': '- Run the query and view the results in a tabular format.\\n- Click on the Visualization tab and choose a chart type (e.g., bar chart, line graph).\\n- Customize the chart by selecting fields, labels, and colors.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 90.0, 'y': 177.0, 'w': 398.0, 'h': 58.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Step 5: Save and Share',\n",
       "      'md': '# Step 5: Save and Share',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 54.0, 'y': 246.0, 'w': 120.0, 'h': 12.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '- Save your query and visualization for future use.\\n- Share it with your team or embed it into a dashboard for real-time updates.\\n\\nNow, let’s explore how you can transform your queries into interactive dashboards for real-time insights and data storytelling!',\n",
       "      'md': '- Save your query and visualization for future use.\\n- Share it with your team or embed it into a dashboard for real-time updates.\\n\\nNow, let’s explore how you can transform your queries into interactive dashboards for real-time insights and data storytelling!',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 269.0, 'w': 520.0, 'h': 141.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Creating Dashboards from Queries',\n",
       "      'md': '# Creating Dashboards from Queries',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 420.96, 'w': 211.0, 'h': 14.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': 'Dashboards in Databricks allow you to present multiple visualizations and insights in one place. Here’s how:\\n\\n1. Save a Query as a Visualization: From the SQL Editor, save your query with its visualization.\\n2. Create a New Dashboard: Go to the Dashboards tab, click on Create Dashboard, and add your saved visualizations.\\n3. Organize Your Dashboard: Drag and drop visualizations to arrange them.\\n4. Schedule Updates: Configure your dashboard to refresh at specific intervals for real-time insights.',\n",
       "      'md': 'Dashboards in Databricks allow you to present multiple visualizations and insights in one place. Here’s how:\\n\\n1. Save a Query as a Visualization: From the SQL Editor, save your query with its visualization.\\n2. Create a New Dashboard: Go to the Dashboards tab, click on Create Dashboard, and add your saved visualizations.\\n3. Organize Your Dashboard: Drag and drop visualizations to arrange them.\\n4. Schedule Updates: Configure your dashboard to refresh at specific intervals for real-time insights.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 446.0, 'w': 538.0, 'h': 119.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Best Practices for Advanced SQL Analytics',\n",
       "      'md': '# Best Practices for Advanced SQL Analytics',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 599.0, 'w': 223.0, 'h': 12.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '- Optimize Queries: Use query filters, indexed tables, and partitioned datasets for faster execution.\\n- Leverage Functions: Use built-in SQL functions like AVG(), CASE, and WINDOW functions for advanced analytics.\\n- Enable Alerts: Stay proactive by setting alerts for KPIs or thresholds critical to your business.\\n- Collaborate: Share queries and dashboards with relevant stakeholders to ensure alignment.',\n",
       "      'md': '- Optimize Queries: Use query filters, indexed tables, and partitioned datasets for faster execution.\\n- Leverage Functions: Use built-in SQL functions like AVG(), CASE, and WINDOW functions for advanced analytics.\\n- Enable Alerts: Stay proactive by setting alerts for KPIs or thresholds critical to your business.\\n- Collaborate: Share queries and dashboards with relevant stakeholders to ensure alignment.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 72.0, 'y': 621.0, 'w': 479.0, 'h': 96.0}}],\n",
       "    'status': 'OK',\n",
       "    'links': [],\n",
       "    'width': 612.0,\n",
       "    'height': 792.0,\n",
       "    'triggeredAutoMode': False,\n",
       "    'parsingMode': 'accurate',\n",
       "    'structuredData': None,\n",
       "    'noStructuredContent': False,\n",
       "    'noTextContent': False},\n",
       "   {'page': 24,\n",
       "    'text': 'Now, let’s dive into how Databricks simplifies building and deploying machine learning models to turn data\\ninto actionable intelligence!\\nBuilding Machine Learning Models\\n        Whether you\\'re working on predictive analytics, recommendation systems, or advanced AI models,\\nDatabricks equips you with the tools and integrations to seamlessly transition from experimentation to\\nproduction.\\nWhy Use Databricks for Machine Learning?\\n    1.  Collaborative Environment: Work on shared notebooks with built-in libraries for ML.\\n    2.  Scalability: Process large datasets efficiently with distributed computing on Apache Spark.\\n    3.  Integration with ML Tools: Pre-integrated with ML frameworks like TensorFlow, PyTorch, Scikit-\\n        learn, and XGBoost.\\n    4.  MLflow for Lifecycle Management: Simplify experiment tracking, model versioning, and deployment.\\n    5.  Feature Store: Share and reuse pre-engineered features across teams.\\nThe Machine Learning Workflow in Databricks\\n    1.  Data Preparation\\n    •   Use Databricks notebooks to clean and preprocess your data.\\n    •   Transform datasets into features ready for ML using libraries like pandas, PySpark, or Databricks\\'\\n        Feature Store.\\n    Example:\\n                   Python                                                                          Copy:code\\n                   from pyspark.sql.functions import col\\n                   training_data    ral_data.filter(col(\"target\")     1) .select(\"featurel\"  \"feature2\\n    2.  Model Training\\n    •   Leverage libraries like Scikit-learn, TensorFlow, or PyTorch to train models.\\n    •   Use Spark’s distributed MLlib library for large-scale datasets.\\n    Example:\\n                  Pytnon                                                                     Copy \\'ᶜᵒᶜᵉ\\n                  from sklearn.ensemble import RandomForestclassifier\\n                 model   RandomForestclassifier(n_estimators-108  max_depth-5)\\n                 model.fit(X_train, Y_train)\\n    3.  Experiment Tracking with MLflow',\n",
       "    'md': '# Now, let’s dive into how Databricks simplifies building and deploying machine learning models to turn data into actionable intelligence!\\n\\n# Building Machine Learning Models\\n\\nWhether you\\'re working on predictive analytics, recommendation systems, or advanced AI models, Databricks equips you with the tools and integrations to seamlessly transition from experimentation to production.\\n\\n# Why Use Databricks for Machine Learning?\\n\\n1. Collaborative Environment: Work on shared notebooks with built-in libraries for ML.\\n2. Scalability: Process large datasets efficiently with distributed computing on Apache Spark.\\n3. Integration with ML Tools: Pre-integrated with ML frameworks like TensorFlow, PyTorch, Scikit-learn, and XGBoost.\\n4. MLflow for Lifecycle Management: Simplify experiment tracking, model versioning, and deployment.\\n5. Feature Store: Share and reuse pre-engineered features across teams.\\n\\n# The Machine Learning Workflow in Databricks\\n\\n1. # Data Preparation\\n\\n- Use Databricks notebooks to clean and preprocess your data.\\n- Transform datasets into features ready for ML using libraries like pandas, PySpark, or Databricks\\' Feature Store.\\n\\nExample:\\n\\nfrom pyspark.sql.functions import col\\ntraining_data = ral_data.filter(col(\"target\") == 1).select(\"feature1\", \"feature2\")\\n2. # Model Training\\n\\n- Leverage libraries like Scikit-learn, TensorFlow, or PyTorch to train models.\\n- Use Spark’s distributed MLlib library for large-scale datasets.\\n\\nExample:\\n\\nfrom sklearn.ensemble import RandomForestClassifier\\nmodel = RandomForestClassifier(n_estimators=108, max_depth=5)\\nmodel.fit(X_train, Y_train)\\n3. # Experiment Tracking with MLflow',\n",
       "    'images': [{'name': 'img_p23_1.png',\n",
       "      'height': 131.0,\n",
       "      'width': 693.0,\n",
       "      'x': 111.0,\n",
       "      'y': 455.09000000000003,\n",
       "      'original_width': 693,\n",
       "      'original_height': 131,\n",
       "      'type': None},\n",
       "     {'name': 'img_p23_2.png',\n",
       "      'height': 152.0,\n",
       "      'width': 688.0,\n",
       "      'x': 104.25,\n",
       "      'y': 624.771,\n",
       "      'original_width': 688,\n",
       "      'original_height': 152,\n",
       "      'type': None}],\n",
       "    'charts': [],\n",
       "    'tables': [],\n",
       "    'layout': [],\n",
       "    'items': [{'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Now, let’s dive into how Databricks simplifies building and deploying machine learning models to turn data into actionable intelligence!',\n",
       "      'md': '# Now, let’s dive into how Databricks simplifies building and deploying machine learning models to turn data into actionable intelligence!',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 36.0, 'w': 518.0, 'h': 650.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Building Machine Learning Models',\n",
       "      'md': '# Building Machine Learning Models',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 74.96, 'w': 213.0, 'h': 611.04}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': \"Whether you're working on predictive analytics, recommendation systems, or advanced AI models, Databricks equips you with the tools and integrations to seamlessly transition from experimentation to production.\",\n",
       "      'md': \"Whether you're working on predictive analytics, recommendation systems, or advanced AI models, Databricks equips you with the tools and integrations to seamlessly transition from experimentation to production.\",\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 102.0, 'w': 512.0, 'h': 584.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Why Use Databricks for Machine Learning?',\n",
       "      'md': '# Why Use Databricks for Machine Learning?',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 154.0, 'w': 231.0, 'h': 12.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '1. Collaborative Environment: Work on shared notebooks with built-in libraries for ML.\\n2. Scalability: Process large datasets efficiently with distributed computing on Apache Spark.\\n3. Integration with ML Tools: Pre-integrated with ML frameworks like TensorFlow, PyTorch, Scikit-learn, and XGBoost.\\n4. MLflow for Lifecycle Management: Simplify experiment tracking, model versioning, and deployment.\\n5. Feature Store: Share and reuse pre-engineered features across teams.',\n",
       "      'md': '1. Collaborative Environment: Work on shared notebooks with built-in libraries for ML.\\n2. Scalability: Process large datasets efficiently with distributed computing on Apache Spark.\\n3. Integration with ML Tools: Pre-integrated with ML frameworks like TensorFlow, PyTorch, Scikit-learn, and XGBoost.\\n4. MLflow for Lifecycle Management: Simplify experiment tracking, model versioning, and deployment.\\n5. Feature Store: Share and reuse pre-engineered features across teams.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 54.0, 'y': 177.0, 'w': 523.0, 'h': 559.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'The Machine Learning Workflow in Databricks',\n",
       "      'md': '# The Machine Learning Workflow in Databricks',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 329.0, 'w': 247.0, 'h': 12.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '1. # Data Preparation\\n\\n- Use Databricks notebooks to clean and preprocess your data.\\n- Transform datasets into features ready for ML using libraries like pandas, PySpark, or Databricks\\' Feature Store.\\n\\nExample:\\n\\nfrom pyspark.sql.functions import col\\ntraining_data = ral_data.filter(col(\"target\") == 1).select(\"feature1\", \"feature2\")\\n2. # Model Training\\n\\n- Leverage libraries like Scikit-learn, TensorFlow, or PyTorch to train models.\\n- Use Spark’s distributed MLlib library for large-scale datasets.\\n\\nExample:\\n\\nfrom sklearn.ensemble import RandomForestClassifier\\nmodel = RandomForestClassifier(n_estimators=108, max_depth=5)\\nmodel.fit(X_train, Y_train)\\n3. # Experiment Tracking with MLflow',\n",
       "      'md': '1. # Data Preparation\\n\\n- Use Databricks notebooks to clean and preprocess your data.\\n- Transform datasets into features ready for ML using libraries like pandas, PySpark, or Databricks\\' Feature Store.\\n\\nExample:\\n\\nfrom pyspark.sql.functions import col\\ntraining_data = ral_data.filter(col(\"target\") == 1).select(\"feature1\", \"feature2\")\\n2. # Model Training\\n\\n- Leverage libraries like Scikit-learn, TensorFlow, or PyTorch to train models.\\n- Use Spark’s distributed MLlib library for large-scale datasets.\\n\\nExample:\\n\\nfrom sklearn.ensemble import RandomForestClassifier\\nmodel = RandomForestClassifier(n_estimators=108, max_depth=5)\\nmodel.fit(X_train, Y_train)\\n3. # Experiment Tracking with MLflow',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 54.0, 'y': 177.0, 'w': 487.0, 'h': 559.0}}],\n",
       "    'status': 'OK',\n",
       "    'links': [],\n",
       "    'width': 612.0,\n",
       "    'height': 792.0,\n",
       "    'triggeredAutoMode': False,\n",
       "    'parsingMode': 'accurate',\n",
       "    'structuredData': None,\n",
       "    'noStructuredContent': False,\n",
       "    'noTextContent': False},\n",
       "   {'page': 25,\n",
       "    'text': '   •   Track metrics like accuracy, loss, and hyperparameters using MLflow Tracking.\\n   •   Automatically log experiments for easy comparison.\\n   Example:\\n          python                                                                              Copy:ᶜᵒᵈᵉ\\n          import mlflow\\n         with mlflow.start_run():\\n              mlflow.log_param( max_ depth\"_\\n              mlflow.log_metric(\"accuracy\\'     0.95)\\n   4.  Model Deployment\\n   •   Deploy models as REST APIs using MLflow Models.\\n   •   Use Databricks Model Serving for real-time predictions.\\n   5.  Monitoring and Optimization\\n   •   Continuously monitor model performance using built-in logging tools.\\n   •   Update models with new data or retrain as needed.\\n\\n\\nMachine Learning Tools in Databricks\\n   1.  MLflow\\n       •   End-to-end ML lifecycle management: tracking, packaging, and deploying models.\\n   2.  Feature Store\\n       •   Centralized repository for sharing pre-computed features across models.\\n   3.  Databricks AutoML\\n       •   Automatically generate ML pipelines with feature engineering and hyperparameter tuning.\\n   4.  Notebooks\\n       •   Collaborative, interactive notebooks for exploring data and building models.\\n   5.  Integrations\\n       •   Supports frameworks like TensorFlow, PyTorch, Keras, and XGBoost.',\n",
       "    'md': '# Machine Learning Tools in Databricks\\n\\n# 1. MLflow\\n\\n- End-to-end ML lifecycle management: tracking, packaging, and deploying models.\\n\\n# 2. Feature Store\\n\\n- Centralized repository for sharing pre-computed features across models.\\n\\n# 3. Databricks AutoML\\n\\n- Automatically generate ML pipelines with feature engineering and hyperparameter tuning.\\n\\n# 4. Notebooks\\n\\n- Collaborative, interactive notebooks for exploring data and building models.\\n\\n# 5. Integrations\\n\\n- Supports frameworks like TensorFlow, PyTorch, Keras, and XGBoost.\\n\\n# Model Deployment\\n\\n- Deploy models as REST APIs using MLflow Models.\\n- Use Databricks Model Serving for real-time predictions.\\n\\n# Monitoring and Optimization\\n\\n- Continuously monitor model performance using built-in logging tools.\\n- Update models with new data or retrain as needed.\\n\\n# Track metrics like accuracy, loss, and hyperparameters using MLflow Tracking.\\n\\n- Automatically log experiments for easy comparison.\\n\\n# Example:\\n\\npython\\nimport mlflow\\nwith mlflow.start_run():\\nmlflow.log_param(\"max_depth\", 5)\\nmlflow.log_metric(\"accuracy\", 0.95)',\n",
       "    'images': [{'name': 'img_p24_1.png',\n",
       "      'height': 177.0,\n",
       "      'width': 689.0,\n",
       "      'x': 71.25,\n",
       "      'y': 104.67,\n",
       "      'original_width': 689,\n",
       "      'original_height': 177,\n",
       "      'type': None}],\n",
       "    'charts': [],\n",
       "    'tables': [],\n",
       "    'layout': [],\n",
       "    'items': [{'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Machine Learning Tools in Databricks',\n",
       "      'md': '# Machine Learning Tools in Databricks',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 467.0, 'w': 200.0, 'h': 12.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': '1. MLflow',\n",
       "      'md': '# 1. MLflow',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 54.0, 'y': 490.0, 'w': 62.0, 'h': 12.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '- End-to-end ML lifecycle management: tracking, packaging, and deploying models.',\n",
       "      'md': '- End-to-end ML lifecycle management: tracking, packaging, and deploying models.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 90.0, 'y': 514.0, 'w': 402.0, 'h': 12.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': '2. Feature Store',\n",
       "      'md': '# 2. Feature Store',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 54.0, 'y': 537.0, 'w': 90.0, 'h': 12.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '- Centralized repository for sharing pre-computed features across models.',\n",
       "      'md': '- Centralized repository for sharing pre-computed features across models.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 90.0, 'y': 561.0, 'w': 349.0, 'h': 12.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': '3. Databricks AutoML',\n",
       "      'md': '# 3. Databricks AutoML',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 54.0, 'y': 584.0, 'w': 124.0, 'h': 12.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '- Automatically generate ML pipelines with feature engineering and hyperparameter tuning.',\n",
       "      'md': '- Automatically generate ML pipelines with feature engineering and hyperparameter tuning.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 90.0, 'y': 608.0, 'w': 437.0, 'h': 12.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': '4. Notebooks',\n",
       "      'md': '# 4. Notebooks',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 54.0, 'y': 236.0, 'w': 75.0, 'h': 406.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '- Collaborative, interactive notebooks for exploring data and building models.',\n",
       "      'md': '- Collaborative, interactive notebooks for exploring data and building models.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 72.0, 'y': 630.0, 'w': 388.0, 'h': 36.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': '5. Integrations',\n",
       "      'md': '# 5. Integrations',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 54.0, 'y': 330.0, 'w': 83.0, 'h': 359.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '- Supports frameworks like TensorFlow, PyTorch, Keras, and XGBoost.',\n",
       "      'md': '- Supports frameworks like TensorFlow, PyTorch, Keras, and XGBoost.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 90.0, 'y': 701.0, 'w': 341.0, 'h': 12.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Model Deployment',\n",
       "      'md': '# Model Deployment',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 72.0, 'y': 236.0, 'w': 98.0, 'h': 12.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '- Deploy models as REST APIs using MLflow Models.\\n- Use Databricks Model Serving for real-time predictions.',\n",
       "      'md': '- Deploy models as REST APIs using MLflow Models.\\n- Use Databricks Model Serving for real-time predictions.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 72.0, 'y': 259.0, 'w': 282.0, 'h': 243.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Monitoring and Optimization',\n",
       "      'md': '# Monitoring and Optimization',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 72.0, 'y': 330.0, 'w': 152.0, 'h': 12.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '- Continuously monitor model performance using built-in logging tools.\\n- Update models with new data or retrain as needed.',\n",
       "      'md': '- Continuously monitor model performance using built-in logging tools.\\n- Update models with new data or retrain as needed.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 72.0, 'y': 353.0, 'w': 341.0, 'h': 35.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Track metrics like accuracy, loss, and hyperparameters using MLflow Tracking.',\n",
       "      'md': '# Track metrics like accuracy, loss, and hyperparameters using MLflow Tracking.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 72.0, 'y': 36.0, 'w': 391.0, 'h': 466.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '- Automatically log experiments for easy comparison.',\n",
       "      'md': '- Automatically log experiments for easy comparison.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 72.0, 'y': 59.0, 'w': 255.0, 'h': 12.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Example:',\n",
       "      'md': '# Example:',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 54.0, 'y': 82.0, 'w': 48.0, 'h': 12.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': 'python\\nimport mlflow\\nwith mlflow.start_run():\\nmlflow.log_param(\"max_depth\", 5)\\nmlflow.log_metric(\"accuracy\", 0.95)',\n",
       "      'md': 'python\\nimport mlflow\\nwith mlflow.start_run():\\nmlflow.log_param(\"max_depth\", 5)\\nmlflow.log_metric(\"accuracy\", 0.95)',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 72.0, 'y': 119.0, 'w': 217.0, 'h': 383.0}}],\n",
       "    'status': 'OK',\n",
       "    'links': [],\n",
       "    'width': 612.0,\n",
       "    'height': 792.0,\n",
       "    'triggeredAutoMode': False,\n",
       "    'parsingMode': 'accurate',\n",
       "    'structuredData': None,\n",
       "    'noStructuredContent': False,\n",
       "    'noTextContent': False},\n",
       "   {'page': 26,\n",
       "    'text': 'Now, let’s discover how MLflow streamlines the entire machine learning lifecycle, from experimentation to\\ndeployment, all within Databricks!\\nMLflow: Simplifying the Machine Learning Lifecycle\\n        Databricks integrates MLflow, an open-source platform that simplifies managing the end-to-end\\nlifecycle of machine learning models. From tracking experiments to deploying models in production, MLflow\\nensures consistency and collaboration at every stage.\\nMLflow is a platform designed to manage the ML lifecycle, including:\\n    1.  Experiment Tracking: Log metrics, parameters, and results of different experiments.\\n    2.  Project Packaging: Package ML code into reusable and reproducible formats.\\n    3.  Model Registry: Centralize model storage with versioning and deployment capabilities.\\n    4.  Model Deployment: Deploy models to production with ease.\\nMLflow Components\\n1. Tracking\\n    •   Log metrics (e.g., accuracy, loss), parameters (e.g., learning rate), and artifacts (e.g., model files).\\n    •   Compare experiments side by side.\\n2. Projects\\n    •   Package code into a standardized format for reproducibility.\\n    •   Use environment specifications (e.g., Conda, Docker).\\n3. Model Registry\\n    •   Central hub for managing model versions.\\n    •   Assign models stages like Staging, Production, or Archived.\\n4. Model Serving\\n    •   Serve models as REST APIs for real-time inference.\\nWhy Use MLflow?\\n    •   End-to-End Workflow: Covers all phases from experimentation to deployment.\\n    •   Collaboration: Share experiments, models, and results with your team.\\n    •   Reproducibility: Ensure consistency across different environments and datasets.\\n    •   Integration: Works seamlessly with Databricks and other ML libraries like TensorFlow, PyTorch, and\\n        Scikit-learn.',\n",
       "    'md': '# MLflow: Simplifying the Machine Learning Lifecycle\\n\\nNow, let’s discover how MLflow streamlines the entire machine learning lifecycle, from experimentation to deployment, all within Databricks!\\n\\nDatabricks integrates MLflow, an open-source platform that simplifies managing the end-to-end lifecycle of machine learning models. From tracking experiments to deploying models in production, MLflow ensures consistency and collaboration at every stage.\\n\\nMLflow is a platform designed to manage the ML lifecycle, including:\\n\\n1. Experiment Tracking: Log metrics, parameters, and results of different experiments.\\n2. Project Packaging: Package ML code into reusable and reproducible formats.\\n3. Model Registry: Centralize model storage with versioning and deployment capabilities.\\n4. Model Deployment: Deploy models to production with ease.\\n\\n# MLflow Components\\n\\n1. # Tracking\\n\\n- Log metrics (e.g., accuracy, loss), parameters (e.g., learning rate), and artifacts (e.g., model files).\\n- Compare experiments side by side.\\n2. # Projects\\n\\n- Package code into a standardized format for reproducibility.\\n- Use environment specifications (e.g., Conda, Docker).\\n3. # Model Registry\\n\\n- Central hub for managing model versions.\\n- Assign models stages like Staging, Production, or Archived.\\n4. # Model Serving\\n\\n- Serve models as REST APIs for real-time inference.\\n\\n# Why Use MLflow?\\n\\n- End-to-End Workflow: Covers all phases from experimentation to deployment.\\n- Collaboration: Share experiments, models, and results with your team.\\n- Reproducibility: Ensure consistency across different environments and datasets.\\n- Integration: Works seamlessly with Databricks and other ML libraries like TensorFlow, PyTorch, and Scikit-learn.',\n",
       "    'images': [],\n",
       "    'charts': [],\n",
       "    'tables': [],\n",
       "    'layout': [],\n",
       "    'items': [{'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'MLflow: Simplifying the Machine Learning Lifecycle',\n",
       "      'md': '# MLflow: Simplifying the Machine Learning Lifecycle',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 74.96, 'w': 319.0, 'h': 14.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': 'Now, let’s discover how MLflow streamlines the entire machine learning lifecycle, from experimentation to deployment, all within Databricks!\\n\\nDatabricks integrates MLflow, an open-source platform that simplifies managing the end-to-end lifecycle of machine learning models. From tracking experiments to deploying models in production, MLflow ensures consistency and collaboration at every stage.\\n\\nMLflow is a platform designed to manage the ML lifecycle, including:\\n\\n1. Experiment Tracking: Log metrics, parameters, and results of different experiments.\\n2. Project Packaging: Package ML code into reusable and reproducible formats.\\n3. Model Registry: Centralize model storage with versioning and deployment capabilities.\\n4. Model Deployment: Deploy models to production with ease.',\n",
       "      'md': 'Now, let’s discover how MLflow streamlines the entire machine learning lifecycle, from experimentation to deployment, all within Databricks!\\n\\nDatabricks integrates MLflow, an open-source platform that simplifies managing the end-to-end lifecycle of machine learning models. From tracking experiments to deploying models in production, MLflow ensures consistency and collaboration at every stage.\\n\\nMLflow is a platform designed to manage the ML lifecycle, including:\\n\\n1. Experiment Tracking: Log metrics, parameters, and results of different experiments.\\n2. Project Packaging: Package ML code into reusable and reproducible formats.\\n3. Model Registry: Centralize model storage with versioning and deployment capabilities.\\n4. Model Deployment: Deploy models to production with ease.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 36.0, 'w': 528.0, 'h': 428.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'MLflow Components',\n",
       "      'md': '# MLflow Components',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 292.0, 'w': 111.0, 'h': 12.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '1. # Tracking\\n\\n- Log metrics (e.g., accuracy, loss), parameters (e.g., learning rate), and artifacts (e.g., model files).\\n- Compare experiments side by side.\\n2. # Projects\\n\\n- Package code into a standardized format for reproducibility.\\n- Use environment specifications (e.g., Conda, Docker).\\n3. # Model Registry\\n\\n- Central hub for managing model versions.\\n- Assign models stages like Staging, Production, or Archived.\\n4. # Model Serving\\n\\n- Serve models as REST APIs for real-time inference.',\n",
       "      'md': '1. # Tracking\\n\\n- Log metrics (e.g., accuracy, loss), parameters (e.g., learning rate), and artifacts (e.g., model files).\\n- Compare experiments side by side.\\n2. # Projects\\n\\n- Package code into a standardized format for reproducibility.\\n- Use environment specifications (e.g., Conda, Docker).\\n3. # Model Registry\\n\\n- Central hub for managing model versions.\\n- Assign models stages like Staging, Production, or Archived.\\n4. # Model Serving\\n\\n- Serve models as REST APIs for real-time inference.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 54.0, 'y': 177.0, 'w': 488.0, 'h': 378.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Why Use MLflow?',\n",
       "      'md': '# Why Use MLflow?',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 589.0, 'w': 99.0, 'h': 12.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '- End-to-End Workflow: Covers all phases from experimentation to deployment.\\n- Collaboration: Share experiments, models, and results with your team.\\n- Reproducibility: Ensure consistency across different environments and datasets.\\n- Integration: Works seamlessly with Databricks and other ML libraries like TensorFlow, PyTorch, and Scikit-learn.',\n",
       "      'md': '- End-to-End Workflow: Covers all phases from experimentation to deployment.\\n- Collaboration: Share experiments, models, and results with your team.\\n- Reproducibility: Ensure consistency across different environments and datasets.\\n- Integration: Works seamlessly with Databricks and other ML libraries like TensorFlow, PyTorch, and Scikit-learn.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 72.0, 'y': 612.0, 'w': 489.0, 'h': 96.0}}],\n",
       "    'status': 'OK',\n",
       "    'links': [],\n",
       "    'width': 612.0,\n",
       "    'height': 792.0,\n",
       "    'triggeredAutoMode': False,\n",
       "    'parsingMode': 'accurate',\n",
       "    'structuredData': None,\n",
       "    'noStructuredContent': False,\n",
       "    'noTextContent': False},\n",
       "   {'page': 27,\n",
       "    'text': \"Let’s explore how Delta Lake transforms traditional data lakes into powerful, reliable, and high-performance\\ndata platforms!\\nDelta Lake: Revolutionizing Data Lakes\\nDelta Lake is an open-source storage layer that brings reliability and performance to data lakes. Integrated\\nseamlessly into Databricks, Delta Lake ensures data quality, consistency, and scalability, making it a\\ncornerstone of modern data engineering workflows.\\nWhy Delta Lake?\\nTraditional data lakes often face challenges like:\\n    1.  Data Reliability Issues: No ACID transactions, leading to data inconsistencies.\\n    2.  Lack of Schema Enforcement: Changes in data structure can lead to errors.\\n    3.  Slow Performance: Scanning large datasets without optimization slows down analytics.\\nDelta Lake addresses these issues by introducing a structured, reliable, and high-performance storage layer on\\ntop of existing cloud storage systems (e.g., AWS S3, Azure Data Lake, Google Cloud Storage).\\nKey Features of Delta Lake:\\n    1.  ACID Transactions\\n           o   Ensures atomicity, consistency, isolation, and durability for all data operations.\\n           o   Example: Concurrent writes won't corrupt your data.\\n    2.  Schema Enforcement and Evolution\\n           o   Prevents inconsistent data writes by enforcing schema.\\n           o   Allows controlled schema updates as requirements evolve.\\n    3.  Time Travel\\n           o   Query historical data versions to understand changes or recover deleted data.\\n           o   Example: Rollback to a previous state after unintended updates.\\n    4.  Data Compaction (Optimized Layouts)\\n           o   Automates the process of merging small files into larger ones, improving query speed.\\n    5.  Streaming and Batch Unification\\n           o   Handles real-time streaming and batch processing in a single framework.\\n    6.  Performance Optimization\\n           o   Indexing and caching boost query speeds.\",\n",
       "    'md': \"# Delta Lake: Revolutionizing Data Lakes\\n\\nLet’s explore how Delta Lake transforms traditional data lakes into powerful, reliable, and high-performance data platforms!\\n\\n# Why Delta Lake?\\n\\nTraditional data lakes often face challenges like:\\n\\n1. Data Reliability Issues: No ACID transactions, leading to data inconsistencies.\\n2. Lack of Schema Enforcement: Changes in data structure can lead to errors.\\n3. Slow Performance: Scanning large datasets without optimization slows down analytics.\\n\\nDelta Lake addresses these issues by introducing a structured, reliable, and high-performance storage layer on top of existing cloud storage systems (e.g., AWS S3, Azure Data Lake, Google Cloud Storage).\\n\\n# Key Features of Delta Lake:\\n\\n1. ACID Transactions\\n- Ensures atomicity, consistency, isolation, and durability for all data operations.\\n- Example: Concurrent writes won't corrupt your data.\\n2. Schema Enforcement and Evolution\\n- Prevents inconsistent data writes by enforcing schema.\\n- Allows controlled schema updates as requirements evolve.\\n3. Time Travel\\n- Query historical data versions to understand changes or recover deleted data.\\n- Example: Rollback to a previous state after unintended updates.\\n4. Data Compaction (Optimized Layouts)\\n- Automates the process of merging small files into larger ones, improving query speed.\\n5. Streaming and Batch Unification\\n- Handles real-time streaming and batch processing in a single framework.\\n6. Performance Optimization\\n- Indexing and caching boost query speeds.\",\n",
       "    'images': [],\n",
       "    'charts': [],\n",
       "    'tables': [],\n",
       "    'layout': [],\n",
       "    'items': [{'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Delta Lake: Revolutionizing Data Lakes',\n",
       "      'md': '# Delta Lake: Revolutionizing Data Lakes',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 74.96, 'w': 240.0, 'h': 624.08}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': 'Let’s explore how Delta Lake transforms traditional data lakes into powerful, reliable, and high-performance data platforms!',\n",
       "      'md': 'Let’s explore how Delta Lake transforms traditional data lakes into powerful, reliable, and high-performance data platforms!',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 36.0, 'w': 522.0, 'h': 663.04}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Why Delta Lake?',\n",
       "      'md': '# Why Delta Lake?',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 154.0, 'w': 93.0, 'h': 12.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': 'Traditional data lakes often face challenges like:\\n\\n1. Data Reliability Issues: No ACID transactions, leading to data inconsistencies.\\n2. Lack of Schema Enforcement: Changes in data structure can lead to errors.\\n3. Slow Performance: Scanning large datasets without optimization slows down analytics.\\n\\nDelta Lake addresses these issues by introducing a structured, reliable, and high-performance storage layer on top of existing cloud storage systems (e.g., AWS S3, Azure Data Lake, Google Cloud Storage).',\n",
       "      'md': 'Traditional data lakes often face challenges like:\\n\\n1. Data Reliability Issues: No ACID transactions, leading to data inconsistencies.\\n2. Lack of Schema Enforcement: Changes in data structure can lead to errors.\\n3. Slow Performance: Scanning large datasets without optimization slows down analytics.\\n\\nDelta Lake addresses these issues by introducing a structured, reliable, and high-performance storage layer on top of existing cloud storage systems (e.g., AWS S3, Azure Data Lake, Google Cloud Storage).',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 177.0, 'w': 527.0, 'h': 522.04}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Key Features of Delta Lake:',\n",
       "      'md': '# Key Features of Delta Lake:',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 329.0, 'w': 147.0, 'h': 370.04}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': \"1. ACID Transactions\\n- Ensures atomicity, consistency, isolation, and durability for all data operations.\\n- Example: Concurrent writes won't corrupt your data.\\n2. Schema Enforcement and Evolution\\n- Prevents inconsistent data writes by enforcing schema.\\n- Allows controlled schema updates as requirements evolve.\\n3. Time Travel\\n- Query historical data versions to understand changes or recover deleted data.\\n- Example: Rollback to a previous state after unintended updates.\\n4. Data Compaction (Optimized Layouts)\\n- Automates the process of merging small files into larger ones, improving query speed.\\n5. Streaming and Batch Unification\\n- Handles real-time streaming and batch processing in a single framework.\\n6. Performance Optimization\\n- Indexing and caching boost query speeds.\",\n",
       "      'md': \"1. ACID Transactions\\n- Ensures atomicity, consistency, isolation, and durability for all data operations.\\n- Example: Concurrent writes won't corrupt your data.\\n2. Schema Enforcement and Evolution\\n- Prevents inconsistent data writes by enforcing schema.\\n- Allows controlled schema updates as requirements evolve.\\n3. Time Travel\\n- Query historical data versions to understand changes or recover deleted data.\\n- Example: Rollback to a previous state after unintended updates.\\n4. Data Compaction (Optimized Layouts)\\n- Automates the process of merging small files into larger ones, improving query speed.\\n5. Streaming and Batch Unification\\n- Handles real-time streaming and batch processing in a single framework.\\n6. Performance Optimization\\n- Indexing and caching boost query speeds.\",\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 54.0, 'y': 200.0, 'w': 471.0, 'h': 499.04}}],\n",
       "    'status': 'OK',\n",
       "    'links': [],\n",
       "    'width': 612.0,\n",
       "    'height': 792.0,\n",
       "    'triggeredAutoMode': False,\n",
       "    'parsingMode': 'accurate',\n",
       "    'structuredData': None,\n",
       "    'noStructuredContent': False,\n",
       "    'noTextContent': False},\n",
       "   {'page': 28,\n",
       "    'text': 'Delta Lake Architecture\\nDelta Lake sits atop your existing data lake and adds additional layers for:\\n    •   Metadata Management: Tracks data versions and schema in a transaction log.\\n    •   Storage Optimization: Organizes data into parquet files with efficient partitioning.\\n    •   Delta Transaction Log: Records every change (insert, update, delete) to your datasets.\\nHow Delta Lake Works:\\n    1.  Data Ingestion\\n            o   Ingest data from multiple sources (e.g., JSON, CSV, streaming data).\\n            o   Delta Lake ensures that all writes follow ACID properties.\\n            Example:\\n                         Python                                                                       Copyᶜᶜᵈᵉ\\n                              spark.read.csV(\"53: / fmy bucket/data.csv\"\\n                         df write format( delta\" ) .save ( [delta-lake-table\\n    2.  Schema Enforcement\\n    •   Enforce schemas to prevent inconsistent writes.\\n        Example:\\n                        Pytnon                                                                    Copy \\'coce\\n                        df.write.format(\"delta   option( \"mergeSchema  true\" ) . save(\" /delta-lake-table\")\\n    3.  Querying and Time Travel\\n    •   Query data at a specific version or timestamp.\\n        Example:\\n                                                                                            Copy code\\n                          SELECT   FROM delta [delta-lake-tabledv5\\n    4.  Data Updates and Deletes\\n    •   Perform updates and deletes directly on your Delta tables.\\n        Example:',\n",
       "    'md': '# Delta Lake Architecture\\n\\nDelta Lake sits atop your existing data lake and adds additional layers for:\\n\\n- Metadata Management: Tracks data versions and schema in a transaction log.\\n- Storage Optimization: Organizes data into parquet files with efficient partitioning.\\n- Delta Transaction Log: Records every change (insert, update, delete) to your datasets.\\n\\n# How Delta Lake Works:\\n\\n1. Data Ingestion\\n\\n- Ingest data from multiple sources (e.g., JSON, CSV, streaming data).\\n- Delta Lake ensures that all writes follow ACID properties.\\n2. Schema Enforcement\\n\\n- Enforce schemas to prevent inconsistent writes.\\n3. Querying and Time Travel\\n\\n- Query data at a specific version or timestamp.\\n4. Data Updates and Deletes\\n\\n- Perform updates and deletes directly on your Delta tables.',\n",
       "    'images': [{'name': 'img_p27_1.png',\n",
       "      'height': 131.0,\n",
       "      'width': 688.0,\n",
       "      'x': 133.5,\n",
       "      'y': 287.8,\n",
       "      'original_width': 688,\n",
       "      'original_height': 131,\n",
       "      'type': None},\n",
       "     {'name': 'img_p27_2.png',\n",
       "      'height': 110.0,\n",
       "      'width': 689.0,\n",
       "      'x': 132.65,\n",
       "      'y': 434.03999999999996,\n",
       "      'original_width': 689,\n",
       "      'original_height': 110,\n",
       "      'type': None},\n",
       "     {'name': 'img_p27_3.png',\n",
       "      'height': 106.0,\n",
       "      'width': 688.0,\n",
       "      'x': 144.43,\n",
       "      'y': 565.2800000000001,\n",
       "      'original_width': 688,\n",
       "      'original_height': 106,\n",
       "      'type': None}],\n",
       "    'charts': [],\n",
       "    'tables': [],\n",
       "    'layout': [],\n",
       "    'items': [{'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Delta Lake Architecture',\n",
       "      'md': '# Delta Lake Architecture',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 36.0, 'w': 126.0, 'h': 12.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': 'Delta Lake sits atop your existing data lake and adds additional layers for:\\n\\n- Metadata Management: Tracks data versions and schema in a transaction log.\\n- Storage Optimization: Organizes data into parquet files with efficient partitioning.\\n- Delta Transaction Log: Records every change (insert, update, delete) to your datasets.',\n",
       "      'md': 'Delta Lake sits atop your existing data lake and adds additional layers for:\\n\\n- Metadata Management: Tracks data versions and schema in a transaction log.\\n- Storage Optimization: Organizes data into parquet files with efficient partitioning.\\n- Delta Transaction Log: Records every change (insert, update, delete) to your datasets.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 59.0, 'w': 457.0, 'h': 195.04}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'How Delta Lake Works:',\n",
       "      'md': '# How Delta Lake Works:',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 173.0, 'w': 127.0, 'h': 81.04}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '1. Data Ingestion\\n\\n- Ingest data from multiple sources (e.g., JSON, CSV, streaming data).\\n- Delta Lake ensures that all writes follow ACID properties.\\n2. Schema Enforcement\\n\\n- Enforce schemas to prevent inconsistent writes.\\n3. Querying and Time Travel\\n\\n- Query data at a specific version or timestamp.\\n4. Data Updates and Deletes\\n\\n- Perform updates and deletes directly on your Delta tables.',\n",
       "      'md': '1. Data Ingestion\\n\\n- Ingest data from multiple sources (e.g., JSON, CSV, streaming data).\\n- Delta Lake ensures that all writes follow ACID properties.\\n2. Schema Enforcement\\n\\n- Enforce schemas to prevent inconsistent writes.\\n3. Querying and Time Travel\\n\\n- Query data at a specific version or timestamp.\\n4. Data Updates and Deletes\\n\\n- Perform updates and deletes directly on your Delta tables.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 54.0, 'y': 196.0, 'w': 387.0, 'h': 466.0}}],\n",
       "    'status': 'OK',\n",
       "    'links': [],\n",
       "    'width': 612.0,\n",
       "    'height': 792.0,\n",
       "    'triggeredAutoMode': False,\n",
       "    'parsingMode': 'accurate',\n",
       "    'structuredData': None,\n",
       "    'noStructuredContent': False,\n",
       "    'noTextContent': False},\n",
       "   {'page': 29,\n",
       "    'text': '                   Pytnon                                                                   Copy \\'coce\\n                    from delta.tables import DeltaTable\\n                   deltaTable   DeltaTable. forPath(spark, \"{delta-lake-table\\n                   deltaTable.delete(\"age  18\\nDelta Lake in Action: Real-World Use Cases\\n    1.  ETL Pipelines\\n        •   Delta Lake ensures reliable and consistent data pipelines by handling schema mismatches and\\n            ensuring ACID compliance.\\n    2.  Data Lakes\\n        •   Replace traditional data lakes to provide a structured, queryable interface without sacrificing\\n            scalability.\\n    3.  Streaming Analytics\\n        •   Process streaming data from IoT devices, clickstreams, or financial markets.\\n    4.  Machine Learning Workflows\\n        •   Use Delta Lake to preprocess and version datasets for reproducible ML experiments.\\nNow, let’s see how the Databricks REST API enables you to automate tasks and scale your workflows\\neffortlessly!\\nDatabricks REST API: Automating Databricks at Scale\\nThe Databricks REST API is a powerful tool for managing your Databricks environment programmatically.\\nWhether you’re automating the creation of clusters, managing jobs, or integrating Databricks into CI/CD\\npipelines, the REST API provides the flexibility to do it all.\\nWhat Can You Do with the REST API?\\n    1.  Cluster Management: Create, configure, start, stop, or delete clusters.\\n    2.  Job Automation: Schedule and manage jobs programmatically.\\n    3.  Notebook Operations: Import, export, and run notebooks.\\n    4.  Data Access: Interact with tables and files stored in Databricks.\\n    5.  Workspace Management: Manage users, groups, and permissions.',\n",
       "    'md': '# Delta Lake in Action: Real-World Use Cases\\n\\n# 1. ETL Pipelines\\n\\n- Delta Lake ensures reliable and consistent data pipelines by handling schema mismatches and ensuring ACID compliance.\\n\\n# 2. Data Lakes\\n\\n- Replace traditional data lakes to provide a structured, queryable interface without sacrificing scalability.\\n\\n# 3. Streaming Analytics\\n\\n- Process streaming data from IoT devices, clickstreams, or financial markets.\\n\\n# 4. Machine Learning Workflows\\n\\n- Use Delta Lake to preprocess and version datasets for reproducible ML experiments.\\n\\nNow, let’s see how the Databricks REST API enables you to automate tasks and scale your workflows effortlessly!\\n\\n# Databricks REST API: Automating Databricks at Scale\\n\\nThe Databricks REST API is a powerful tool for managing your Databricks environment programmatically. Whether you’re automating the creation of clusters, managing jobs, or integrating Databricks into CI/CD pipelines, the REST API provides the flexibility to do it all.\\n\\n# What Can You Do with the REST API?\\n\\n# 1. Cluster Management\\n\\n- Create, configure, start, stop, or delete clusters.\\n\\n# 2. Job Automation\\n\\n- Schedule and manage jobs programmatically.\\n\\n# 3. Notebook Operations\\n\\n- Import, export, and run notebooks.\\n\\n# 4. Data Access\\n\\n- Interact with tables and files stored in Databricks.\\n\\n# 5. Workspace Management\\n\\n- Manage users, groups, and permissions.',\n",
       "    'images': [{'name': 'img_p28_1.png',\n",
       "      'height': 164.0,\n",
       "      'width': 695.0,\n",
       "      'x': 112.4,\n",
       "      'y': 36.00000000000003,\n",
       "      'original_width': 695,\n",
       "      'original_height': 164,\n",
       "      'type': None}],\n",
       "    'charts': [],\n",
       "    'tables': [],\n",
       "    'layout': [],\n",
       "    'items': [{'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Delta Lake in Action: Real-World Use Cases',\n",
       "      'md': '# Delta Lake in Action: Real-World Use Cases',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 159.0, 'w': 229.0, 'h': 12.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': '1. ETL Pipelines',\n",
       "      'md': '# 1. ETL Pipelines',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 54.0, 'y': 182.0, 'w': 93.0, 'h': 413.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '- Delta Lake ensures reliable and consistent data pipelines by handling schema mismatches and ensuring ACID compliance.',\n",
       "      'md': '- Delta Lake ensures reliable and consistent data pipelines by handling schema mismatches and ensuring ACID compliance.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 90.0, 'y': 206.0, 'w': 450.0, 'h': 27.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': '2. Data Lakes',\n",
       "      'md': '# 2. Data Lakes',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 54.0, 'y': 244.0, 'w': 79.0, 'h': 374.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '- Replace traditional data lakes to provide a structured, queryable interface without sacrificing scalability.',\n",
       "      'md': '- Replace traditional data lakes to provide a structured, queryable interface without sacrificing scalability.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 72.0, 'y': 244.0, 'w': 462.0, 'h': 50.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': '3. Streaming Analytics',\n",
       "      'md': '# 3. Streaming Analytics',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 54.0, 'y': 305.0, 'w': 124.0, 'h': 336.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '- Process streaming data from IoT devices, clickstreams, or financial markets.',\n",
       "      'md': '- Process streaming data from IoT devices, clickstreams, or financial markets.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 90.0, 'y': 329.0, 'w': 369.0, 'h': 12.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': '4. Machine Learning Workflows',\n",
       "      'md': '# 4. Machine Learning Workflows',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 54.0, 'y': 352.0, 'w': 174.0, 'h': 312.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '- Use Delta Lake to preprocess and version datasets for reproducible ML experiments.\\n\\nNow, let’s see how the Databricks REST API enables you to automate tasks and scale your workflows effortlessly!',\n",
       "      'md': '- Use Delta Lake to preprocess and version datasets for reproducible ML experiments.\\n\\nNow, let’s see how the Databricks REST API enables you to automate tasks and scale your workflows effortlessly!',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 376.0, 'w': 490.0, 'h': 72.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Databricks REST API: Automating Databricks at Scale',\n",
       "      'md': '# Databricks REST API: Automating Databricks at Scale',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 458.96, 'w': 331.0, 'h': 14.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': 'The Databricks REST API is a powerful tool for managing your Databricks environment programmatically. Whether you’re automating the creation of clusters, managing jobs, or integrating Databricks into CI/CD pipelines, the REST API provides the flexibility to do it all.',\n",
       "      'md': 'The Databricks REST API is a powerful tool for managing your Databricks environment programmatically. Whether you’re automating the creation of clusters, managing jobs, or integrating Databricks into CI/CD pipelines, the REST API provides the flexibility to do it all.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 485.0, 'w': 524.0, 'h': 41.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'What Can You Do with the REST API?',\n",
       "      'md': '# What Can You Do with the REST API?',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 560.0, 'w': 204.0, 'h': 12.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': '1. Cluster Management',\n",
       "      'md': '# 1. Cluster Management',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 54.0, 'y': 182.0, 'w': 9.0, 'h': 413.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '- Create, configure, start, stop, or delete clusters.',\n",
       "      'md': '- Create, configure, start, stop, or delete clusters.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 0.0, 'y': 0.0, 'w': 612.0, 'h': 792.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': '2. Job Automation',\n",
       "      'md': '# 2. Job Automation',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 54.0, 'y': 244.0, 'w': 9.0, 'h': 374.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '- Schedule and manage jobs programmatically.',\n",
       "      'md': '- Schedule and manage jobs programmatically.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 0.0, 'y': 0.0, 'w': 612.0, 'h': 792.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': '3. Notebook Operations',\n",
       "      'md': '# 3. Notebook Operations',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 54.0, 'y': 305.0, 'w': 9.0, 'h': 336.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '- Import, export, and run notebooks.',\n",
       "      'md': '- Import, export, and run notebooks.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 0.0, 'y': 0.0, 'w': 612.0, 'h': 792.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': '4. Data Access',\n",
       "      'md': '# 4. Data Access',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 54.0, 'y': 352.0, 'w': 9.0, 'h': 312.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '- Interact with tables and files stored in Databricks.',\n",
       "      'md': '- Interact with tables and files stored in Databricks.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 0.0, 'y': 0.0, 'w': 612.0, 'h': 792.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': '5. Workspace Management',\n",
       "      'md': '# 5. Workspace Management',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 54.0, 'y': 675.0, 'w': 9.0, 'h': 12.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '- Manage users, groups, and permissions.',\n",
       "      'md': '- Manage users, groups, and permissions.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 0.0, 'y': 0.0, 'w': 612.0, 'h': 792.0}}],\n",
       "    'status': 'OK',\n",
       "    'links': [],\n",
       "    'width': 612.0,\n",
       "    'height': 792.0,\n",
       "    'triggeredAutoMode': False,\n",
       "    'parsingMode': 'accurate',\n",
       "    'structuredData': None,\n",
       "    'noStructuredContent': False,\n",
       "    'noTextContent': False},\n",
       "   {'page': 30,\n",
       "    'text': 'Example: Starting a Cluster\\nHere’s a Python example using the Databricks REST API:\\n                                      Pytnon                                                            Copy <coce\\n                                      import requests\\n                                      url  \"https:/ / <databricks-instance>/ api/2 O/clusters/create\"\\n                                      headers  {\"Authorization  Bearer <your_ access-token>\"}\\n                                      payload\\n                                          cluster_name  example-cluster\"\\n                                          spark_version\\'  \"12.0.X-scala2.12\"\\n                                          node_type_id\" \"i3.xlarge\\n                                          num_workers\\n                                      response  requests.post(url, headers=headers_ json-payload)\\n                                      print(response. json() )\\nNow, let’s dive into how Databricks Streaming brings real-time analytics to life, enabling instant insights from\\nyour data streams!\\nDatabricks Streaming: Real-Time Analytics\\nDatabricks Streaming, powered by Apache Spark Structured Streaming, enables real-time processing of data\\nstreams, making it ideal for use cases like IoT data processing, fraud detection, and log monitoring.\\nKey Features:\\n     1.   Unified Batch and Streaming: Use the same code for both real-time and batch processing.\\n     2.   Exactly-Once Processing: Ensures reliable and accurate results.\\n     3.   Fault Tolerance: Handles failures seamlessly, ensuring no data is lost.\\nExample: Processing a Real-Time Stream\\n                               Pytnon                                                                         Copy code\\n                                 Read streaming data from Kafka\\n                               streaming_df   spark.readStream\\n                                    format(\"kafka\\n                                    option(\"kafka.bootstrap.servers   brokerl:9092\\n                                    option(\"subscribe    events\")\\n                                    load ( )\\n                                 Transform data\\n                               parsed_df   streaming_df selectExpr(\"CAST(value AS STRING) as event_data\\n                                 Nrite to Delta Lake\\n                               parsed_df writestream\\n                                    format(\"delta\\n                                    outputMlode( \"append\\n                                    option(\"checkpointLocation   [deltalevents/_checkpoints/\\n                                    start(\" /deltalevents',\n",
       "    'md': '# Example: Starting a Cluster\\n\\nHere’s a Python example using the Databricks REST API:\\n\\nimport requests\\nurl = \"https://<databricks-instance>/api/2.0/clusters/create\"\\nheaders = {\"Authorization\": \"Bearer <your_access-token>\"}\\npayload = {\\n\"cluster_name\": \"example-cluster\",\\n\"spark_version\": \"12.0.X-scala2.12\",\\n\"node_type_id\": \"i3.xlarge\",\\n\"num_workers\": 2\\n}\\nresponse = requests.post(url, headers=headers, json=payload)\\nprint(response.json())\\n\\nNow, let’s dive into how Databricks Streaming brings real-time analytics to life, enabling instant insights from your data streams!\\n\\n# Databricks Streaming: Real-Time Analytics\\n\\nDatabricks Streaming, powered by Apache Spark Structured Streaming, enables real-time processing of data streams, making it ideal for use cases like IoT data processing, fraud detection, and log monitoring.\\n\\n# Key Features:\\n\\n1. Unified Batch and Streaming: Use the same code for both real-time and batch processing.\\n2. Exactly-Once Processing: Ensures reliable and accurate results.\\n3. Fault Tolerance: Handles failures seamlessly, ensuring no data is lost.\\n\\n# Example: Processing a Real-Time Stream\\n\\n# Read streaming data from Kafka\\nstreaming_df = spark.readStream \\\\\\n.format(\"kafka\") \\\\\\n.option(\"kafka.bootstrap.servers\", \"broker1:9092\") \\\\\\n.option(\"subscribe\", \"events\") \\\\\\n.load()\\n\\n# Transform data\\nparsed_df = streaming_df.selectExpr(\"CAST(value AS STRING) as event_data\")\\n\\n# Write to Delta Lake\\nparsed_df.writeStream \\\\\\n.format(\"delta\") \\\\\\n.outputMode(\"append\") \\\\\\n.option(\"checkpointLocation\", \"/deltalevents/_checkpoints/\") \\\\\\n.start(\"/deltalevents\")',\n",
       "    'images': [{'name': 'img_p29_1.png',\n",
       "      'height': 394.0,\n",
       "      'width': 723.0,\n",
       "      'x': 166.5,\n",
       "      'y': 81.78000000000003,\n",
       "      'original_width': 723,\n",
       "      'original_height': 394,\n",
       "      'type': None},\n",
       "     {'name': 'img_p29_2.png',\n",
       "      'height': 464.0,\n",
       "      'width': 725.0,\n",
       "      'x': 138.65,\n",
       "      'y': 526.865,\n",
       "      'original_width': 725,\n",
       "      'original_height': 464,\n",
       "      'type': None}],\n",
       "    'charts': [],\n",
       "    'tables': [],\n",
       "    'layout': [],\n",
       "    'items': [{'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Example: Starting a Cluster',\n",
       "      'md': '# Example: Starting a Cluster',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 36.0, 'w': 147.0, 'h': 12.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': 'Here’s a Python example using the Databricks REST API:\\n\\nimport requests\\nurl = \"https://<databricks-instance>/api/2.0/clusters/create\"\\nheaders = {\"Authorization\": \"Bearer <your_access-token>\"}\\npayload = {\\n\"cluster_name\": \"example-cluster\",\\n\"spark_version\": \"12.0.X-scala2.12\",\\n\"node_type_id\": \"i3.xlarge\",\\n\"num_workers\": 2\\n}\\nresponse = requests.post(url, headers=headers, json=payload)\\nprint(response.json())\\n\\nNow, let’s dive into how Databricks Streaming brings real-time analytics to life, enabling instant insights from your data streams!',\n",
       "      'md': 'Here’s a Python example using the Databricks REST API:\\n\\nimport requests\\nurl = \"https://<databricks-instance>/api/2.0/clusters/create\"\\nheaders = {\"Authorization\": \"Bearer <your_access-token>\"}\\npayload = {\\n\"cluster_name\": \"example-cluster\",\\n\"spark_version\": \"12.0.X-scala2.12\",\\n\"node_type_id\": \"i3.xlarge\",\\n\"num_workers\": 2\\n}\\nresponse = requests.post(url, headers=headers, json=payload)\\nprint(response.json())\\n\\nNow, let’s dive into how Databricks Streaming brings real-time analytics to life, enabling instant insights from your data streams!',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 59.0, 'w': 529.0, 'h': 411.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Databricks Streaming: Real-Time Analytics',\n",
       "      'md': '# Databricks Streaming: Real-Time Analytics',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 302.96, 'w': 261.0, 'h': 14.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': 'Databricks Streaming, powered by Apache Spark Structured Streaming, enables real-time processing of data streams, making it ideal for use cases like IoT data processing, fraud detection, and log monitoring.',\n",
       "      'md': 'Databricks Streaming, powered by Apache Spark Structured Streaming, enables real-time processing of data streams, making it ideal for use cases like IoT data processing, fraud detection, and log monitoring.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 329.0, 'w': 535.0, 'h': 26.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Key Features:',\n",
       "      'md': '# Key Features:',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 389.0, 'w': 75.0, 'h': 12.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': '1. Unified Batch and Streaming: Use the same code for both real-time and batch processing.\\n2. Exactly-Once Processing: Ensures reliable and accurate results.\\n3. Fault Tolerance: Handles failures seamlessly, ensuring no data is lost.',\n",
       "      'md': '1. Unified Batch and Streaming: Use the same code for both real-time and batch processing.\\n2. Exactly-Once Processing: Ensures reliable and accurate results.\\n3. Fault Tolerance: Handles failures seamlessly, ensuring no data is lost.',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 54.0, 'y': 412.0, 'w': 460.0, 'h': 58.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Example: Processing a Real-Time Stream',\n",
       "      'md': '# Example: Processing a Real-Time Stream',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 504.0, 'w': 215.0, 'h': 12.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Read streaming data from Kafka',\n",
       "      'md': '# Read streaming data from Kafka',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 157.0, 'y': 554.0, 'w': 109.0, 'h': 9.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': 'streaming_df = spark.readStream \\\\\\n.format(\"kafka\") \\\\\\n.option(\"kafka.bootstrap.servers\", \"broker1:9092\") \\\\\\n.option(\"subscribe\", \"events\") \\\\\\n.load()',\n",
       "      'md': 'streaming_df = spark.readStream \\\\\\n.format(\"kafka\") \\\\\\n.option(\"kafka.bootstrap.servers\", \"broker1:9092\") \\\\\\n.option(\"subscribe\", \"events\") \\\\\\n.load()',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 149.0, 'y': 565.0, 'w': 130.0, 'h': 53.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Transform data',\n",
       "      'md': '# Transform data',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 157.0, 'y': 633.0, 'w': 51.0, 'h': 7.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': 'parsed_df = streaming_df.selectExpr(\"CAST(value AS STRING) as event_data\")',\n",
       "      'md': 'parsed_df = streaming_df.selectExpr(\"CAST(value AS STRING) as event_data\")',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 149.0, 'y': 565.0, 'w': 45.0, 'h': 87.0}},\n",
       "     {'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Write to Delta Lake',\n",
       "      'md': '# Write to Delta Lake',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 0.0, 'y': 0.0, 'w': 612.0, 'h': 792.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': 'parsed_df.writeStream \\\\\\n.format(\"delta\") \\\\\\n.outputMode(\"append\") \\\\\\n.option(\"checkpointLocation\", \"/deltalevents/_checkpoints/\") \\\\\\n.start(\"/deltalevents\")',\n",
       "      'md': 'parsed_df.writeStream \\\\\\n.format(\"delta\") \\\\\\n.outputMode(\"append\") \\\\\\n.option(\"checkpointLocation\", \"/deltalevents/_checkpoints/\") \\\\\\n.start(\"/deltalevents\")',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 149.0, 'y': 599.0, 'w': 123.0, 'h': 129.0}}],\n",
       "    'status': 'OK',\n",
       "    'links': [],\n",
       "    'width': 612.0,\n",
       "    'height': 792.0,\n",
       "    'triggeredAutoMode': False,\n",
       "    'parsingMode': 'accurate',\n",
       "    'structuredData': None,\n",
       "    'noStructuredContent': False,\n",
       "    'noTextContent': False},\n",
       "   {'page': 31,\n",
       "    'text': \"Conclusion: Bringing It All Together\\nThank you for joining me on this journey through Databricks! Whether you're a seasoned data professional or\\njust starting to explore the world of big data and AI, I hope this guide has given you valuable insights into the\\nplatform’s capabilities.\\nDatabricks is more than a tool—it's a gateway to innovation, empowering you to build scalable, intelligent\\nsolutions for the future. As you dive deeper, remember to experiment with its features like Delta Lake, Unity\\nCatalog, and Databricks Streaming to truly harness the platform's potential.\\nHappy exploring, and here’s to unlocking the power of your data!\",\n",
       "    'md': \"# Conclusion: Bringing It All Together\\n\\nThank you for joining me on this journey through Databricks! Whether you're a seasoned data professional or just starting to explore the world of big data and AI, I hope this guide has given you valuable insights into the platform’s capabilities.\\n\\nDatabricks is more than a tool—it's a gateway to innovation, empowering you to build scalable, intelligent solutions for the future. As you dive deeper, remember to experiment with its features like Delta Lake, Unity Catalog, and Databricks Streaming to truly harness the platform's potential.\\n\\nHappy exploring, and here’s to unlocking the power of your data!\",\n",
       "    'images': [],\n",
       "    'charts': [],\n",
       "    'tables': [],\n",
       "    'layout': [],\n",
       "    'items': [{'type': 'heading',\n",
       "      'lvl': 1,\n",
       "      'value': 'Conclusion: Bringing It All Together',\n",
       "      'md': '# Conclusion: Bringing It All Together',\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 82.96, 'w': 219.0, 'h': 14.0}},\n",
       "     {'type': 'text',\n",
       "      'lvl': None,\n",
       "      'value': \"Thank you for joining me on this journey through Databricks! Whether you're a seasoned data professional or just starting to explore the world of big data and AI, I hope this guide has given you valuable insights into the platform’s capabilities.\\n\\nDatabricks is more than a tool—it's a gateway to innovation, empowering you to build scalable, intelligent solutions for the future. As you dive deeper, remember to experiment with its features like Delta Lake, Unity Catalog, and Databricks Streaming to truly harness the platform's potential.\\n\\nHappy exploring, and here’s to unlocking the power of your data!\",\n",
       "      'md': \"Thank you for joining me on this journey through Databricks! Whether you're a seasoned data professional or just starting to explore the world of big data and AI, I hope this guide has given you valuable insights into the platform’s capabilities.\\n\\nDatabricks is more than a tool—it's a gateway to innovation, empowering you to build scalable, intelligent solutions for the future. As you dive deeper, remember to experiment with its features like Delta Lake, Unity Catalog, and Databricks Streaming to truly harness the platform's potential.\\n\\nHappy exploring, and here’s to unlocking the power of your data!\",\n",
       "      'rows': None,\n",
       "      'bBox': {'x': 36.0, 'y': 109.0, 'w': 526.0, 'h': 119.0}}],\n",
       "    'status': 'OK',\n",
       "    'links': [],\n",
       "    'width': 612.0,\n",
       "    'height': 792.0,\n",
       "    'triggeredAutoMode': False,\n",
       "    'parsingMode': 'accurate',\n",
       "    'structuredData': None,\n",
       "    'noStructuredContent': False,\n",
       "    'noTextContent': False}],\n",
       "  'job_metadata': {'job_pages': 0,\n",
       "   'job_auto_mode_triggered_pages': 0,\n",
       "   'job_is_cache_hit': True},\n",
       "  'file_name': '/tmp/tmp_l_kmqn2.pdf',\n",
       "  'job_id': 'a952d2bf-6dcd-4be4-b286-f2dc9c368de2',\n",
       "  'is_done': False,\n",
       "  'error': None}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "#url = \"http://127.0.0.1:8000/llama_parse_batch\"\n",
    "url = \"https://ancient-almeda-personal-personal-22e19704.koyeb.app/llama_parse_batch\"\n",
    "files = [\n",
    "    ('files', ('PDF1.pdf', open('PDF1.pdf', 'rb'), 'application/pdf')),\n",
    "    ('files', ('PDF2.pdf', open('PDF2.pdf', 'rb'), 'application/pdf')),\n",
    "]\n",
    "data = {\n",
    "    \"apikey\": llama_cloud_apikey\n",
    "}\n",
    "response = requests.post(url, data=data, files=files)\n",
    "response.json()['results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-407c7bbb-92ae-4d3b-86c7-2801234b3e84',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1749381642,\n",
       " 'model': 'gpt-4o-search-preview-2025-03-11',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': 'Determining the \"best\" action movie of 2024 can be subjective, as it depends on individual preferences and critical reception. However, several films have stood out for their impact and acclaim:\\n\\n\\n\\n**Furiosa: A Mad Max Saga**  \\nDirected by George Miller, this prequel to *Mad Max: Fury Road* explores the origins of Furiosa, portrayed by Anya Taylor-Joy. The film is lauded for its intense action sequences and deep character development. ([collider.com](https://collider.com/best-action-movies-2024-ranked/?utm_source=openai))\\n\\n\\n\\n\\n**The Fall Guy**  \\nStarring Ryan Gosling and Emily Blunt, this action-comedy pays homage to stunt performers. Directed by David Leitch, it features thrilling stunts and a nostalgic nod to the 1980s TV series. ([en.wikipedia.org](https://en.wikipedia.org/wiki/The_Fall_Guy_%282024_film%29?utm_source=openai))\\n\\n\\n\\n\\n**Kill**  \\nAn Indian action thriller that redefines Bollywood action with its brutal and gory sequences. The story follows an army commando on a mission to stop an arranged marriage, leading to intense combat scenes. ([polygon.com](https://www.polygon.com/24091121/best-action-movies-2024-watch?utm_source=openai))\\n\\n\\n\\n\\n**Godzilla x Kong: The New Empire**  \\nThis sequel brings together the iconic titans in a new adventure. Directed by Adam Wingard, it continues the Monsterverse saga with epic battles and stunning visuals. ([en.wikipedia.org](https://en.wikipedia.org/wiki/Godzilla_x_Kong%3A_The_New_Empire?utm_source=openai))\\n\\n\\n\\n\\n**Long Gone Heroes**  \\nStarring Frank Grillo and Josh Hutcherson, this action thriller follows a former soldier assembling a team to rescue his niece from a Venezuelan drug lord. The film is noted for its tactical and efficient storytelling. ([en.wikipedia.org](https://en.wikipedia.org/wiki/Long_Gone_Heroes?utm_source=openai))\\n\\n\\nThese films have been highlighted for their contributions to the action genre in 2024. Your enjoyment may vary based on personal taste, but each offers a unique take on action cinema. ',\n",
       "    'refusal': None,\n",
       "    'annotations': [{'type': 'url_citation',\n",
       "      'url_citation': {'end_index': 527,\n",
       "       'start_index': 439,\n",
       "       'title': '10 Best Action Movies of 2024, Ranked',\n",
       "       'url': 'https://collider.com/best-action-movies-2024-ranked/?utm_source=openai'}},\n",
       "     {'type': 'url_citation',\n",
       "      'url_citation': {'end_index': 839,\n",
       "       'start_index': 741,\n",
       "       'title': 'The Fall Guy (2024 film)',\n",
       "       'url': 'https://en.wikipedia.org/wiki/The_Fall_Guy_%282024_film%29?utm_source=openai'}},\n",
       "     {'type': 'url_citation',\n",
       "      'url_citation': {'end_index': 1158,\n",
       "       'start_index': 1061,\n",
       "       'title': 'The best action movies of the year so far | Polygon',\n",
       "       'url': 'https://www.polygon.com/24091121/best-action-movies-2024-watch?utm_source=openai'}},\n",
       "     {'type': 'url_citation',\n",
       "      'url_citation': {'end_index': 1471,\n",
       "       'start_index': 1368,\n",
       "       'title': 'Godzilla x Kong: The New Empire',\n",
       "       'url': 'https://en.wikipedia.org/wiki/Godzilla_x_Kong%3A_The_New_Empire?utm_source=openai'}},\n",
       "     {'type': 'url_citation',\n",
       "      'url_citation': {'end_index': 1804,\n",
       "       'start_index': 1718,\n",
       "       'title': 'Long Gone Heroes',\n",
       "       'url': 'https://en.wikipedia.org/wiki/Long_Gone_Heroes?utm_source=openai'}}]},\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 10,\n",
       "  'completion_tokens': 460,\n",
       "  'total_tokens': 470,\n",
       "  'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0},\n",
       "  'completion_tokens_details': {'reasoning_tokens': 0,\n",
       "   'audio_tokens': 0,\n",
       "   'accepted_prediction_tokens': 0,\n",
       "   'rejected_prediction_tokens': 0}},\n",
       " 'system_fingerprint': ''}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "url = \"http://127.0.0.1:8000/openai_search_single\"\n",
    "question = 'what is the best action movie of 2024'\n",
    "response = requests.post(url, params={\"apikey\": openai_apikey, 'question': question})\n",
    "response.json()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312-llm-tunnel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
